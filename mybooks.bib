@incollection{boltzmann,
chapter = {8.3},
pages = {385},
crossref = {Bishop2006}
}

@incollection{factorgraph,
chapter = {8.4.3},
crossref = {Bishop2006}
}

@inbook{posteriormackay,
chapter = {2.3},
pages = {29},
crossref = {MacKay2003}
}
@inbook{samplingmackay,
chapter = {29},
crossref = {MacKay2003} 
}
@article{Ising25,
author = {Ising, E},
doi = {10.1007/BF02980577},
journal = {Zeitschrift fur Physik},
pages = {253--258},
title = {{Beitrag zur Theorie des Ferromagnetismus}},
volume = {31},
year = {1925}
}


@article{Kruschke,
author = {Kruschke, John K},
file = {:home/alec/ba/papers/Doing Bayesian Data Analysis - A Tutorial with R and BUGS.John K. Kruschke.pdf:pdf},
title = {{No Title}}
}
@article{Hyvarinen2007,
abstract = {Score matching (SM) and contrastive divergence (CD) are two recently proposed methods for estimation of nonnormalized statistical methods without computation of the normalization constant (partition function). Although they are based on very different approaches, we show in this letter that they are equivalent in a special case: in the limit of infinitesimal noise in a specific Monte Carlo method. Further, we show how these methods can be interpreted as approximations of pseudolikelihood.},
author = {Hyv\"{a}rinen, Aapo},
file = {:home/alec/ba/papers/04298117.pdf:pdf},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Likelihood Functions,Models, Statistical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = sep,
number = {5},
pages = {1529--31},
pmid = {18220201},
title = {{Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18220201},
volume = {18},
year = {2007}
}
@misc{mfDCA,
author = {{Morcos, F., Pagnani, A., Lunt, B., Bertolino, A., Marks, D.S., Sander, C., Zecchina, R., Onuchic, J.N., Hwa, T., Weigt}, M.},
title = {{mfDCA, "Direct-coupling analysis of residue coevolution captures native contacts across many protein families."}},
url = {http://dca.upmc.fr/DCA/DCA.html},
year = {2011}
}
@misc{gplmDCA,
author = {{C. Feinauer, M.J. Skwark, A. Pagnani}, E. Aurell},
title = {{gplmDCA, "Improving contact prediction along three dimensions"}},
url = {http://gplmdca.aurell.org/download},
year = {2014}
}
@article{Pfam,
author = {Finn, Robert D and Bateman, Alex and Clements, Jody and Coggill, Penelope and Eberhardt, Ruth Y and Eddy, Sean R and Heger, Andreas and Hetherington, Kirstie and Holm, Liisa and Mistry, Jaina and Sonnhammer, Erik L L and Tate, John and Punta, Marco},
doi = {10.1093/nar/gkt1223},
journal = {Nucleic Acids Research},
number = {D1},
pages = {D222--D230},
title = {{Pfam: the protein families database}},
url = {http://pfam.sanger.ac.uk/},
volume = {42},
year = {2014}
}
@article{pdb,
author = {Fc, Bernstein and Koetzle, T K and Williams, G J and Meyer, E E and Brice, M D and Rodgers, J R and Kennard, O and Shimanouchi, T and Tasum, M},
journal = {J. of. Mol. Biol},
pages = {535+},
title = {{The Protein Data Bank: A Computer-based Archival File For Macromolecular Structures}},
volume = {112},
year = {1977}
}
@manual{stan2014,
author = {{Stan Development Team}},
title = {{Stan Modeling Language Users Guide and Reference Manual, Version 2.3}},
url = {http://mc-stan.org/},
year = {2014}
}
@article{,
file = {:home/alec/ba/papers/nealMCMC.pdf:pdf},
title = {{No Title}},
volume = {1}
}
@article{Informatik,
author = {Informatik, Bachelorstudiengangs},
file = {:home/alec/ba/papers/BSc-Inf\_Abschlussarbeit.pdf:pdf},
pages = {11--12},
title = {{No Title}}
}
@misc{,
title = {{ESLII\_print10.pdf.crdownload}}
}
@article{year,
title = {{No Title}}
}
@article{Kruschke,
author = {Kruschke, John K},
file = {:home/alec/ba/papers/Doing Bayesian Data Analysis - A Tutorial with R and BUGS.John K. Kruschke.pdf:pdf},
title = {{No Title}}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
chapter = {Graphical},
doi = {10.1117/1.2819119},
editor = {Jordan, M and Kleinberg, J and Sch\"{o}lkopf, B},
eprint = {0-387-31073-8},
file = {:home/alec/ba/papers/Bishop-PRML-sample.pdf:pdf},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Kommission2013,
author = {Kommission, Gemeinsame},
file = {:home/alec/ba/papers/gkwi-baiv\_sb\_ws2013.pdf:pdf},
number = {November},
pages = {1--27},
title = {{Bachelorarbeitsinfoveranstaltung 27.}},
year = {2013}
}
@article{Heckerman1991,
author = {Heckerman, David and Microsoft, One},
file = {:home/alec/ba/papers/hbtnn2e-III.pdf:pdf},
pages = {1--15},
title = {x 1 : : : x},
year = {1991}
}
@article{,
file = {:home/alec/ba/papers/ZSP2013-2.2.3.23-Statistik.pdf:pdf},
pages = {1--3},
title = {{No Title}}
}
@article{Sonnhammer1998,
author = {Sonnhammer, Erik L L and Krogh, Anders},
number = {June},
title = {{A hidden Markov model for predicting transmembrane helices in protein sequences}},
year = {1998}
}
@article{,
file = {:home/alec/ba/assisthesis\_leitfaden\_betreuung\_abschlussarbeit.pdf:pdf},
title = {{No Title}}
}
@misc{,
file = {:home/alec/ba/papers/ESLII\_print4.pdf:pdf},
title = {{ESLII\_print4.pdf}}
}
@article{,
file = {:home/alec/ba/papers/nc\_grenz\_Master1314.pdf:pdf},
pages = {1--2},
title = {{No Title}},
year = {2013}
}
@article{Feinauer14,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.0379v2},
author = {Feinauer, Christoph and Skwark, Marcin J and Pagnani, Andrea and Aurell, Erik and Torino, Politecnico and Science, Computer},
eprint = {arXiv:1403.0379v2},
file = {:home/alec/ba/papers/1403.0379v2.pdf:pdf},
title = {{Improving contact prediction along three dimensions}},
year = {2014}
}
@article{Potts52,
author = {Potts, R B},
doi = {10.1017/S0305004100027419},
issn = {1469-8064},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
number = {01},
pages = {106--109},
title = {{Some generalized order-disorder transformations}},
url = {http://journals.cambridge.org/article\_S0305004100027419},
volume = {48},
year = {1952}
}
@inproceedings{Jerrum90,
address = {New York, NY, USA},
author = {Jerrum, M and Sinclair, Alistair},
booktitle = {Proceedings of the Seventeenth International Colloquium on Automata, Languages and Programming},
isbn = {0-387-52826-1},
pages = {462--475},
publisher = {Springer-Verlag New York, Inc.},
title = {{Polynomial-time Approximation Algorithms for the Ising Model}},
url = {http://dl.acm.org/citation.cfm?id=90397.92354},
year = {1990}
}
@article{Barahona82,
abstract = {In a spin glass with Ising spins, the problems of computing the magnetic partition function and finding a ground state are studied. In a finite two-dimensional lattice these problems can be solved by algorithms that require a number of steps bounded by a polynomial function of the size of the lattice. In contrast to this fact, the same problems are shown to belong to the class of NP-hard problems, both in the two-dimensional case within a magnetic field, and in the three-dimensional case. NP-hardness of a problem suggests that it is very unlikely that a polynomial algorithm could exist to solve it.},
author = {Barahona, F},
journal = {Journal of Physics A: Mathematical and General},
number = {10},
pages = {3241},
title = {{On the computational complexity of Ising spin glass models}},
url = {http://stacks.iop.org/0305-4470/15/i=10/a=028},
volume = {15},
year = {1982}
}
@article{Suel2003,
abstract = {A fundamental goal in cellular signaling is to understand allosteric communication, the process by which signals originating at one site in a protein propagate reliably to affect distant functional sites. The general principles of protein structure that underlie this process remain unknown. Here, we describe a sequence-based statistical method for quantitatively mapping the global network of amino acid interactions in a protein. Application of this method for three structurally and functionally distinct protein families (G protein-coupled receptors, the chymotrypsin class of serine proteases and hemoglobins) reveals a surprisingly simple architecture for amino acid interactions in each protein family: a small subset of residues forms physically connected networks that link distant functional sites in the tertiary structure. Although small in number, residues comprising the network show excellent correlation with the large body of mechanistic data available for each family. The data suggest that evolutionarily conserved sparse networks of amino acid interactions represent structural motifs for allosteric communication in proteins.},
author = {S\"{u}el, G\"{u}rol M and Lockless, Steve W and Wall, Mark a and Ranganathan, Rama},
doi = {10.1038/nsb881},
file = {:home/alec/ba/papers/Suel\_et\_al.2002.pdf:pdf},
issn = {1072-8368},
journal = {Nature structural biology},
keywords = {Allosteric Regulation,Amino Acids,Amino Acids: chemistry,Amino Acids: genetics,Amino Acids: metabolism,Binding Sites,Binding Sites: genetics,Chymotrypsin,Chymotrypsin: chemistry,Chymotrypsin: genetics,Cluster Analysis,Conserved Sequence,Conserved Sequence: genetics,Databases, Protein,Evolution, Molecular,Hemoglobins,Hemoglobins: chemistry,Hemoglobins: genetics,Models, Molecular,Models, Statistical,Protein Structure, Tertiary,Proteins,Proteins: chemistry,Proteins: genetics,Proteins: metabolism,Receptors, G-Protein-Coupled,Receptors, G-Protein-Coupled: chemistry,Receptors, G-Protein-Coupled: genetics,Serine Endopeptidases,Serine Endopeptidases: chemistry,Serine Endopeptidases: genetics},
month = jan,
number = {1},
pages = {59--69},
pmid = {12483203},
title = {{Evolutionarily conserved networks of residues mediate allosteric communication in proteins.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12483203},
volume = {10},
year = {2003}
}
@article{Bengio2009a,
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
file = {:home/alec/ba/papers/ftml\_book.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends® in Machine Learning},
number = {1},
pages = {1--127},
title = {{Learning Deep Architectures for AI}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL\&doi=2200000006},
volume = {2},
year = {2009}
}
@article{Park2007,
author = {Park, Y Albert and Problem, Least Squares and Problem, Logistic Regression},
file = {:home/alec/ba/papers/L1norm.pdf:pdf},
pages = {1--7},
title = {{L1-norm Regularization}},
url = {http://cseweb.ucsd.edu/~saul/teaching/cse291s07/L1norm.pdf},
year = {2007}
}
@unpublished{Dyer,
author = {Dyer, Chris},
file = {:home/alec/ba/papers/adagrad.pdf:pdf},
pages = {1--3},
title = {{Notes on AdaGrad}},
howpublished = {\url{http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf}},
note = "[accessed 09-July-2014]",
url = {http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf}
}
@article{Bengio2009,
abstract = {We study an expansion of the log likelihood in undirected graphical models such as the restricted Boltzmann machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector in RBMs). We are particularly interested in estimators of the gradient of the log likelihood obtained through this expansion. We show that its residual term converges to zero, justifying the use of a truncation--running only a short Gibbs chain, which is the main idea behind the contrastive divergence (CD) estimator of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked autoassociators. The derivation is not specific to the particular parametric forms used in RBMs and requires only convergence of the Gibbs chain. We present theoretical and empirical evidence linking the number of Gibbs steps k and the magnitude of the RBM parameters to the bias in the CD estimator. These experiments also suggest that the sign of the CD estimator is correct most of the time, even when the bias is large, so that CD-k is a good descent direction even for small k.},
author = {Bengio, Yoshua and Delalleau, Olivier},
doi = {10.1162/neco.2008.11-07-647},
file = {:home/alec/ba/papers/cd\_expansion\_nc\_final.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Bias (Epidemiology),Humans,Learning,Learning: physiology,Likelihood Functions,Models, Statistical},
month = jun,
number = {6},
pages = {1601--21},
pmid = {19018704},
title = {{Justifying and generalizing contrastive divergence.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19018704},
volume = {21},
year = {2009}
}
@article{Tieleman2008,
address = {New York, New York, USA},
author = {Tieleman, Tijmen},
doi = {10.1145/1390156.1390290},
file = {:home/alec/ba/papers/Tieleman.2008.pdf:pdf},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
pages = {1064--1071},
publisher = {ACM Press},
title = {{Training restricted Boltzmann machines using approximations to the likelihood gradient}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390290},
year = {2008}
}
@misc{,
file = {:home/alec/ba/papers/ESLII\_print4.pdf:pdf},
title = {{ESLII\_print4.pdf}}
}
@misc{Topics2012,
author = {Topics, Discrete and Universit, Data Mining and Semester, Winter},
file = {:home/alec/ba/papers/maxent.pdf:pdf},
title = {{Topic III.2: Maximum Entropy Models}},
url = {http://www.mpi-inf.mpg.de/~pmiettin/dtdm/slides/TIII\_2.pdf},
year = {2012}
}
@article{Neal1993,
author = {Neal, Radford M},
file = {:home/alec/ba/papers/neal.pdf:pdf},
number = {September},
title = {{Probabilistic Inference Using Markov Chain Monte Carlo Methods}},
year = {1993}
}
@article{Ogata1990,
author = {Ogata, Yosihiko},
doi = {10.1007/BF00049299},
file = {:home/alec/ba/papers/amcmcmethod.pdf:pdf},
issn = {0020-3157},
journal = {Annals of the Institute of Statistical Mathematics},
month = sep,
number = {3},
pages = {403--433},
title = {{A Monte Carlo method for an objective Bayesian procedure}},
url = {http://link.springer.com/10.1007/BF00049299},
volume = {42},
year = {1990}
}
@article{Potamianos1997,
author = {Potamianos, G. and Goutsias, J.},
doi = {10.1109/18.641558},
file = {:home/alec/ba/papers/part-sampling:},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
number = {6},
pages = {1948--1965},
title = {{Stochastic approximation algorithms for partition function estimation of Gibbs random fields}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=641558},
volume = {43},
year = {1997}
}
@article{Mackay2001,
author = {Mackay, David J C},
file = {:home/alec/ba/papers/cdmackay.pdf:pdf},
number = {3},
pages = {1--9},
title = {{Failures of the One-Step Learning Algorithm}},
year = {2001}
}
@article{Geman1984a,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
author = {Geman, S and Geman, D},
file = {:home/alec/ba/papers/GemanandGeman84.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = jun,
number = {6},
pages = {721--41},
pmid = {22499653},
title = {{Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22499653},
volume = {6},
year = {1984}
}
@article{,
file = {:home/alec/ba/papers/nealMCMC.pdf:pdf},
title = {{No Title}},
volume = {1}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
file = {:home/alec/ba/papers/ncfast.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
month = jul,
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513},
volume = {18},
year = {2006}
}
@book{Lindholm99,
address = {Boston, MA, USA},
author = {Lindholm, Tim and Yellin, Frank},
edition = {2nd},
isbn = {0201432943},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
title = {{Java Virtual Machine Specification}},
year = {1999}
}
@techreport{Odersky04,
address = {Lausanne, Switzerland},
author = {Odersky, Martin and Al.},
institution = {EPFL},
number = {IC/2004/64},
title = {{An Overview of the Scala Programming Language}},
howpublished = {\url{www.scala-lang.org/docu/files/ScalaOverview.pdf}},
year = {2004}
}
@unpublished{Odersky2014,
author = {Odersky, Martin},
file = {:home/alec/ba/papers/ScalaReference.pdf:pdf},
institution = {EPFL},
title = {{The Scala Language Specification}},
year = {2014}
}
@unpublished{MALLET,
annote = {http://mallet.cs.umass.edu},
author = {McCallum, Andrew Kachites},
title = {{MALLET: A Machine Learning for Language Toolkit}},
howpublished = {\url{http://mallet.cs.umass.edu}},
url = {http://mallet.cs.umass.edu},
year = {2002}
}
@misc{InferNET12,
annote = {Microsoft Research Cambridge. http://research.microsoft.com/infernet},
author = {Minka, T and Winn, J M and Guiver, J P and Knowles, D A},
howpublished = {\url{http://research.microsoft.com/infernet}},
title = {{Infer.NET 2.5}},
year = {2012}
}
@unpublished{Woodford,
author = {Woodford, Oliver},
file = {:home/alec/ba/papers/NotesOnCD.pdf:pdf},
number = {Cd},
pages = {1--3},
title = {{Notes on Contrastive Divergence}},
howpublished = {\url{http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf}},
note = "[accessed 09-July-2014",
url = {http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf}
}
@article{Wu2013,
author = {Wu, Rui and Srikant, R. and Ni, Jian},
doi = {10.1214/12-SSY073},
file = {:home/alec/ba/papers/SSY-2012-73 (1).pdf:pdf},
issn = {1946-5238},
journal = {Stochastic Systems},
keywords = {62-09, 68W40, 68T05, 91C99, Markov random field, s},
number = {2},
pages = {362--404},
title = {{Learning loosely connected Markov random fields}},
url = {http://projecteuclid.org/euclid.ssy/1392131420},
volume = {3},
year = {2013}
}
@article{Finley2007,
author = {Finley, Thomas and Joachims, Thorsten and Hall, Upson},
file = {:home/alec/ba/papers/finley\_joachims\_07a.pdf:pdf},
title = {{Parameter Learning for Loopy Markov Random Fields with Structural Support Vector Machines}},
year = {2007}
}
@article{Wu2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1204.5540},
author = {Wu, Rui and Srikant, R and Ni, Jian},
eprint = {arXiv:1204.5540},
file = {:home/alec/ba/papers/WuSrikantNi(StochasticSystems12).pdf:pdf},
journal = {Stochastic Systems},
keywords = {62-09,68T05,68W40,91C99,Markov random field,s},
number = {2},
pages = {362--404},
title = {{Learning loosely connected Markov random fields}},
volume = {3},
year = {2012}
}
@article{year,
title = {{No Title}}
}
@inproceedings{Varun2006,
author = {Ganapahthi, Varun and Koller, Daphne and Lee, Su-in},
booktitle = {Conference on Neural Information Processing Systems},
file = {:home/alec/ba/papers/Lee+al-NIPS06.pdf:pdf},
number = {Ml},
title = {{Efficient Structure Learning of Markov Networks using L 1 -Regularization}},
year = {2006}
}
@article{Mining,
author = {Mining, Data},
file = {:home/alec/ba/papers/ESLII\_print10.pdf:pdf},
title = {{The Elements of Statistical Learning}}
}
@misc{,
file = {:home/alec/ba/papers/ESLII\_print10.pdf.crdownload:crdownload},
title = {{ESLII\_print10.pdf.crdownload}}
}
@inproceedings{Sato1997,
author = {Sato, Taisuke and Kameya, Yoshitaka},
booktitle = {Proceedings of the 15th International Joint Conference on Artificial Intelligence},
file = {:home/alec/ba/papers/prism.pdf:pdf},
title = {{PRISM: a language for symbolic-statistical modeling}},
year = {1997}
}
@misc{Pfeffer09,
author = {Pfeffer, Avi},
file = {:home/alec/ba/papers/figaro.pdf:pdf},
pages = {1--9},
publisher = {Charles River Analytics Technical Report},
title = {{Figaro: An Object-Oriented Probabilistic Programming Language}},
howpublished = {\url{https://www.cra.com/commercial-solutions/probabilistic-modeling-services.asp}},
year = {2009}
}
@article{Kimmig2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.4442v1},
author = {Kimmig, Angelika and Demoen, Bart and Raedt, Luc De and Rocha, Ricardo},
eprint = {arXiv:1006.4442v1},
file = {:home/alec/ba/papers/problog2.pdf:pdf},
journal = {Theory and Practice of Logic Programming},
number = {235-262},
title = {{On the Implementation of the Probabilistic Logic Programming Language ProbLog}},
volume = {11},
year = {2011}
}
@article{Kimmig2003,
author = {Kimmig, Angelika and Demoen, Bart and Raedt, Luc De and Rocha, Ricardo},
file = {:home/alec/ba/papers/problog.pdf:pdf},
title = {{On the Implementation of the Probabilistic Logic Programming Language ProbLog}},
year = {2003}
}
@inproceedings{Niu,
author = {Niu, Feng and Doan, Anhai and Shavlik, Jude},
booktitle = {Proceedings of the VLDB Endowment},
file = {:home/alec/ba/papers/tuffy.pdf:pdf},
pages = {373--384},
title = {{Tuffy: Scaling up Statistical Inference in Markov Logic Networks using an RDBMS}},
year = {2011}
}
@article{Kok2007,
author = {Kok, Stanley and Singla, Parag and Richardson, Matthew and Domingos, Pedro and Sumner, Marc and Poon, Hoifung},
file = {:home/alec/ba/papers/alchemy.pdf:pdf},
title = {{The Alchemy System for Statistical Relational AI: User Manual}},
howpublished = {\url{http://alchemy.cs.washington.edu/}},
url = {http://alchemy.cs.washington.edu/},
year = {2007}
}
@unpublished{Milch,
author = {Milch, Brian and Marthi, Bhaskara and Russell, Stuart and Division, Computer Science},
file = {:home/alec/ba/papers/milch.pdf:pdf},
title = {{BLOG: Relational Modeling with Unknown Objects}},
howpublished = {\url{http://www.cs.berkeley.edu/~russell/papers/ijcai05-blog.pdf}},
url = {http://www.cs.berkeley.edu/~russell/papers/ijcai05-blog.pdf},
year = {2006}
}
@article{Pfeffer,
author = {Pfeffer, Avi},
file = {:home/alec/ba/papers/ibal-journal.pdf:pdf},
journal = {Introduction to Statistical Relational Learning},
keywords = {expressive languages,functional languages,inference,probabilistic modeling},
pages = {399},
title = {{IBAL: An Expressive, Functional Probabilistic Modeling Language}},
year = {2007}
}
@unpublished{Goodman2008,
author = {Goodman, Noah D and Mansinghka, Vikash K and Roy, Daniel and Bonawitz, Keith and Tenenbaum, Joshua B},
file = {:home/alec/ba/papers/church\_GooManRoyBonTen-UAI-2008.pdf:pdf},
title = {{Church: a language for generative models}},

url = {http://arxiv.org/pdf/1206.3255.pdf},
year = {2008}
}
@article{Gilks94,
author = {Gilks, W R and Thomas, A and Spiegelhalter, D J},
journal = {The Statistician},
number = {1},
pages = {169--177},
title = {{A language and program for complex Bayesian modelling. 43}},
volume = {43},
year = {1994}
}
@article{Gordon2013,
author = {Gordon, Andrew D},
file = {:home/alec/ba/papers/agendaforprobabilisticprogramming.pdf:pdf},
number = {January},
title = {{An Agenda for Probabilistic Programming: Usable, Portable, and Ubiquitous}},
year = {2013}
}
@article{Goodman2013,
address = {New York, New York, USA},
author = {Goodman, Noah D.},
doi = {10.1145/2429069.2429117},
file = {:home/alec/ba/papers/POPL2013-abstract.pdf:pdf},
isbn = {9781450318327},
journal = {Proceedings of the 40th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages - POPL '13},
keywords = {and probabilistic infer-,belief,der,ence describes rational reasoning,exploded onto the scene,it is no won-,of,probabilistic models,probabilistic programs,probabilities describe degrees of,that probabilistic models have,then,under uncertainty},
pages = {399},
publisher = {ACM Press},
title = {{The principles and practice of probabilistic programming}},
url = {http://dl.acm.org/citation.cfm?doid=2429069.2429117},
year = {2013}
}
@article{Steuer2002,
abstract = {MOTIVATION: Clustering co-expressed genes usually requires the definition of 'distance' or 'similarity' between measured datasets, the most common choices being Pearson correlation or Euclidean distance. With the size of available datasets steadily increasing, it has become feasible to consider other, more general, definitions as well. One alternative, based on information theory, is the mutual information, providing a general measure of dependencies between variables. While the use of mutual information in cluster analysis and visualization of large-scale gene expression data has been suggested previously, the earlier studies did not focus on comparing different algorithms to estimate the mutual information from finite data.

RESULTS: Here we describe and review several approaches to estimate the mutual information from finite datasets. Our findings show that the algorithms used so far may be quite substantially improved upon. In particular when dealing with small datasets, finite sample effects and other sources of potentially misleading results have to be taken into account.},
author = {Steuer, R and Kurths, J and Daub, C O and Weise, J and Selbig, J},
file = {:home/alec/ba/papers/Bioinformatics-2002-Steuer-S231-40.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Software},
month = jan,
pages = {S231--40},
pmid = {12386007},
title = {{The mutual information: detecting and evaluating dependencies between variables.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12386007},
volume = {18 Suppl 2},
year = {2002}
}
@article{Xiao,
author = {Xiao, Lin},
file = {:home/alec/ba/papers/rda\_nips09\_fixed.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {2116--2124},
title = {{Dual Averaging Method for Regularized Stochastic Learning and Online Optimization}},
year = {2009}
}
@article{Duchi2010,
author = {Duchi, John and Hazan, Elad},
file = {:home/alec/ba/papers/adagradr:},
journal = {The Journal of Machine Learning Research},
pages = {1--23},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
year = {2011}
}
@unpublished{Byrd1994,
author = {Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
file = {:home/alec/ba/papers/A Limited Memory Algorithm for Bound Constrained Optimization .pdf:pdf},
title = {{A limited memory algorithm for bound constrained optimization}},
journal = {SIAM J. Sci. Comput.},
volume = {16},
number = {5},
month = sep,
year = {1995},
issn = {1064-8275},
pages = {1190--1208},
publisher = {Society for Industrial and Applied Mathematics},
address = {Philadelphia, PA, USA},

}
@unpublished{Byrd1996,
author = {Byrd, Richard H and Nocedal, Jorge and Schnabel, Robert B},
file = {:home/alec/ba/papers/ByrdNocedalSchnabel,:},
title = {{Representations of quasi-newton matrices and their use in limited memory methods}},
journal = {Math. Program.},

volume = {63},
number = {2},
year = {1994},
pages = {129--156},
publisher = {Springer-Verlag New York, Inc.},
}
@book{Bottou98,
address = {Cambridge, UK},
author = {Bottou, Leon},
editor = {Saad, David},
file = {:home/alec/ba/papers/online-1998.pdf:pdf},
publisher = {Cambridge University Press},
title = {{Online Learning and Stochastic Approximations}},
year = {1998}
}
@book{Pearl88,
address = {San Francisco, CA, USA},
author = {Pearl, Judea},
isbn = {0-934613-73-7},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference}},
year = {1988}
}
@inproceedings{Gordon2014,
author = {Gordon, Andrew D and Henzinger, Thomas A and Nori, Aditya V and Rajamani, Sriram K},
booktitle = {International Conference on Software Engineering},
file = {:home/alec/ba/papers/fose-icse2014.pdf:pdf},
title = {{Probabilistic Programming}},
year = {2014}
}
@article{Hastings70,
author = {Hastings, W K},
doi = {10.1093/biomet/57.1.97},
issn = {0006-3444; 1464-3510/e},
journal = {Biometrika},
pages = {97--109},
publisher = {Biometrika Trust, Department of Statistical Science, University College London; Oxford University Press, Oxford},
title = {{Monte Carlo sampling methods using Markov chains and their applications.}},
volume = {57},
year = {1970}
}
@article{Metropolis53,
author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
journal = {The Journal of Chemical Physics},
number = {6},
title = {{Equation of State Calculations by Fast Computing Machines}},
volume = {21},
year = {1953}
}
@article{Geman1984,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
author = {Geman, S and Geman, D},
file = {:home/alec/ba/papers/geman.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = jun,
number = {6},
pages = {721--41},
pmid = {22499653},
title = {{Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22499653},
volume = {6},
year = {1984}
}
@inproceedings{Hinton05,
author = {Miguel, A and Hinton, Geoffrey E},
booktitle = {Tenth International Workshop on Artificial Intelligence and Statistics},
file = {:home/alec/ba/papers/constrastive\_divergence2.pdf:pdf},
pages = {33--40},
title = {{On Contrastive Divergence Learning}},
volume = {0},
year = {2005}
}
@article{Aji2000,
author = {Aji, S.M. and McEliece, R.J.},
doi = {10.1109/18.825794},
file = {:home/alec/ba/papers/AJIieeetit00.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
month = mar,
number = {2},
pages = {325--343},
title = {{The generalized distributive law}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=825794},
volume = {46},
year = {2000}
}
@article{Aji,
author = {Aji, Srinivas M. and McEliece, Robert J.},
file = {:home/alec/ba/papers/Allerton01.pdf:pdf},
title = {{The Generalized Distributive Law and Free Energy Minimization}}
}
@article{Viterbi1967,
author = {Viterbi, A J},
doi = {10.1109/TIT.1967.1054010},
issn = {0018-9448},
journal = {Information Theory, IEEE Transactions on},
keywords = {Convolutional codes},
month = apr,
number = {2},
pages = {260--269},
title = {{Error bounds for convolutional codes and an asymptotically optimum decoding algorithm}},
volume = {13},
year = {1967}
}
@inproceedings{Pearl1982,
author = {Pearl, Judea and Science, Applied and Angeles, Los},
booktitle = {AAAI-82},
file = {:home/alec/ba/papers/AAAI82-032.pdf:pdf},
pages = {133--136},
title = {{Reverend Bayes on Inference Engines: A Distributied Hierarchical Approach}},
year = {1982}
}
@article{Yedidia2001,
author = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
file = {:home/alec/ba/papers/TR2001-22.pdf:pdf},
title = {{Understanding Belief Propagation and its Generalizations}},
year = {2001}
}
@book{Wiberg1996,
author = {Wiberg, Niclas},
file = {:home/alec/ba/papers/tannergraphs.pdf:pdf},
isbn = {9178717299},
number = {440},
title = {{Codes and Decoding on General Graphs}},
year = {1996}
}
@article{Loeliger2004,
author = {Loeliger, Hans-Andrea},
file = {:home/alec/ba/papers/factorgraphs.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {January},
title = {{An Introduction to Factor Graphs}},
year = {2004}
}
@book{Mitchell1990,
author = {Mitchell, T},
doi = {10.1146/annurev.cs.04.060190.002221},
file = {:home/alec/ba/papers/McGrawHill\_-\_Machine\_Learning\_-Tom\_Mitchell.pdf:pdf},
isbn = {0070428077},
issn = {8756-7016},
month = jun,
number = {1},
pages = {417--433},
title = {{Machine Learning}},
volume = {4},
year = {1990}
}
@book{MacKay2003,
author = {MacKay, David J.C.},
file = {:home/alec/ba/papers/mackay.pdf:pdf},
isbn = {0521642981},
keywords = {MacKay2003},
mendeley-tags = {MacKay2003},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference, and Learning Algorithms}},
year = {2003}
}
@article{Science2012,
author = {Science, Computer},
file = {:home/alec/ba/other papers/UM-CS-2012-015.pdf:pdf},
pages = {1--14},
title = {{Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia}},
year = {2012}
}
@article{Singh2013,
address = {New York, New York, USA},
author = {Singh, Sameer and Riedel, Sebastian and Martin, Brian and Zheng, Jiaping and McCallum, Andrew},
doi = {10.1145/2509558.2509559},
file = {:home/alec/ba/other papers/572aeb69-f86f-41fb-a673-b7b6bea69abf.pdf:pdf},
isbn = {9781450324113},
journal = {Proceedings of the 2013 workshop on Automated knowledge base construction - AKBC '13},
pages = {1--6},
publisher = {ACM Press},
title = {{Joint inference of entities, relations, and coreference}},
url = {http://dl.acm.org/citation.cfm?doid=2509558.2509559},
year = {2013}
}
@article{,
file = {:home/alec/ba/assisthesis\_leitfaden\_betreuung\_abschlussarbeit.pdf:pdf},
title = {{No Title}}
}
@article{Wallach2009,
address = {New York, New York, USA},
author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
doi = {10.1145/1553374.1553515},
file = {:home/alec/ba/papers/wallach09evaluation.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
number = {4},
pages = {1--8},
publisher = {ACM Press},
title = {{Evaluation methods for topic models}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553515},
year = {2009}
}
@article{Haddow2011,
author = {Haddow, Barry and Arun, Abhishek and Koehn, Philipp},
file = {:home/alec/ba/papers/W11-2130.pdf:pdf},
pages = {261--271},
title = {{SampleRank Training for Phrase-Based Machine Translation}},
year = {2011}
}
@article{Ghahramani2004,
author = {Ghahramani, Zoubin},
file = {:home/alec/ba/papers/ul.pdf:pdf},
pages = {1--32},
title = {{Unsupervised Learning ∗}},
year = {2004}
}
@article{Waite2014,
abstract = {Having worked as a specialty doctor in palliative care for the past 6 years and now returning to general practice, I have become increasingly uncertain as to the future role of in-patient hospices. Perhaps the time has come for a realistic look at where exactly they fit in to the bigger picture of end-of-life care. Of particular concern is whether their existence inhibits the development of other services and fuels the myth that palliative care is different from simply good care.},
author = {Waite, Anthony},
file = {:home/alec/ba/papers/When The Saints Go Marching In (Final).pdf:pdf},
issn = {1357-6321},
journal = {International journal of palliative nursing},
month = jan,
number = {1},
pages = {46--Unknown},
pmid = {24464173},
title = {{When the saints go marching on.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24464173},
volume = {20},
year = {2014}
}
@article{Singh2001,
author = {Singh, Sameer and Mccallum, Andrew and Wick, Michael},
file = {:home/alec/ba/papers/singh12mcmcmc.pdf:pdf},
title = {{Monte Carlo MCMC: Efficient Inference by Sampling Factors}},
year = {2001}
}
@article{Raedt,
author = {Raedt, Luc De},
file = {:home/alec/ba/papers/salvador.pdf:pdf},
title = {{An Introduction to Statistical Relational Learning}}
}
@inproceedings{Parise2006,
author = {Parise, Sridevi and Science, Computer and Welling, Max},
booktitle = {Conference on Neural Information Processing Systems},
file = {:home/alec/ba/papers/StructLearnMRF-submit.pdf:pdf},
keywords = {contrastive divergende use},
mendeley-tags = {contrastive divergende use},
title = {{Structure Learning in Markov Random Fields}},
year = {2006}
}
@article{Hinton2002,
abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual "expert" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called "contrastive divergence" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
author = {Hinton, Geoffrey E},
doi = {10.1162/089976602760128018},
file = {:home/alec/ba/papers/nccd.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
month = aug,
number = {8},
pages = {1771--800},
pmid = {12180402},
title = {{Training products of experts by minimizing contrastive divergence.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12180402},
volume = {14},
year = {2002}
}
@article{Passos,
author = {Passos, Alexandre and Vilnis, Luke and Mccallum, Andrew},
file = {:home/alec/ba/papers/nips-ws-factorie-optimization.pdf:pdf},
number = {2},
pages = {1--6},
title = {{Optimization and Learning in Factorie}},
volume = {01003}
}
@article{Carlo2004,
author = {Carlo, Monte},
file = {:home/alec/ba/papers/mcmc-gibbs-intro.pdf:pdf},
number = {April},
title = {{Markov Chain Monte Carlo and Gibbs Sampling}},
year = {2004}
}
@article{Rohanimanesh2009,
author = {Rohanimanesh, Khashayar and Wick, Michael and Mccallum, Andrew},
file = {:home/alec/ba/papers/IR-726.pdf:pdf},
title = {{Inference and Learning in Large Factor Graphs with Adaptive Proposal Distributions and a Rank-based Objective}},
year = {2009}
}
@article{Wallach,
author = {Wallach, Hanna M},
file = {:home/alec/ba/papers/learning\_dbns.pdf:pdf},
title = {{Learning the Structure of Deep, Sparse Graphical Models}}
}
@article{Kommission2013,
author = {Kommission, Gemeinsame},
file = {:home/alec/ba/papers/gkwi-baiv\_sb\_ws2013.pdf:pdf},
number = {November},
pages = {1--27},
title = {{Bachelorarbeitsinfoveranstaltung 27.}},
year = {2013}
}
@article{Ng2005,
author = {Ng, Andrew},
file = {:home/alec/ba/papers/Hammon.pdf:pdf},
title = {{Feature selection, L1 vs. L2 regularization, and rotational invariance}},
year = {2005}
}
@article{Arr,
author = {Arr, Traditional and West, Blake},
file = {:home/alec/ba/papers/God Rest Ye Merry Gentleman (Final).pdf:pdf},
pages = {4--6},
title = {{God Rest Ye Merry Gentleman}}
}
@article{Heckerman1991,
author = {Heckerman, David and Microsoft, One},
file = {:home/alec/ba/papers/hbtnn2e-III.pdf:pdf},
pages = {1--15},
title = {x 1 : : : x},
year = {1991}
}
@article{Dembo2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1110.4821v1},
author = {Dembo, Amir and Montanari, Andrea and Sun, Nike},
eprint = {arXiv:1110.4821v1},
file = {:home/alec/ba/papers/factor\_nike.pdf:pdf},
pages = {1--35},
title = {{Factor models on locally tree-like graphs}},
year = {2011}
}
@inproceedings{Daume2009,
author = {Daume, Hal},
booktitle = {26th INfernational Conference on Machine Learning},
file = {:home/alec/ba/papers/daume09unsearn.pdf:pdf},
number = {Section 4},
title = {{Unsupervised Search-based Structured Prediction}},
year = {2009}
}
@article{Ay2011,
author = {Ay, Ciro Donalek},
file = {:home/alec/ba/papers/Donalek\_Classif.pdf:pdf},
number = {April},
title = {{Supervised and Unsupervised Learning}},
year = {2011}
}
@article{Buhmann,
author = {Buhmann, Joachim M and Held, Marcus},
file = {:home/alec/ba/papers/download (1).pdf:pdf},
title = {{Unsupervised learning without overfitting: empirical risk approximation as an induction principle for reliable clustering}}
}
@article{Presented2008,
author = {Presented, A Dissertation and Culotta, Aron},
file = {:home/alec/ba/papers/culotta08learning.pdf:pdf},
number = {May},
title = {{Learning and inference in weighted logic with application to natural language processing}},
year = {2008}
}
@article{Chen2009,
author = {Chen, Lisha and Buja, Andreas},
file = {:home/alec/ba/papers/Chen-Buja-Power Laws and Clustering Properties.pdf:pdf},
keywords = {and phrases,box-cox transforma-,cluster analysis,force-directed layout,mds,tions,unsupervised learning},
pages = {1--37},
title = {{Energy/Stress Functions for Dimension Reduction and Graph Drawing: Power Laws and Their Clustering Properties}},
volume = {06511},
year = {2009}
}
@article{,
file = {:home/alec/ba/papers/CS194 Fall 2011 Lecture 04.pdf:pdf},
pages = {1--17},
title = {{Machine learning methodology: Overfitting, regularization, and all that}},
year = {2011}
}
@article{Green2002,
author = {Green, Peter J and Richardson, Sylvia},
doi = {10.1198/00000000},
file = {:home/alec/ba/papers/asatm-01209r1.pdf:pdf},
keywords = {allocation,bayesian hierarchical model,disease mapping,finite mixture distributions,heterogeneity,hidden markov,markov chain monte carlo,merge moves,model,models,poisson mixtures,potts model,reversible jump algorithms,semiparametric,spatial mixtures,split},
number = {460},
title = {{Hidden Markov Models and Disease Mapping}},
volume = {97},
year = {2002}
}
@article{Bengtson2004,
author = {Bengtson, Eric and Roth, Dan},
file = {:home/alec/ba/papers/BengtsonRo08.pdf:pdf},
title = {{Understanding the Value of Features for Coreference Resolution}},
year = {2004}
}
@article{Boureau,
author = {Boureau, Y-lan and Chopra, Sumit and Lecun, Yann},
file = {:home/alec/ba/papers/AISTATS07\_RanzatoBCL.pdf:pdf},
title = {{A Unified Energy-Based Framework for Unsupervised Learning}}
}
@article{Abbeel2006,
author = {Abbeel, Pieter and Koller, Daphne and Ng, Andrew Y},
file = {:home/alec/ba/papers/Abbeel+al-JMLR06.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bayesian networks,factor graphs,markov,networks,parameter and structure learning,probabilistic graphical models},
pages = {1743--1788},
title = {{Learning Factor Graphs in Polynomial Time and Sample Complexity}},
volume = {7},
year = {2006}
}
@article{Adams,
author = {Adams, Ryan Prescott and Wallach, Hanna M and Ghahramani, Zoubin},
file = {:home/alec/ba/papers/adams-structure-aistats-2010.pdf:pdf},
title = {{Learning the Structure of Deep Sparse Graphical Models}}
}
@article{Broderick2013,
archivePrefix = {arXiv},
arxivId = {arXiv:0712.2437v2},
author = {Broderick, Tamara and Schapire, Robert E and Bialek, William},
eprint = {arXiv:0712.2437v2},
file = {:home/alec/ba/papers/0712.2437v2.pdf:pdf},
title = {{Faster solutions of the inverse pairwise Ising problem}},
year = {2013}
}
@unpublished{Chen2009a,
author = {Chen, Yutian and Welling, Max},
file = {:home/alec/ba/papers/237.pdf:pdf},
title = {{Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior}},
year = {2009}
}
@article{Ginestet2010,
author = {Ginestet, Cedric},
doi = {10.1111/j.1467-985X.2010.00663\_3.x},
file = {:home/alec/ba/papers/194982971.pdf:pdf},
issn = {09641998},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
month = sep,
number = {4},
pages = {934--935},
title = {{Introduction to Statistical Relational Learning}},
url = {http://doi.wiley.com/10.1111/j.1467-985X.2010.00663\_3.x},
volume = {173},
year = {2010}
}
@article{Burger2010,
abstract = {Predicting protein structure from primary sequence is one of the ultimate challenges in computational biology. Given the large amount of available sequence data, the analysis of co-evolution, i.e., statistical dependency, between columns in multiple alignments of protein domain sequences remains one of the most promising avenues for predicting residues that are contacting in the structure. A key impediment to this approach is that strong statistical dependencies are also observed for many residue pairs that are distal in the structure. Using a comprehensive analysis of protein domains with available three-dimensional structures we show that co-evolving contacts very commonly form chains that percolate through the protein structure, inducing indirect statistical dependencies between many distal pairs of residues. We characterize the distributions of length and spatial distance traveled by these co-evolving contact chains and show that they explain a large fraction of observed statistical dependencies between structurally distal pairs. We adapt a recently developed Bayesian network model into a rigorous procedure for disentangling direct from indirect statistical dependencies, and we demonstrate that this method not only successfully accomplishes this task, but also allows contacts with weak statistical dependency to be detected. To illustrate how additional information can be incorporated into our method, we incorporate a phylogenetic correction, and we develop an informative prior that takes into account that the probability for a pair of residues to contact depends strongly on their primary-sequence distance and the amount of conservation that the corresponding columns in the multiple alignment exhibit. We show that our model including these extensions dramatically improves the accuracy of contact prediction from multiple sequence alignments.},
author = {Burger, Lukas and van Nimwegen, Erik},
doi = {10.1371/journal.pcbi.1000633},
file = {:home/alec/ba/papers/journal.pcbi.1000633.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Amino Acid Sequence,Binding Sites,Molecular Sequence Data,Protein Binding,Proteins,Proteins: chemistry,Proteins: genetics,Proteins: ultrastructure,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, Protein,Sequence Analysis, Protein: methods},
month = jan,
number = {1},
pages = {e1000633},
pmid = {20052271},
title = {{Disentangling direct from indirect co-evolution of residues in protein alignments.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2793430\&tool=pmcentrez\&rendertype=abstract},
volume = {6},
year = {2010}
}
@article{Wick2008,
author = {Wick, Michael and Rohanimanesh, Khashayar and Culotta, Aron and Mccallum, Andrew},
file = {:home/alec/ba/papers/wick09samplerank.pdf:pdf},
journal = {Advances in Ranking},
pages = {69},
title = {{SampleRank: Learning Preferences from Atomic Gradients}},
year = {2008}
}
@inproceedings{Wick,
author = {Wick, Michael and Mccallum, Andrew and Miklau, Gerome},
booktitle = {International Conference on Very Large Data Bases},
file = {:home/alec/ba/papers/wick10scalable.pdf:pdf},
title = {{Scalable Probabilistic Databases with Factor Graphs and MCMC}},
year = {2010}
}
@inproceedings{Singh,
author = {Singh, Sameer and Schultz, Karl and Mccallum, Andrew},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
file = {:home/alec/ba/papers/bidirectional-ecml09.pdf:pdf},
title = {{Bi-directional Joint Inference for Entity Resolution and Segmentation Using Imperatively-Defined Factor Graphs}},
year = {2009}
}
@article{Sutton2012,
author = {Sutton, Charles},
doi = {10.1561/2200000013},
file = {:home/alec/ba/papers/crftut-fnt.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends® in Machine Learning},
number = {4},
pages = {267--373},
title = {{An Introduction to Conditional Random Fields}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL\&doi=2200000013},
volume = {4},
year = {2012}
}
@article{,
file = {:home/alec/ba/papers/Seminar2009MartinVejmelka.pdf:pdf},
title = {{Introduction Approaches to SGC Advanced approaches}}
}
@article{,
file = {:home/alec/ba/papers/ZSP2013-2.2.3.23-Statistik.pdf:pdf},
pages = {1--3},
title = {{No Title}}
}
@article{Yedidia2000,
author = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
file = {:home/alec/ba/papers/yedidia\_2001.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Generalized Belief Propagation}},
volume = {13},
year = {2001}
}
@article{Yoshida2010,
abstract = {We describe a class of sparse latent factor models, called graphical factor models (GFMs), and relevant sparse learning algorithms for posterior mode estimation. Linear, Gaussian GFMs have sparse, orthogonal factor loadings matrices, that, in addition to sparsity of the implied covariance matrices, also induce conditional independence structures via zeros in the implied precision matrices. We describe the models and their use for robust estimation of sparse latent factor structure and data/signal reconstruction. We develop computational algorithms for model exploration and posterior mode search, addressing the hard combinatorial optimization involved in the search over a huge space of potential sparse configurations. A mean-field variational technique coupled with annealing is developed to successively generate "artificial" posterior distributions that, at the limiting temperature in the annealing schedule, define required posterior modes in the GFM parameter space. Several detailed empirical studies and comparisons to related approaches are discussed, including analyses of handwritten digit image and cancer gene expression data.},
author = {Yoshida, Ryo and West, Mike},
file = {:home/alec/ba/papers/yoshida10a.pdf:pdf},
issn = {1532-4435},
journal = {Journal of machine learning research : JMLR},
keywords = {annealing,gene expression profiling,graphical factor models,map estimation,sparse factor analysis,variational mean-field method},
month = may,
pages = {1771--1798},
pmid = {20890391},
title = {{Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2947451\&tool=pmcentrez\&rendertype=abstract},
volume = {11},
year = {2010}
}
@unpublished{Wick2011,
author = {Wick, Michael and Rohanimanesh, Khashayar and Bellare, Kedar and Culotta, Aron and Mccallum, Andrew},
file = {:home/alec/ba/papers/wick11sample.pdf:pdf},
title = {{SampleRank: Training Factor Graphs with Atomic Gradients}},
year = {2011}
}
@article{Beaudin,
author = {Beaudin, Laura},
file = {:home/alec/ba/papers/v8n1-13pd.pdf:pdf},
pages = {1--25},
title = {{A Review of the Potts Model: Its Connection to the Tutte Polynomial and its Application to Complex Experiments}}
}
@article{Higuchi1988,
author = {Higuchi, Y and Fields, Random and Math, Colloquis and Dobrushin, R L and Shlosman, B B and Rev, Sci},
file = {:home/alec/ba/papers/wu-potts2.pdf:pdf},
keywords = {chromatic function,correlation function,flow polynomial,graph,partition function,potts model,theory},
number = {1},
pages = {1--8},
title = {{Potts Model and Graph Theory}},
volume = {0},
year = {1988}
}
@unpublished{Herbrich2006,
author = {Herbrich, Ralf and Graepel, Thore},
file = {:home/alec/ba/papers/TR-2006-80.pdf:pdf},
title = {{TrueSkill: A Bayesian Skill Rating System}},
volume = {5},
year = {2006}
}
@unpublished{Jordan,
author = {Jordan, Michael I. and Russell, Stuart and Xing, Eric P.},
file = {:home/alec/ba/papers/uai03-gmf (1).pdf:pdf},
title = {{A generalized mean field algorithm for variational inference in exponential families}}
}
@article{Guttmann2009,
author = {Guttmann, Tony},
file = {:home/alec/ba/papers/tonyguttmann.pdf:pdf},
title = {{The Ising Model susceptibility}},
year = {2009}
}
@article{Real2013,
author = {Real, Under and Ecologies, World and Events, Random},
file = {:home/alec/ba/papers/textbook.pdf:pdf},
number = {November},
title = {{Fat tails and convexity}},
year = {2013}
}
@article{Manual2013,
author = {Manual, Reference},
file = {:home/alec/ba/papers/stan-reference-2.0.1.pdf:pdf},
title = {{Stan Modeling Language}},
year = {2013}
}
@unpublished{Roudi,
archivePrefix = {arXiv},
arxivId = {arXiv:0905.1410v1},
author = {Roudi, Yasser and Aurell, Erik and Hertz, John A},
eprint = {arXiv:0905.1410v1},
file = {:home/alec/ba/papers/Statistical physics of pairwise probability models.pdf:pdf},
pages = {1--25},
title = {{Statistical physics of pairwise probability models}}
}
@article{Sire1995,
author = {Sire, Clement and Majumdar, Satya N.},
file = {:home/alec/ba/papers/siremajumdar95.pdf:pdf},
journal = {Physical Review Letters},
number = {21},
pages = {4321--4324},
title = {{Correlations and Coaersening in the q-State Potts Model}},
volume = {74},
year = {1995}
}
@article{Way2011,
author = {Way, Your and Top, T O T H E},
file = {:home/alec/ba/papers/ScriptAutomationReport.pdf:pdf},
title = {year of the hustle},
year = {2011}
}
@book{Suereth,
author = {Suereth, Joshua D},
file = {:home/alec/ba/papers/Scala in Depth.pdf:pdf},
title = {{Scala in Depth}}
}
@article{Mccallum,
author = {Mccallum, Andrew and Rohanemanesh, Khashayar and Wick, Michael and Schultz, Karl and Singh, Sameer},
file = {:home/alec/ba/papers/factorie-nipsws.pdf:pdf},
journal = {Advances on Neural Information Processing Systems},
pages = {1--3},
title = {{FACTORIE: Efficient Probabilistic Programming for Relational Factor Graphs via Imperative Declarations of Structure, Inference and Learning}},
year = {2008}
}
@article{Wu1982,
author = {Wu, F. Y.},
file = {:home/alec/ba/papers/Wu090\_RMP54\_235.pdf:pdf},
journal = {Reviews of Modern Physics},
number = {1},
pages = {235--268},
title = {{The Potts model}},
volume = {54},
year = {1982}
}
@book{Bishop2006a,
author = {Bishop, Christopher M},
file = {:home/alec/ba/papers/Bishop-PRML-sample.pdf:pdf},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}

@article{MacKay1998,
author = {MacKay, David and Frey, Brendan},
file = {:home/alec/ba/papers/rev.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{A Revolution: Belief Propagation in Graphs With Cycles}},
volume = {10},
year = {1998}
}
@article{Johanse2010,
abstract = {In a series of papers based on analogies with statistical physics models, we have proposed that most financial crashes are the climax of so-called log-periodic power law signatures (LPPS) associated with speculative bubbles (Sornette and Johansen, 1998; Johansen and Sornette, 1999; Johansen et al., 1999; Johansen et al., 2000; Sornette and Johansen, 2001a). In addition, a large body of empirical evidence supporting this proposition have been presented (Sornette et al., 1996; Sornette and Johansen, 1998; Johansen et al., 2000; Johansen and Sornette, 2000; Johansen and Sornette, 2001a, Sornette and Johansen, 2001b). Along a complementary line of research, we have established that, while the vast majority of drawdowns occurring on the major financial markets have a distribution which is well-described by a stretched exponential, the largest drawdowns are occurring with a significantly larger rate than predicted by extrapolating the bulk of the distribution and should thus be considered as outliers (Johansen and Sornette, 1998; Sornette and Johansen, 2001; Johansen and Sornette, 2001; Johansen, 2002). Here, these two lines of research are merged in a systematic way to offer a classification of crashes as either events of an endogenous origin associated with preceding speculative bubbles or as events of an exogenous origin associated with the markets response to external shocks},
author = {Johanse, Andes and Sornette, Didier},
file = {:home/alec/ba/papers/Proofs\_BER52-1-ART4-v2.pdf:pdf},
journal = {Brussels Economic Review},
keywords = {Potts,economics},
mendeley-tags = {Potts,economics},
number = {2},
pages = {201--253},
title = {{SHOCKS, CRASHES AND BUBBLES IN FINANCIAL MARKETS}},
volume = {53},
year = {2010}
}
@article{Odersky2008,
abstract = {This book is a tutorial for the Scala programming language, written by peo- ple directly involved in the development of Scala. Our goal is that by reading this book, you can learn everything you need to be a productive Scala pro- grammer. All examples in this book compile with Scala version 2.7.2.},
author = {Odersky, Martin and Spoon, Lex and Venners, Bill},
chapter = {2},
file = {:home/alec/ba/papers/Programming in Scala, 2nd edition (Artima, 2011, 0981531644).pdf:pdf},
journal = {Theoretical Computer Science},
keywords = {version=December 13, 2010, orderNumber=00MSjn9PL1},
number = {2-3},
pages = {202--220},
publisher = {Artima Inc},
title = {{Programming in Scala}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0304397508006695},
volume = {410},
year = {2008}
}
@phdthesis{Zhou2009,
abstract = {Ising and Potts models are discrete Gibbs random field models originating in sta- tistical physics, which are now widely used in statistics for applications in spatial modeling, image processing, computational biology, and computational neuroscience. However, parameter estimation in these models remains challenging due to the appear- ance of intractable normalizing constants in the likelihood. Here we compare several proposed approximation schemes for Bayesian parameter estimation, including mul- tiple Monte Carlo methods for approximating ratios of normalizing constants based on importance sampling, bridge sampling, and recently proposed perfect simulation methods. On small lattices where exact recursions can be used for comparison, we evaluate the accuracy and rate of convergence for these methods, and compare to a pseudo-likelihood based method. We conclude that a pseudo-likelihood approxima- tion to the posterior performs surprisingly well, and is the only method that scales to realistic-size problems. We demonstrate this approach for statistical protein modeling, and compare the results on a protein fold recognition experiment, where it signifi- cantly outperforms knowledge-based statistical potentials based on the ‘quasi-chemical approximation’ commonly used in structural bioinformatics. ∗Corresponding},
author = {Zhou, Xiang and Schmidler, Scott C},
file = {:home/alec/ba/papers/PottsParamEst\_BA.pdf:pdf},
keywords = {Inverse Potts Model,Pseudolikelihood},
mendeley-tags = {Inverse Potts Model,Pseudolikelihood},
number = {919},
pages = {1--32},
title = {{Bayesian Parameter Estimation in Ising and Potts Models: A Comparative Study with Applications to Protein Modeling}},
year = {2009}
}
@article{Murua2008a,
author = {Murua, Alejandro and Stanberry, Larissa and Stuetzle, Werner},
doi = {10.1198/106186008X318855},
file = {:home/alec/ba/papers/pottsModelClustering.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {alejandro murua is associate,consensus clusters,d,gene expression,ized cut,monte carlo,multiway normal-,penalized wolff algorithm,professor,superparamagnetic method},
month = sep,
number = {3},
pages = {629--658},
title = {{On Potts Model Clustering, Kernel K -Means and Density Estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186008X318855},
volume = {17},
year = {2008}
}
@article{Weigt2009,
abstract = {Understanding the molecular determinants of specificity in protein-protein interaction is an outstanding challenge of postgenome biology. The availability of large protein databases generated from sequences of hundreds of bacterial genomes enables various statistical approaches to this problem. In this context covariance-based methods have been used to identify correlation between amino acid positions in interacting proteins. However, these methods have an important shortcoming, in that they cannot distinguish between directly and indirectly correlated residues. We developed a method that combines covariance analysis with global inference analysis, adopted from use in statistical physics. Applied to a set of >2,500 representatives of the bacterial two-component signal transduction system, the combination of covariance with global inference successfully and robustly identified residue pairs that are proximal in space without resorting to ad hoc tuning parameters, both for heterointeractions between sensor kinase (SK) and response regulator (RR) proteins and for homointeractions between RR proteins. The spectacular success of this approach illustrates the effectiveness of the global inference approach in identifying direct interaction based on sequence information alone. We expect this method to be applicable soon to interaction surfaces between proteins present in only 1 copy per genome as the number of sequenced genomes continues to expand. Use of this method could significantly increase the potential targets for therapeutic intervention, shed light on the mechanism of protein-protein interaction, and establish the foundation for the accurate prediction of interacting protein partners.},
author = {Weigt, Martin and White, Robert a and Szurmant, Hendrik and Hoch, James a and Hwa, Terence},
file = {:home/alec/ba/papers/PNAS-2009-Weigt-67-72.pdf:pdf},
institution = {Center for Theoretical Biological Physics and Department of Physics, University of California at San Diego, La Jolla, CA 92093-0374, USA.},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {1},
pages = {67--72},
title = {{Identification of direct residue contacts in protein-protein interaction by message passing.}},
volume = {106},
year = {2009}
}
@article{Murua2008,
author = {Murua, Alejandro and Stanberry, Larissa and Stuetzle, Werner},
doi = {10.1198/106186008X318855},
file = {:home/alec/ba/papers/potts.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {alejandro murua is associate,consensus clusters,d,gene expression,ized cut,monte carlo,multiway normal-,penalized wolff algorithm,professor,superparamagnetic method},
month = sep,
number = {3},
pages = {629--658},
title = {{On Potts Model Clustering, Kernel K -Means and Density Estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186008X318855},
volume = {17},
year = {2008}
}
@article{Halabi2006,
author = {Halabi, Najeeb and Rivoire, Olivier and Leibler, Stanislas and Ranganathan, Rama and Procedures, I Supplemental Experimental and Alignments, Multiple Sequence},
file = {:home/alec/ba/papers/PIIS0092867409009635.mmc1.pdf:pdf},
title = {{Supplemental Data Theory Protein Sectors: Evolutionary Units of Three-Dimensional Structure}},
volume = {138},
year = {2006}
}
@article{Shlens2009,
author = {Shlens, Jonathon and City, New York and Jolla, La and Introduction, I},
file = {:home/alec/ba/papers/pca.pdf:pdf},
title = {{A Tutorial on Principal Component Analysis}},
year = {2009}
}
@article{Essamf1986,
author = {Essamf, J W},
file = {:home/alec/ba/papers/PF1.pdf:pdf},
pages = {409--422},
title = {{The Potts model and flows: I. The pair correlation function}},
volume = {19},
year = {1986}
}
@article{Hoffman2011,
abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{$\backslash$epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{$\backslash$epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
author = {Hoffman, Matthew D. and Gelman, Andrew},
file = {:home/alec/ba/papers/nuts.pdf:pdf},
journal = {arXiv},
keywords = {adaptive monte carlo,bayesian inference,dual averaging,hamiltonian monte carlo,markov chain monte carlo},
number = {2008},
pages = {30},
title = {{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1111.4246},
year = {2011}
}
@article{Master2010,
author = {Master, Charles Ollion and Stockholm, Science Thesis},
file = {:home/alec/ba/papers/ollion\_charles\_10021.pdf:pdf},
title = {{Susceptibility Propagation for the Inverse Ising Model}},
year = {2010}
}
@article{Levada2008,
author = {Levada, Alexandre L M and Mascarenhas, Nelson D A and Tann\'{u}s, Alberto and Salvadeo, Denis H P},
file = {:home/alec/ba/papers/ACMSAC2008\_MRF.pdf:pdf},
isbn = {9781595937537},
keywords = {Contextual Classificatoin, Markov Random Fields, M,contextual classification,markov random fields,maximum pseudo-likelihood,spatially non-homogeneous potts model},
pages = {1733--1737},
title = {{Spatially Non-Homogeneous Potts Model Parameter Estimation on Higher-Order Neighborhood Systems by Maximum Pseudo-Likelihood}},
year = {2008}
}
@article{Hagberg2013,
author = {Hagberg, Aric and Schult, Dan and Swart, Pieter},
file = {:home/alec/ba/papers/networkx\_reference.pdf:pdf},
title = {{NetworkX Reference}},
year = {2013}
}
@article{Mccallum2009,
abstract = {Discriminatively trained undirected graphical models have had wide empirical success, and there has been increasing interest in toolkits that ease their applica- tion to complex relational data. The power in relational models is in their repeated structure and tied parameters; at issue is how to define these structures in a pow- erful and flexible way. Rather than using a declarative language, such as SQL or first-order logic, we advocate using an imperative language to express various aspects of model structure, inference, and learning. By combining the traditional, declarative, statistical semantics of factor graphs with imperative definitions of their construction and operation, we allow the user to mix declarative and proce- dural domain knowledge, and also gain significant efficiencies. We have imple- mented such imperatively defined factor graphs in a system we call FACTORIE, a software library for an object-oriented, strongly-typed, functional language. In experimental comparisons to Markov Logic Networks on joint segmentation and coreference, we find our approach to be 3-15 times faster while reducing error by 20-25\%achieving a new state of the art.},
author = {Mccallum, Andrew and Schultz, Karl and Singh, Sameer},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C K I and Culotta, A},
file = {:home/alec/ba/papers/NIPS2009\_0857.pdf:pdf},
journal = {Neural Information Processing Systems (NIPS)},
title = {{FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs}},
url = {http://www.cs.umass.edu/~kschultz/publications/factorie-nips-2009.pdf},
year = {2009}
}
@article{Murray,
author = {Murray, Iain and Computational, Gatsby and Unit, Neuroscience and Ghahramani, Zoubin and Mackay, David J C and Skilling, John and Entropy, Maximum},
file = {:home/alec/ba/papers/nest\_nips05.pdf:pdf},
title = {{Nested sampling for Potts models}}
}
@article{Mezard2002,
abstract = {We study the satisfiability of random Boolean expressions built from many clauses with K variables per clause (K-satisfiability). Expressions with a ratio alpha of clauses to variables less than a threshold alphac are almost always satisfiable, whereas those with a ratio above this threshold are almost always unsatisfiable. We show the existence of an intermediate phase below alphac, where the proliferation of metastable states is responsible for the onset of complexity in search algorithms. We introduce a class of optimization algorithms that can deal with these metastable states; one such algorithm has been tested successfully on the largest existing benchmark of K-satisfiability.},
author = {M\'{e}zard, M and Parisi, G and Zecchina, R},
file = {:home/alec/ba/papers/Mezard.Science.297\_812.pdf:pdf},
institution = {Laboratoire de Physique Th\'{e}orique et Mod\`{e}les Statistiques, CNRS and Universit\'{e} Paris Sud, B\^{a}t. 100, 91405 Orsay Cedex, France.},
journal = {Science (New York, N.Y.)},
number = {5582},
pages = {812--815},
title = {{Analytic and algorithmic solution of random satisfiability problems.}},
volume = {297},
year = {2002}
}
@article{,
file = {:home/alec/ba/papers/nc\_grenz\_Master1314.pdf:pdf},
pages = {1--2},
title = {{No Title}},
year = {2013}
}
@article{Potts-modell,
author = {Potts-modell, Das},
file = {:home/alec/ba/papers/nathje.pdf:pdf},
title = {{Das Phasendiagramm des 3-Zustands- Pottsmodells}}
}
@article{Mezard2008,
abstract = {A new field of research is rapidly expanding at the crossroad between statistical physics, information theory and combinatorial optimization. In particular, the use of cutting edge statistical physics concepts and methods allow one to solve very large constraint satisfaction problems like random satisfiability, coloring, or error correction. Several aspects of these developments should be relevant for the understanding of functional complexity in neural networks. On the one hand the message passing procedures which are used in these new algorithms are based on local exchange of information, and succeed in solving some of the hardest computational problems. On the other hand some crucial inference problems in neurobiology, like those generated in multi-electrode recordings, naturally translate into hard constraint satisfaction problems. This paper gives a non-technical introduction to this field, emphasizing the main ideas at work in message passing strategies and their possible relevance to neural networks modeling. It also introduces a new message passing algorithm for inferring interactions between variables from correlation data, which could be useful in the analysis of multi-electrode recording data.},
archivePrefix = {arXiv},
arxivId = {0803.3061},
author = {Mezard, Marc and Mora, Thierry},
eprint = {0803.3061},
file = {:home/alec/ba/papers/Mezard\_Mora\_2008.pdf:pdf},
month = mar,
number = {March 2008},
title = {{Constraint satisfaction problems and neural networks: a statistical physics perspective}},
url = {http://arxiv.org/abs/0803.3061},
year = {2008}
}
@article{Geyer,
author = {Geyer, Charles J.},
file = {:home/alec/ba/papers/Markov chain Monte Carlo maximum likelihood..pdf:pdf},
keywords = {gibbs sampler,likelihood,markov chain,maximum,metropolis algorithm,monte carlo,vari-},
number = {1},
title = {{Markov Chain Monte Carlo Maximum Likelihood}}
}
@article{Mishkovskyi2011,
author = {Mishkovskyi, Andrii V},
file = {:home/alec/ba/papers/making-use-of-openstreetmap-data-with-python.pdf:pdf},
title = {{Using OpenStreetMap data with Python}},
year = {2011}
}
@article{Kindermann1980,
author = {Kindermann, Ross; J. Laurie Snell},
file = {:home/alec/ba/papers/marov\_random\_fields\_and\_their\_applic.pdf:pdf},
journal = {Contemporary Mathematics},
title = {{Markov Random Fields and Their Applications}},
volume = {1},
year = {1980}
}
@article{Kschischang2001a,
abstract = {Algorithms that must deal with complicated global functions of
many variables often exploit the manner in which the given functions
factor as a product of \&amp;ldquo;local\&amp;rdquo; functions, each of which
depends on a subset of the variables. Such a factorization can be
visualized with a bipartite graph that we call a factor graph, In this
tutorial paper, we present a generic message-passing algorithm, the
sum-product algorithm, that operates in a factor graph. Following a
single, simple computational rule, the sum-product algorithm
computes-either exactly or approximately-various marginal functions
derived from the global function. A wide variety of algorithms developed
in artificial intelligence, signal processing, and digital
communications can be derived as specific instances of the sum-product
algorithm, including the forward/backward algorithm, the Viterbi
algorithm, the iterative \&amp;ldquo;turbo\&amp;rdquo; decoding algorithm, Pearl's
(1988) belief propagation algorithm for Bayesian networks, the Kalman
filter, and certain fast Fourier transform (FFT) algorithms},
author = {Kschischang, F.R. and Frey, B.J. and Loeliger, H.-a.},
file = {:home/alec/ba/papers/KFL01.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {498--519},
title = {{Factor graphs and the sum-product algorithm}},
volume = {47},
year = {2001}
}
@article{Cocco2013a,
abstract = {Various approaches have explored the covariation of residues in multiple-sequence alignments of homologous proteins to extract functional and structural information. Among those are principal component analysis (PCA), which identifies the most correlated groups of residues, and direct coupling analysis (DCA), a global inference method based on the maximum entropy principle, which aims at predicting residue-residue contacts. In this paper, inspired by the statistical physics of disordered systems, we introduce the Hopfield-Potts model to naturally interpolate between these two approaches. The Hopfield-Potts model allows us to identify relevant 'patterns' of residues from the knowledge of the eigenmodes and eigenvalues of the residue-residue correlation matrix. We show how the computation of such statistical patterns makes it possible to accurately predict residue-residue contacts with a much smaller number of parameters than DCA. This dimensional reduction allows us to avoid overfitting and to extract contact information from multiple-sequence alignments of reduced size. In addition, we show that low-eigenvalue correlation modes, discarded by PCA, are important to recover structural information: the corresponding patterns are highly localized, that is, they are concentrated in few sites, which we find to be in close contact in the three-dimensional protein fold.},
author = {Cocco, Simona and Monasson, Remi and Weigt, Martin},
doi = {10.1371/journal.pcbi.1003176},
file = {:home/alec/ba/papers/journal.pcbi.1003176.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
month = jan,
number = {8},
pages = {e1003176},
pmid = {23990764},
title = {{From principal component to direct coupling analysis of coevolution in proteins: low-eigenvalue modes are needed for structure prediction.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3749948\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2013}
}
@article{Markenprodukt,
author = {Markenprodukt, Menstruation Produktauswahl and St, Produkt},
file = {:home/alec/ba/papers/KLM\_Kostenrechner\_02.pdf:pdf},
title = {{Menstruation W\"{a}scheschutz}}
}
@article{Baio1879,
author = {Baio, Gianluca and Blangiardo, Marta A},
file = {:home/alec/ba/papers/jj.pdf:pdf},
keywords = {bayesian hierarchical models,bivariate poisson distribution,football data,overshrinkage},
pages = {1--13},
title = {{Bayesian hierarchical model for the prediction of football results}},
year = {1879}
}
@article{Jaynes1957a,
author = {Jaynes, E. T.},
file = {:home/alec/ba/papers/Jaynes1.pdf:pdf},
journal = {The Physical Review},
keywords = {Maximum Entropy,Modeling},
mendeley-tags = {Maximum Entropy,Modeling},
number = {4},
pages = {620--630},
title = {{Information Theory and Statistical Mechanics}},
volume = {106},
year = {1957}
}
@article{Cocco2013b,
author = {Cocco, Simona and Monasson, R\'{e}mi and Weigt, Martin},
doi = {10.1088/1742-6596/473/1/012010},
file = {:home/alec/ba/papers/a85.pdf:pdf},
issn = {1742-6588},
journal = {Journal of Physics: Conference Series},
month = dec,
pages = {012010},
title = {{Inference of Hopfield-Potts patterns from covariation in protein families: calculation and statistical error bars}},
url = {http://stacks.iop.org/1742-6596/473/i=1/a=012010?key=crossref.621b977590d0ac796c0d937b7c215a19},
volume = {473},
year = {2013}
}
@article{Yasudaa,
author = {Yasuda, Muneki},
file = {:home/alec/ba/papers/iwi-yasuda.pdf:pdf},
title = {{Advanced Susceptibility Propagation}}
}
@article{Intel2010,
abstract = {The Recommended Reading List is a valuable resource for technical professionals who want to thoroughly explore topics such as software threading, wireless technologies, power management, and more. Dozens of industry technologists, corporate fellows, and engineers have helped by suggesting books and reviewing the list.},
author = {Intel},
file = {:home/alec/ba/papers/Intel-Recommended-Reading-List\_1H14\_0.pdf:pdf},
isbn = {9780750679589},
journal = {Wireless Networks},
pages = {1--7},
publisher = {Intel Corporation},
title = {{Recommended Reading List for Developers}},
year = {2010}
}
@article{Jaynes1957,
author = {Jaynes, E. T.},
file = {:home/alec/ba/papers/Jaynes2.pdf:pdf},
journal = {The Physical Review1},
keywords = {Information Theory,Maximum Entropy,Modeling},
mendeley-tags = {Information Theory,Maximum Entropy,Modeling},
number = {2},
pages = {171--190},
title = {{Information Theory and Statistical Mechanics. II}},
volume = {108},
year = {1957}
}
@article{Igl2006,
author = {Igl, Ferenc},
file = {:home/alec/ba/papers/Igloi.pdf:pdf},
pages = {1--17},
title = {{Random-bond Potts model for large q}},
year = {2006}
}
@article{Mccalluma,
author = {Mccallum, Andrew},
file = {:home/alec/ba/papers/iedatamining-ijcaiws03.pdf:pdf},
title = {{A Note on the Unification of Information Extraction and Data Mining using Conditional-Probability, Relational Models}}
}
@article{Halabi2009,
abstract = {Proteins display a hierarchy of structural features at primary, secondary, tertiary, and higher-order levels, an organization that guides our current understanding of their biological properties and evolutionary origins. Here, we reveal a structural organization distinct from this traditional hierarchy by statistical analysis of correlated evolution between amino acids. Applied to the S1A serine proteases, the analysis indicates a decomposition of the protein into three quasi-independent groups of correlated amino acids that we term "protein sectors." Each sector is physically connected in the tertiary structure, has a distinct functional role, and constitutes an independent mode of sequence divergence in the protein family. Functionally relevant sectors are evident in other protein families as well, suggesting that they may be general features of proteins. We propose that sectors represent a structural organization of proteins that reflects their evolutionary histories.},
author = {Halabi, Najeeb and Rivoire, Olivier and Leibler, Stanislas and Ranganathan, Rama},
doi = {10.1016/j.cell.2009.07.038},
file = {:home/alec/ba/papers/halabi+al\_09.pdf:pdf},
institution = {Department of Pharmacology, University of Texas Southwestern Medical Center, Dallas, TX 75390-9050, USA.},
issn = {1097-4172},
journal = {Cell},
keywords = {Amino Acid Sequence,Amino Acids,Amino Acids: chemistry,Amino Acids: genetics,Amino Acids: metabolism,Animals,Conserved Sequence,Enzyme Stability,Evolution, Molecular,Humans,Models, Molecular,Rats,Serine Endopeptidases,Serine Endopeptidases: chemistry,Serine Endopeptidases: genetics,Serine Endopeptidases: metabolism},
month = aug,
number = {4},
pages = {774--786},
pmid = {19703402},
publisher = {Elsevier Ltd},
title = {{Protein sectors: evolutionary units of three-dimensional structure.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3210731\&tool=pmcentrez\&rendertype=abstract},
volume = {138},
year = {2009}
}
@article{Georges1999,
abstract = {High-temperature expansions performed at a fixed-order parameter provide a simple and systematic way to derive and correct mean-field theories for statistical mechanical models. For models like spin glasses which have general couplings between spins, the authors show that these expansions generate the Thouless-Anderson-Palmer equations at low order. They explicitly calculate the corrections to TAP theory for these models. For ferromagnetic models, they show that their expansions can easily be converted into 1/d expansions around mean-field theory, where d is the number of spatial dimensions. Only a small finite number of graphs need to be calculated to generate each order in 1/d for thermodynamic quantities like free energy or magnetization. Unlike previous 1/d expansions, the expansions are valid in the low-temperature phases of the models considered. They consider alternative ways to expand around mean-field theory besides 1/d expansions. In contrast to the 1/d expansion for the critical temperature, which is presumably asymptotic, these schemes can be used to devise convergent expansions for the critical temperature. They also appear to give convergent series for thermodynamic quantities and critical exponents. They test the schemes using the spherical model, where their properties can be studied using exact expressions.},
author = {Georges, a and Yedidia, J S},
file = {:home/alec/ba/papers/How to expand around.pdf:pdf},
journal = {Journal of Physics A: Mathematical and General},
number = {9},
pages = {2173--2192},
title = {{How to expand around mean-field theory using high-temperature expansions}},
volume = {24},
year = {1999}
}
@article{Way2011a,
author = {Way, Your and Top, T O T H E},
file = {:home/alec/ba/papers/GetInsideTheirHeads (1).pdf:pdf},
title = {year of the hustle},
year = {2011}
}
@article{Henrich2008,
author = {Henrich, Andreas},
file = {:home/alec/ba/papers/henrich-ir1-1.2.pdf:pdf},
pages = {2001--2008},
title = {{Information Retrieval 1}},
volume = {2},
year = {2008}
}
@article{Hopfield1982,
author = {Hopfield, J. J.},
doi = {10.1073/pnas.79.8.2554},
file = {:home/alec/ba/papers/hopfield82.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = apr,
number = {8},
pages = {2554--2558},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.79.8.2554},
volume = {79},
year = {1982}
}
@phdthesis{Ekeberg2012,
author = {Ekeberg, Magnus},
file = {:home/alec/ba/papers/FULLTEXT01.pdf:pdf},
school = {Royal Institute of Technology, Stockholm, Sweden},
title = {{Detecting contacts in protein folds by solving the inverse Potts problem – a pseudolikelihood approach}},
url = {http://www.math.kth.se/matstat/seminarier/reports/M-exjobb12/120522.pdf},
year = {2012}
}
@article{Friedman2003,
author = {Friedman, Nir and Koller, Daphne},
file = {:home/alec/ba/papers/Friedman+Koller-MLJ03.pdf:pdf},
journal = {Machine Learning},
keywords = {abbreviations,bayesian model averaging,bayesian network,bayesian networks,bn,markov chain monte carlo,mcmc,structure learning},
number = {1-2},
pages = {95--125},
title = {{Being Bayesian About Network Structure}},
volume = {50},
year = {2003}
}
@article{Ergebnisse2013,
author = {Ergebnisse, Erste},
file = {:home/alec/ba/papers/eb80\_first\_de.pdf:pdf},
number = {December},
title = {{Die \"{o}ffentliche meinung in der europ\"{a}ischen union}},
year = {2013}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
file = {:home/alec/ba/papers/DuchiHaSi10.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
volume = {12},
year = {2011}
}
@article{Kschischang2001,
abstract = {Algorithms that must deal with complicated global functions of
many variables often exploit the manner in which the given functions
factor as a product of \&amp;ldquo;local\&amp;rdquo; functions, each of which
depends on a subset of the variables. Such a factorization can be
visualized with a bipartite graph that we call a factor graph, In this
tutorial paper, we present a generic message-passing algorithm, the
sum-product algorithm, that operates in a factor graph. Following a
single, simple computational rule, the sum-product algorithm
computes-either exactly or approximately-various marginal functions
derived from the global function. A wide variety of algorithms developed
in artificial intelligence, signal processing, and digital
communications can be derived as specific instances of the sum-product
algorithm, including the forward/backward algorithm, the Viterbi
algorithm, the iterative \&amp;ldquo;turbo\&amp;rdquo; decoding algorithm, Pearl's
(1988) belief propagation algorithm for Bayesian networks, the Kalman
filter, and certain fast Fourier transform (FFT) algorithms},
author = {Kschischang, F.R. and Frey, B.J. and Loeliger, H.-a.},
file = {:home/alec/ba/papers/KFL01.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {498--519},
title = {{Factor graphs and the sum-product algorithm}},
volume = {47},
year = {2001}
}
@article{Kruschke,
author = {Kruschke, John K},
file = {:home/alec/ba/papers/Doing Bayesian Data Analysis - A Tutorial with R and BUGS.John K. Kruschke.pdf:pdf},
title = {{No Title}}
}
@article{Mccallum1999,
abstract = {Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ's.},
author = {Mccallum, Andrew and Pereira, Fernando and Ave, Park and Park, Florham},
file = {:home/alec/ba/papers/download.pdf:pdf},
journal = {Proceedings of the Seventeenth International Conference on Machine Learning},
number = {5},
pages = {591--598},
publisher = {Citeseer},
title = {{Maximum Entropy Markov Models for Information Extraction and Segmentation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.351\&amp;rep=rep1\&amp;type=pdf},
volume = {3},
year = {1999}
}
@article{Moshiri,
author = {Moshiri, Alexander Niema},
file = {:home/alec/ba/papers/direct\_coupling\_analysis\_presentation\_\_moores\_.pdf:pdf},
title = {{Direct Coupling Analysis}}
}
@article{Weigt2011,
abstract = {The similarity in the three-dimensional structures of homologous proteins imposes strong constraints on their sequence variability. It has long been suggested that the resulting correlations among amino acid compositions at different sequence positions can be exploited to infer spatial contacts within the tertiary protein structure. Crucial to this inference is the ability to disentangle direct and indirect correlations, as accomplished by the recently introduced direct-coupling analysis (DCA). Here we develop a computationally efficient implementation of DCA, which allows us to evaluate the accuracy of contact prediction by DCA for a large number of protein domains, based purely on sequence information. DCA is shown to yield a large number of correctly predicted contacts, recapitulating the global structure of the contact map for the majority of the protein domains examined. Furthermore, our analysis captures clear signals beyond intradomain residue contacts, arising, e.g., from alternative protein conformations, ligand-mediated residue couplings, and interdomain interactions in protein oligomers. Our findings suggest that contacts predicted by DCA can be used as a reliable guide to facilitate computational predictions of alternative protein conformations, protein complex formation, and even the de novo prediction of protein domain structures, contingent on the existence of a large number of homologous sequences which are being rapidly made available due to advances in genome sequencing.},
author = {Weigt, M. and Morcos, F. and Pagnani, A. and Lunt, B. and Bertolino, A. and Marks, D. S. and Sander, C. and Zecchina, R. and Onuchic, J. N. and Hwa, T.},
doi = {10.1073/pnas.1111471108},
file = {:home/alec/ba/papers/Direct-coupling analysis of residue coevolution
captures native contacts across many protein families.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences},
keywords = {Algorithms,Amino Acids,Amino Acids: chemistry,Amino Acids: genetics,Amino Acids: metabolism,Binding Sites,Binding Sites: genetics,Computational Biology,Computational Biology: methods,Models,Molecular,Protein Binding,Protein Conformation,Protein Interaction Mapping,Protein Interaction Mapping: methods,Protein Multimerization,Proteins,Proteins: chemistry,Proteins: genetics,Proteins: metabolism,Reproducibility of Results},
month = dec,
number = {49},
pages = {E1293--E1301},
pmid = {22106262},
title = {{PNAS Plus: Direct-coupling analysis of residue coevolution captures native contacts across many protein families}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3241805\&tool=pmcentrez\&rendertype=abstract},
volume = {108},
year = {2011}
}
@article{Nowozin2012,
author = {Nowozin, Sebastian and Lampert, Christoph H},
file = {:home/alec/ba/papers/CVPR 2012 Graphical Models Introduction.pdf:pdf},
number = {June},
title = {{Part 2: Introduction to Graphical Models}},
year = {2012}
}
@article{Mccallum2003,
abstract = {Conditional random fields (CRFs) for sequence modeling have several advantages over joint models such as HMMs, including the ability to relax strong independence assumptions made in those models, and the ability to incorporate arbitrary overlapping features. Previous work has focused on linear-chain CRFs, which correspond to finite-statemachines, and have efficient exact inference algorithms. Often, however, we wish to label sequence data in multiple interacting ways—for example, performing part-of-speech tagging and noun phrase segmentation simulta- neously, increasing joint accuracy by sharing information between them. We present dynamic conditional randomfields (DCRFs),which areCRFs in which each time slice has a set of state variables and edges—a distributed state representation as in dynamic Bayesian networks—and parameters are tied across slices. (They could also be called conditionally- trained Dynamic Markov Networks.) Since exact inference can be in- tractable in these models, we perform approximate inference using the tree-based reparameterization framework (TRP). We also present empirical results comparing DCRFs with linear-chain CRFs on natural- language data.},
author = {Mccallum, Andrew and Rohanimanesh, Khashayar and Sutton, Charles},
file = {:home/alec/ba/papers/dcrf-nips03.pdf:pdf},
journal = {Neural Information and Processing Systems (NIPS)},
title = {{Dynamic Conditional Random Fields for Jointly Labeling Multiple Sequences}},
year = {2003}
}
@article{Sutton,
author = {Sutton, Charles and Mccallum, Andrew},
file = {:home/alec/ba/papers/crftutv2.pdf:pdf},
number = {xx},
pages = {1--87},
title = {{An Introduction to Conditional Random Fields}},
volume = {xx}
}
@article{Informatik,
author = {Informatik, Bachelorstudiengangs},
file = {:home/alec/ba/papers/BSc-Inf\_Abschlussarbeit.pdf:pdf},
pages = {11--12},
title = {{No Title}}
}
@article{Culotta2007,
abstract = {Traditional coreference uses features only over pairs of mentions. Here we present a conditional random field with first-order logic for expressing features, enabling features over sets of mentions. The result is a new state-of-the-art results on ACE 2004 coref, jumping from 69 to 79---a 45\% reduction in error. The advance depends crucially on a new method of parameter estimation for such "weighted logic" models based on learning rankings and error-driven training.},
author = {Culotta, a. and Wick, M. and Hall, R. and McCallum, a.},
file = {:home/alec/ba/papers/coref-hlt2007.pdf:pdf},
journal = {Proceedings of NAACL HLT},
pages = {81--88},
title = {{First-order probabilistic models for coreference resolution}},
url = {http://www.cs.umass.edu/~mccallum/papers/coref-hlt2007.pdf$\backslash$nhttp://acl.ldc.upenn.edu/N/N07/N07-1011.pdf},
year = {2007}
}
@article{Sutton2002,
author = {Sutton, Charles and Mccallum, Andrew},
file = {:home/alec/ba/papers/crf-tutorial.pdf:pdf},
title = {{1 An Introduction to Conditional Random Fields for Relational Learning}},
year = {2002}
}
@article{,
file = {:home/alec/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
title = {{Qualitative Anforderungen an wissenschaftliche Arbeiten an der TU Berlin}}
}
@article{Weigt,
author = {Weigt, Martin},
file = {:home/alec/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weigt - Unknown - Identification of direct residue contacts in protein-protein interaction by message passing. SI.pdf:pdf},
title = {{Identification of direct residue contacts in protein-protein interaction by message passing. SI}}
}
@article{Cocco2013,
author = {Cocco, Simona and Monasson, R\'{e}mi and Weigt, Martin},
doi = {10.1088/1742-6596/473/1/012010},
file = {:home/alec/ba/papers/a85.pdf:pdf},
issn = {1742-6588},
journal = {Journal of Physics: Conference Series},
month = dec,
pages = {012010},
title = {{Inference of Hopfield-Potts patterns from covariation in protein families: calculation and statistical error bars}},
url = {http://stacks.iop.org/1742-6596/473/i=1/a=012010?key=crossref.621b977590d0ac796c0d937b7c215a19},
volume = {473},
year = {2013}
}
@article{Sonnhammer1998,
author = {Sonnhammer, Erik L L and Krogh, Anders},
file = {:home/alec/ba/papers/A hidden Markov model for predicting transmembrane helices in protein
sequences.pdf:pdf},
number = {June},
title = {{A hidden Markov model for predicting transmembrane helices in protein sequences}},
year = {1998}
}
@article{Levada2008a,
author = {Levada, Alexandre L M and Mascarenhas, Nelson D A and Tann\'{u}s, Alberto and Salvadeo, Denis H P},
file = {:home/alec/ba/papers/ACMSAC2008\_MRF.pdf:pdf},
isbn = {9781595937537},
keywords = {Contextual Classificatoin, Markov Random Fields, M,contextual classification,markov random fields,maximum pseudo-likelihood,spatially non-homogeneous potts model},
pages = {1733--1737},
title = {{Spatially Non-Homogeneous Potts Model Parameter Estimation on Higher-Order Neighborhood Systems by Maximum Pseudo-Likelihood}},
year = {2008}
}
@article{Yasuda,
author = {Yasuda, Muneki},
file = {:home/alec/ba/papers/iwi-yasuda.pdf:pdf},
title = {{Advanced Susceptibility Propagation}}
}
@article{Barzel2013,
abstract = {Predictions of physical and functional links between cellular components are often based on correlations between experimental measurements, such as gene expression. However, correlations are affected by both direct and indirect paths, confounding our ability to identify true pairwise interactions. Here we exploit the fundamental properties of dynamical correlations in networks to develop a method to silence indirect effects. The method receives as input the observed correlations between node pairs and uses a matrix transformation to turn the correlation matrix into a highly discriminative silenced matrix, which enhances only the terms associated with direct causal links. Against empirical data for Escherichia coli regulatory interactions, the method enhanced the discriminative power of the correlations by twofold, yielding >50\% predictive improvement over traditional correlation measures and 6\% over mutual information. Overall this silencing method will help translate the abundant correlation data into insights about a system's interactions, with applications ranging from link prediction to inferring the dynamical mechanisms governing biological networks.},
author = {Barzel, Baruch and Barab\'{a}si, Albert-L\'{a}szl\'{o}},
doi = {10.1038/nbt.2601},
file = {:home/alec/ba/papers/201307-14\_NatureBio-Silencing.pdf:pdf},
issn = {1546-1696},
journal = {Nature biotechnology},
month = aug,
number = {8},
pages = {720--5},
pmid = {23851447},
title = {{Network link prediction by global silencing of indirect correlations.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23851447},
volume = {31},
year = {2013}
}
@article{Report1997,
author = {Report, Sandia},
file = {:home/alec/ba/papers/971925.pdf:pdf},
number = {August},
title = {{Potts-model Grain Growth Simulations: Parallel Algorithms and Applications}},
year = {1997}
}
@article{Takahashi2013,
address = {Berlin, Heidelberg},
author = {Takahashi, Yoshinori},
doi = {10.1007/978-3-642-36666-6},
file = {:home/alec/ba/papers/9783642366659-c2.pdf:pdf},
isbn = {978-3-642-36665-9},
publisher = {Springer Berlin Heidelberg},
series = {Springer Tracts in Modern Physics},
title = {{Spin Fluctuation Theory of Itinerant Electron Magnetism}},
url = {http://link.springer.com/10.1007/978-3-642-36666-6},
volume = {253},
year = {2013}
}
@article{Nemiroff,
author = {Nemiroff, Robert J and Wilson, Teresa},
file = {:home/alec/ba/papers/1312.7128v1.pdf:pdf},
title = {{Searching the Internet for evidence of time travelers}}
}
@article{Balakrishnan2011,
abstract = {We introduce a new approach to learning statistical models from multiple sequence alignments (MSA) of proteins. Our method, called GREMLIN (Generative REgularized ModeLs of proteINs), learns an undirected probabilistic graphical model of the amino acid composition within the MSA. The resulting model encodes both the position-specific conservation statistics and the correlated mutation statistics between sequential and long-range pairs of residues. Existing techniques for learning graphical models from MSA either make strong, and often inappropriate assumptions about the conditional independencies within the MSA (e.g., Hidden Markov Models), or else use suboptimal algorithms to learn the parameters of the model. In contrast, GREMLIN makes no a priori assumptions about the conditional independencies within the MSA. We formulate and solve a convex optimization problem, thus guaranteeing that we find a globally optimal model at convergence. The resulting model is also generative, allowing for the design of new protein sequences that have the same statistical properties as those in the MSA. We perform a detailed analysis of covariation statistics on the extensively studied WW and PDZ domains and show that our method out-performs an existing algorithm for learning undirected probabilistic graphical models from MSA. We then apply our approach to 71 additional families from the PFAM database and demonstrate that the resulting models significantly out-perform Hidden Markov Models in terms of predictive accuracy.},
author = {Balakrishnan, Sivaraman and Kamisetty, Hetunandan and Carbonell, Jaime G and Lee, Su-In and Langmead, Christopher James},
doi = {10.1002/prot.22934},
file = {:home/alec/ba/papers/22934\_ftp.pdf:pdf},
institution = {Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.},
issn = {1097-0134},
journal = {Proteins},
keywords = {Amino Acid Sequence,Area Under Curve,Computational Biology,Computer Graphics,Computer Simulation,Markov Chains,Models, Chemical,Models, Molecular,Models, Statistical,PDZ Domains,Protein Folding,Proteins,Proteins: chemistry,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, Protein,Structure-Activity Relationship},
month = apr,
number = {4},
pages = {1061--1078},
pmid = {21268112},
title = {{Learning generative models for protein fold families.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21268112},
volume = {79},
year = {2011}
}
@article{Xing2002,
abstract = {Pollen-mediated gene flow and the male reproductive success of wind-pollinated trees depend on the initial viability of the pollen and the changes that occur in its viability during transport in the atmosphere. The viability of Quercus robur pollen was determined before and during exposure to sunlight by in vitro germination and the fluorescein diacetate reaction (FCR) in 2002 and 2003, respectively. These experiments allowed us to calculate initial pollen viability and pollen sensitivity to sunlight. The germination test revealed a lower initial pollen viability (25-65\%) than the FCR (53- 92\%). Following 9.5 h of irradiation the viability was reduced to 75-100\% as determined by the in vitro germination test or to 40-70\% as determined by the FCR. The actual values of initial pollen viability and pollen sensitivity to sunlight were used to define a range of values for modelling pollen dispersal using the mesoscale meteorological model METRAS. The deposition patterns of viable pollen varied by as much as a factor of 14 by changing the viability parameters in the range of the observed values. This suggests significant differences in male reproductive success. Variations in initial pollen viability have stronger effects on the gene-flow pattern than do variations in pollen sensitivity to sunlight. In particular, pollen distribution throughout the local environment is shaped by the initial pollen viability, while pollen sensitivity to sunlight mainly influences long-distance pollen dispersal.},
author = {Xing, Ep and Jordan, Mi and Russell, S},
file = {:home/alec/ba/papers/1212.2512v1.pdf:pdf},
journal = {Proceedings of the Nineteenth \ldots},
pages = {583--591},
title = {{A generalized mean field algorithm for variational inference in exponential families}},
url = {http://dl.acm.org/citation.cfm?id=2100655},
year = {2002}
}
@article{Kahn2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0711.3136v1},
author = {Kahn, Jeff and Weininger, Nicholas},
doi = {10.1214/009117907000000042},
eprint = {arXiv:0711.3136v1},
file = {:home/alec/ba/papers/0711.3136.pdf:pdf},
isbn = {0091179070000},
issn = {0091-1798},
journal = {The Annals of Probability},
keywords = {60C05, 05D40, Positive association, random cluster},
month = nov,
number = {6},
pages = {2038--2043},
title = {{Positive association in the fractional fuzzy Potts model}},
url = {http://projecteuclid.org/euclid.aop/1191860414},
volume = {35},
year = {2007}
}
@article{Thdor1984,
author = {Thdor, Phystque and Sup, Ecole Normale and Cedex, Parzs},
file = {:home/alec/ba/papers/79e415143996792ef4.pdf:pdf},
pages = {431--452},
title = {{Nuclear Physics B240 [FSI2] (1984) 431-452 © North-Holland Pubhshmg Company THE SIMPLEST SPIN GLASS D J GROSS 1 and M MEZARD}},
year = {1984}
}
@article{Haggstrom1998,
author = {H\"{a}ggstr\"{o}m, Olle},
file = {:home/alec/ba/papers/50.pdf:pdf},
title = {{Positive correlations in the fuzzy Potts model}},
year = {1998}
}
@article{Assa2013,
author = {Assa, Afsoon and {Vafaei Jahan}, Majid},
doi = {10.5923/j.ajis.20120207.01},
file = {:home/alec/ba/papers/10.5923.j.ajis.20120207.01.pdf:pdf},
issn = {2165-8978},
journal = {American Journal of Intelligent Systems},
keywords = {active,an event may occur,and tracking led to,and unpredictable,ising model,markov random field,mrf,of nodes are,potts model,reduced,serious,so if operational cycles,when nodes are not,which its incorrect detection,wireless sensor networks,wsns},
month = jan,
number = {7},
pages = {157--162},
title = {{Adaptive Scheduling in Wireless Sensor Networks Based on Potts Model}},
url = {http://article.sapub.org/10.5923.j.ajis.20120207.01.html},
volume = {2},
year = {2013}
}
@article{Berche2008,
author = {Berche, Bertrand and Statistique, Groupe De Physique},
file = {:home/alec/ba/papers/08Lpzg.pdf:pdf},
number = {April},
title = {{Critical behaviour of disordered Potts models}},
year = {2008}
}
