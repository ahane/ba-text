{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Parameter Estimation in Potts Models as an Instance of Probabilistic Programming via Imperatively Defined Factor Graphs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The question of what statistical model to chose given"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2. The Ising Model\n",
      "The Ising Model is an statistical model, proposed by Ising as an explanation for ferro-magnetism in 1952 [Ising52]. Dispite, or espacially because, of its simplicity it has proved over time to be of great use in areas outside of physics. It's generalization, the Potts Model, shall be the main focus of this thesis. But because of the Ising Model's intuitive nature we will take a look at it first. \n",
      "\n",
      "Consider a two dimensional, quadratic, grid of spins $X$ with size $N$ such that each component $x_{ij}$ ($i, j \\epsilon \\{1,...,N\\})$ can take one of two values $+1$ and $-1$. In a ferro-magnetic substance. Each pair of neighbouring spins will influence each other to take on the same value, thus making homogenous states for X, where all x_{ij} have the same value, more likely. Formally we can express this as an energy function: $$E(X) = -\\sum_{i, j}J_{ij}x_ix_j$$ With $J_{i,j} = J$ if $x_i$ and $x_j$ are neighbours and $J$ being a positive real number, ie. $1$ which we might call *interaction strength*. \n",
      "\n",
      "We can further introduce a *local field* $h_i$, indicating a preference for a given $x_i$ for either of our two values. Our modified energy function thus looks like this:\n",
      "$$E(X) = -\\sum_{i, j}J_{ij}x_ix_j - \\sum_{i}h_ix_i$$\n",
      "We thus find that the energy function should have minimums, at states fo $X$ where all the terms in the first sum evalueate to $1$, which is the case if they have the same state.\n",
      "\n",
      "The probability distribution over all of the possible states of $X$ is given by the Boltzmann distribution\n",
      "$$P(X) = \\frac{1}{Z}e^{-E(X)} $$\n",
      "$$= \\frac{1}{Z}exp(\\sum_{i, j}J_{ij}x_ix_j + \\sum_{i}h_ix_i) $$\n",
      "$$= \\frac{1}{Z}\\prod_{i, j}{exp(J_{ij}x_ix_j)}\\prod_iexp(h_i)$$\n",
      "with the normalization constant or partition function Z over all possible states of $X$:\n",
      "$$Z(X, J, h) = \\sum_{X} exp(\\sum_{i, j}J_{ij}x_ix_j + \\sum_{i}h_ix_i)$$\n",
      "$$ = \\sum_{X}(\\prod_{i, j}{exp(J_{ij}x_ix_j)}\\prod_iexp(h_i))$$\n",
      "\n",
      "Computing the partition function is intractable in general. It involves enumerating all possible states of $X$ and is therefore of complexity ########. This intractactability of the partition functions is common to many complex statistical models and many approximation methods have been developed, some of which will be discussed later.\n",
      "\n",
      "Assuming that "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Protein Structure Prediction\n",
      "Computational biology and biophysics have created immense amounts of data in recent years. One such rich data source is the sequencing of proteins. A protein fundamentally is a sequence of length $N$, with each sequence member $A_i i\\epsilon\\{1,...,N\\}$ being one of $Q=20$ different amino-acids. In a biological context sequence members might be called *sites* or *positions*, amino-acids might be called *residues*. Instead of a whole protein one might also look at a *domain*, which is a building block of a larger protein, but has the same properties as just defined. The two-dimensional amino-acid sequence is folded in three-dimensional space to make up the actual protein. This three-dimensional structure is of great interest to biologists, and might be observed through expensive radio-christallography. Protein sequencing is much cheaper and data abundant, so one might wonder how to infer the three-dimensional structure from the two-dimensional sequence. This task is calls *Protein Structure Prediction* (PSP). Weight et al. [Weight08] have used statistical models and approximate inference techniques to answer a very similar question, namely the three-dimensional alignment of to different, interacting proteins. In their further work they have called this technique *Direct Coupling Analysis* (DCA). PSP and DCA are actually the same problem, as in DCA the sequences of the two proteins in question are simply concatenated and can then be treated with the same methods used for PSP. \n",
      "\n",
      "The key biological phenomenon that enables us to use statistical models on amino-acid sequences is that there are *mico-evolutions* happening within a certain domain family. A micro-evolution is when individual or multiple sites change or drop their amino-acid, while the domain preserves its general structure. To account for the dropped amino-acids, we introduce the notion of a *gap* in a sequence and expand our $Q=21$ to account for this. The output of protein sequencing over $M$ samples of the same protein is therefore not a unique sequence but $M$ different sequences which show a lot of similarities. Sites that interact in three-dimensional space are expected to show correlations across these different sequences. This is can be explained as following: if one amino-acid of an interacting pair is changed and the other is not, the domain might not be stable and not show in nature at all.\n",
      "\n",
      "Simple methods involving the covariance of sites have been used before [SOURCE] to infer these interactions. Those methodes can not differenciate between indirect correlations and direct interactions. If site $A_1$ interacts with $A_5$ and $A_{10}$, then $A_5$ and $A_{10}$ will show a high correlation even if they don't interact directly at all. Thus methods are required to mute these indirect correlations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4. The Potts Model for PSP\n",
      "We shall now examine Weigt et al.'s derivation of the Potts Model as a solution to the problem stated above.\n",
      "We want to build a global model $P(A_1,...,A_N)$ to discribe our domain's behaviour. Deriving such a model from a histogram would require at least $Q^N$ samples, to give every amino-acid the chance to appear at least once per site. Because common sizes of $M$ are in the range of $1 - 5 \\cdot 10^3$ this approach is not feasible.\n",
      "\n",
      "The question is thus; to what degree shall our model be consistant with the the observed data? Weight et al. opted for consistency in the single site- and pairwise frequencies. Thus our constraints are that the empircal frequencies are equal to the marginalized probabilities for individual and pairwise variables:\n",
      "\n",
      "\n",
      "$$f_i(A_i) \\equiv P(A_i) = \\sum_{A_k| k \\neq i}{P(A_1,...,A_N)}$$\n",
      "$$f_{ij}(A_i, A_j) \\equiv P(A_i, A_j) = \\sum_{A_k| k \\neq i, j}{P(A_1,...,A_N)}$$\n",
      "\n",
      "To account for undersampling effects the emperical local frequencies $f_i$ and pairwise frequencies $f_{ij}$ are adjusted with a pseudocount:\n",
      "$$f_i(A_i) = \\frac{1}{\\lambda Q + M}[\\lambda + \\sum_{a=1}^{M}\\delta(A_i,A_i^a)]$$\n",
      "$$f_{ij}(A_i, A_j) = \\frac{1}{\\lambda Q + M}[\\frac{\\lambda}{Q} + \\sum_{a=1}^{M}\\delta(A_i,A_i^a)\\delta(A_j,A_j^a)]$$\n",
      "With the Kronecker-delta $\\delta(a, b) = 1$ if $a=b$ and $\\delta(a, b) = 0$ otherwise.\n",
      "\n",
      "Apart from these constraints, we want the least constained model. This can be achieved by maximizing the entropy, as proposed by Jaynes in 1949. \n",
      "###4.1 The Maximum Entropy Principle\n",
      "The entropy $S$ of a distribution $P(X)$ is defined as follows:\n",
      "$$H[P(X)] = - \\sum_X{P(X)lnP(X)}$$\n",
      "Shannon famously transferred the physical quantity of entropy to probability distributions in general. It can be seen as a measure of how even the probability mass is spread out between different states of $X$. [SOURCE]\n",
      "It thus intuitively seems to make sense that, given some constraints, we would want to maximize the entropy to find the least constrained distribution as proposed by Jaynes in 1949 [SOURCE].\n",
      "###4.2 Derivation via Lagrange Optimization\n",
      "We thuse have the problem of optimizing a function given some constrains and resort to Lagrange optimization:\n",
      "\n",
      "\n",
      "###4.3 The q-State Potts Model\n",
      "The derived model is very similar to the Ising model discussed in Chapter 1. It contains several generalizations though.\n",
      "First, our \"spins\" $x_i$ can now take on any of your Q values, it is therefore called a *q-state Potts model* as this model was first proposed by Potts in 19XX [SOURCE]. In fact the complete analytical form of the Potts model looks as follows;\n",
      "$$P(X|\\beta, \\e, \\h) = \\frac{1}{Z(\\beta, \\e, \\h}exp $$\n",
      "Our second generalization is the fact that there is no grid pattern among the variable. Every variable is potentially interacting with every other variable, the model is completely connected. \n",
      "\n",
      "The Potts model has been throughly examined in statistical physics. It is used for analyzing systems, making predictions about their phase transitions and computing phase transitions. Wu [SOURCE] offers a good overview. In those classical physical applications of the parameters **e** and **h** are ususally known.\n",
      "###4.4 Parameter Estimation as the Inverse Potts Problem\n",
      "Remember that our goal is to make statements about the interactions of sites in a protein domain. We are not given the values of our model parameters **e** and **h**. Instead we want to estimate the parameters from observed date. We can then look at the parameters e_{ij} to provide a measure of the interaction strength of sites $i$ and $j$. [Cocco2013] provides a method for doing so: e_{ij} is a matrix and we want a scalar score to measure how \"large\" that matrix is.  The *Frobenius Norm* of e_{ij} is introduced for doing this: \n",
      "$$F_{ij} = \\sqrt{\\sum_{a, b = 1}^q \\tilde{e}_{ij}(a, b)^2 }$$\n",
      "with \\tiled{e}_{ij} being the transformed coupling matrices:\n",
      "$$\\tilde{e}_{ij}(a, b) = e_{ij}(a, b) - e_{ij}(\\cdot, b) + e_{ij}(a, \\cdot) + e_{ij}(\\cdot, \\cdot)\n",
      "The dot denotes averages over all values of q for the concerned position. Results in the sums over all rows and columns in $\\tilde{e}_{ij} equaling zero. Cocco et al. explain this gauging with the intuition of \"putting as much as possible\" of the statistical mass into  the local field parameters and \"as little as necessary\" into the couplings.(MAYBE average product correction as in Burger2010)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. Markov Random Fields and Factor Graphs\n",
      "###5.1 The Ising and Potts Models as natural Markov Random Fields\n",
      "###5.2 Factor Graphs\n",
      "###5.3 Conversion of MRFs into Factor Graphs\n",
      "###5.4 Message Passing and Belief Propagation\n",
      "###5.5 The Sum-Product Algorithm\n",
      "###5.6 Loopy Belief Propagation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6. Parameter Learning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##6. Imperatively Defined Factor Graphs and Probabilistic Programming\n",
      "###6.1 "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}