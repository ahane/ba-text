{
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "name": "",
  "signature": "sha256:d4ab48bd36a7e3a7b938a8b918ad9f2b66b27ba060da19dc52202654c773ed0c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Motivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It has become commonplace to hear talk about the abundance of data that is being generated each day. Of course the essential question that follows from this statement is: how do we gain actionable knowledge from this flood of data? This, among other trends, explains the incredible mass of research and the breakthroughs that have taken place in fields like machine learning, artificial intelligence and pattern recognition in the recent years. \n",
      "\n",
      "Many of the methods on which the complex probabilistic models that are in common use these days are built, have their origin in multiple fields. Fundamentals where first discussed in statistical physics, while some of the biggest breakthroughs have been contributed by coding- and information theory.<cite data-cite=\"MacKay2003\">[MacKay2003]</cite> Recently the need to gain insights from huge unstructured piles of data has seen big contributions from artificial intelligence disciplines like natural language processing and image recognition.\n",
      "Because of this interdisciplinary and somewhat parallel history of the field, practitioners might struggle to get a hold on the subject. Potential users in empirical disciplines which want to take advantage of these sophisticated tools are confronted with a huge mass of different models, methods, notations and terminologies to navigate. This means that far less data is being exploited with state-of-the art technology than would be possible otherwise.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "*Probabilistic Programming*<cite data-cite=\"Gordon2014\">[Gordon2014]</cite> promises to bridge this gap. By providing probabilistic concepts as basic constructs of a programming language, lay-people are enabled to use probabilistic models. They are freed from needing to know about implementation details and enabled to mix-and-match inference and learning algorithms to their liking.\n",
      "\n",
      "Factor graphs<cite data-cite=\"Kschischang2001a\">[Kschischang2001a]</cite>, which have their origin in coding theory, are a very general kind of graphical models. They provide us with a common way to express graphical models such as Markov Random Fields<cite data-cite=\"Kindermann1980\">[Kindermann1980]</cite>, Bayes' Nets<cite data-cite=\"Pearl1982\">[Pearl1982]</cite> and statistical physics models like the Potts Model<cite data-cite=\"Potts52\">[Potts52]</cite>.\n",
      "\n",
      "In this work we will use a Probabilistic Programming environment based on this universality of factor graphs, Factorie<cite data-cite=\"Mccallum2009\">[Mccallum2009]</cite>, to perform a *Direct Coupling Analysis* (DCA)<cite data-cite=\"Weigt2009\">[Weigt2009]</cite> on protein sequences. DCA aims to mute indirect correlations in protein sequences, in order to use the direct correlations for structure prediction. To do this we will derive a statistical model, train it using the Contrastive Divergence<cite data-cite=\"Hinton2002\">[Hinton2002]</cite> algorithm and evaluate our predictions by comparing with radio-crystallography data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Direct Coupling Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One massive data source of our time is the sequencing of proteins. A protein fundamentally is a sequence $\\mathbf{x}$ of length $N$, with each sequence member $x_i (i \\in \\{1,...,N\\})$ being one of $q=20$ different amino-acids. In a biological context sequence members are called *sites* or *positions*, amino-acids are called *residues*. Instead of a whole protein one might also look at a *domain*, which is a building block of a larger protein, but has the same properties as just defined. The one-dimensional amino-acid sequence is folded in three-dimensional space to make up the actual protein. This three-dimensional structure is of great interest to biologists, and might be observed through expensive radio-christallography. Protein sequencing is much cheaper and data abundant, so one might wonder how to infer the three-dimensional structure from the two-dimensional sequence. This task is called *Protein Structure Prediction* (PSP). \n",
      "\n",
      "The key biological phenomenon that enables us to use statistical methods on amino-acid sequences is that there are *micro-evolutions* happening within a certain domain family. A micro-evolution is when individual or multiple sites change or drop their amino-acid, while the domain preserves its general structure. To account for the dropped amino-acids, we introduce the notion of a *gap* in a sequence and expand our $q=21$ to account for this. The output of protein sequencing over $M$ samples of the same protein is therefore not a unique sequence, but up to $M$ different sequences which show a lot of similarities (called *Multiple Sequence Alignment*, MSA). Sites that interact in three-dimensional space are expected to show correlations across these different sequences. This can be explained as following: if one site out of a pair that somehow interacts with one another, is changed and the other is not, the domain might not be stable and thus not show in nature at all. \n",
      "\n",
      "Simple methods involving the co-variance of sites have been used (see for example <cite data-cite=\"Suel2003\">[Suel2003]</cite>) to infer these interactions. The problem is that these methods can not differentiate between indirect correlations and direct interactions. Two sites might show show a high correlation, simply because they both interact with a third site. An ideal method would be able to mute these indirect correlations and only present the researcher with correlations that result from direct interaction.\n",
      "\n",
      "To tackle this problem, Weigt et al.\\ <cite data-cite=\"Weigt2009\">[Weigt2009]</cite> have trained a global statistical model to represent the complex interactions between multiple amino-acids. They opted for modeling pairwise and single-site probabilities, thus employing a model known as the Potts model. The norm of the pairwise parameters is then used as a predictor for the physical distance. They call this technique *Direct Coupling Analysis* (DCA), and the best way to estimate the parameters from the MSA is an active field of research.<cite data-cite=\"Weigt2009\">[Weigt2009]</cite><cite data-cite=\"Weigt2011\">[Weigt2011]</cite><cite data-cite=\"Cocco2013\">[Cocco2013]</cite><cite data-cite=\"Ekeberg2012\">[Ekeberg2012]</cite><cite data-cite=\"Burger2010\">[Burger2010]</cite><cite data-cite=\"Feinauer14\">[Feinauer14]</cite> Some of the found techniques have been made available to practitioners, mostly in the form of MATLAB scripts.<cite data-cite=\"gplmDCA\">[gplmDAC]</cite> <cite data-cite=\"mfDCA\">[mfDCA]</cite> \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Enabling Code Reuse and Abstraction Through Probabilistic Programming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Employing Probabilistic Programming<cite data-cite=\"Gordon2014\">[Gordon2014]</cite> technologies in the DCA context could offer attractive advantages. By performing DCA in an object-oriented language, the problem can be expressed in domain specific terms instead of linear-algebra concepts.  This should lift some cognitive load off the researcher and allow him to tinker and think on a level of abstraction more closely resembling her prior education. By using the imperative programming language constructs to describe the model, she might even be able to include domain specific knowledge that would be hard or impossible (from the domain expert's perspective) to express in mathematical terms.\n",
      "Furthermore the separation of concerns between model description, model inference and model training allows for trivial reuse of implementations of algorithms. This takes further load off the domain expert. It also bears the possibility for serendipitous cross-pollination from state-of-the-art research in other fields."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Structure of this Work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To ease the understanding of the Potts model, we will start with a specialized case and historical predecessor; the Ising model. After this, some preliminaries from information theory will be introduced, as these concepts will be used later in the work.\n",
      "\n",
      "We then take a closer look at how the Potts model can be derived from our MSA data set through the maximum-entropy principle. After discussing some model details, we shall see how we turn our parameter-matrices into scalar predictions for DCA.\n",
      "\n",
      "As a lead-up to the discussion of Imparatively Defined Factor Graphs, a general introduction to factor graphs shall be given, along with notes on the algebraic and notational details. To complement this we will look at the idea of Probabilistic Programming and provide an overview of availiable systems.\n",
      "We will then proceed to introduce \\textsc{Factorie}, its background, architectural concepts and available algorithms. \n",
      "\n",
      "Next up is a close inspection of the theoretical foundations of parameter estimation and structure learning in graphical models. After explaining maximum-likelihood learning, and discussing its intractability, we will introduce Contrastive Divergence learning. Contrastive Divergence is an algorithm that was developed with intractable models in mind, and was chosen as the most promising candidate among the learning algorithms available in \\textsc{Factorie}. To conclude this part, we will give a brief overview of the optimization algorithm and regularization method that will be used alongside CD, namely AdaGrad Stochastic Gradient Descent and Regularized Dual Averaging.\n",
      "\n",
      "After having introduced the theory behind the methods that were used in our experiment, we will briefly describe its setup. Following are our empirical results, which will be compared to a naive mutual information based measure.\n",
      "\n",
      "In our conclusion we will asses our results, and answer the questions asked above, regarding the practicability of \\textsc{Factorie} for DCA.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preliminaries"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Ising Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Ising Model is a statistical model, proposed by Ising as an explanation for ferro-magnetism in 1925 <cite data-cite=\"Ising25\">[Ising25]</cite>. Despite, or especially because, of its simplicity it has proved over time to be of great use in areas outside of physics. It's generalization, the Potts Model, shall be the main focus of this thesis. But because of the Ising Model's intuitive nature we will take a look at it first. \n",
      "\n",
      "Consider a two dimensional, quadratic, grid of spins $\\mathbf{x}$ with size $N$ such that each component $x_{i}$ | $i \\in \\{1,...,N\\}$ can take one of two values $+1$ and $-1$. In a ferro-magnetic substance each pair of neighbouring spins will influence each other to take on the same value, thus making homogeneous states for $\\mathbf{x}$, where all $x_{ij}$ have the same value, more likely. Formally we can express this as an energy function: $$E(\\mathbf{x}) = -\\sum_{i, j}J_{ij}x_ix_j$$ \n",
      "With $$J_{i,j} = \\begin{cases}\n",
      "                J & \\text{if}\\ x_i, x_j\\ \\text{are neighbours}\\\\\n",
      "                0 & \\text{otherwise}\n",
      "                \\end{cases}$$\n",
      "                \n",
      "and $J$ being a positive real number, e.g.\\ $1$ which we might call *interaction strength*. \n",
      "\n",
      "We can further introduce a *local field* $h_i$, indicating a preference for a given $x_i$ for either of our two values. Our modified energy function thus looks like this:\n",
      "$$E(\\mathbf{x}) = -\\sum_{i, j}J_{ij}x_ix_j - \\sum_{i}h_ix_i$$\n",
      "We thus find that the energy function should have minimums at states of $\\mathbf{x}$ where all the terms in the first sum evaluate to $1$, which is the case if they have the same state.\n",
      "\n",
      "The probability distribution over all of the possible states of $\\mathbf{x}$ is given by the Boltzmann distribution<cite data-cite=\"boltzmann\">[boltzmann]</cite>\n",
      "$$P(\\mathbf{x}) = \\frac{1}{Z}e^{-E(\\mathbf{x})} $$\n",
      "$$= \\frac{1}{Z}\\exp(\\sum_{i, j}J_{ij}x_ix_j + \\sum_{i}h_ix_i) $$\n",
      "$$= \\frac{1}{Z}\\prod_{i, j}{\\exp(J_{ij}x_ix_j)}\\prod_i\\exp(h_ix_i)$$\n",
      "with the normalization constant or *partition function* $Z$ over all possible states of $X$:\n",
      "$$Z(\\mathbf{x}, J, h) = \\sum_{\\mathbf{x} \\ in \\Omega_X}\\exp(\\sum_{i, j}J_{ij}x_ix_j + \\sum_{i}h_ix_i)$$\n",
      "$$ = \\sum_{\\mathbf{x} \\ in \\Omega_X}(\\prod_{i, j}{\\exp(J_{ij}x_ix_j)}\\prod_i\\exp(h_ix_i))$$\n",
      "\n",
      "Computing the partition function is NP-hard in general <cite data-cite=\"Barahona82\">[Barahona82]</cite>. It involves enumerating all possible states of $\\mathbf{x}$, and is therefore of complexity $O(2^n)$ for our Ising model. This intractability of the partition function is common to many complex statistical models and many approximation methods have been developed. For our problem these approximate methods unfortunately are NP-hard as well <cite data-cite=\"Jerrum90\">[Jerrum90]</cite>. We shall therefore look at approaches which try to avoid evaluating the partition function altogether."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\\begin{figure}[ht]\n",
      "    \\centering\n",
      "    \\begin{subfigure}{0.49\\textwidth}\n",
      "        \\centering\n",
      "        \\includegraphics[width=0.78\\textwidth]{img/IsingNormal}\n",
      "        \\caption{As an undirected graphical model}\n",
      "        \\label{fig:isingnormal}\n",
      "    \\end{subfigure}\n",
      "    \\begin{subfigure}{0.49\\textwidth}\n",
      "        \\centering\n",
      "        \\includegraphics[width=0.7\\textwidth]{img/IsingFactor}\n",
      "        \\caption{As a factor graph}\n",
      "        \\label{fig:isingfactor}\n",
      "    \\end{subfigure}\n",
      "\\label{fig:ising}\n",
      "\\caption{A small Ising model and two different graphical representations}\n",
      "\\end{figure}\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Entropy and Mutual Information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition**: Entropy <cite data-cite=\"shannon\">[shannon]</cite>\n",
      "    $$H[\\mathbf{x}] = - \\sum_{\\mathbf{x} \\ in \\Omega_X}P(\\mathbf{x})lnP(\\mathbf{x})$$\n",
      "\n",
      "\n",
      "The entropy is non-negative and is $0$ if one possible realization of $\\mathbf{x} \\in \\Omega_X$  carries all the probability mass and all others $\\Omega_X \\setminus \\mathbf{x}$ none. It takes its maximum value (dependent on the magnitude $|\\Omega_X|$ of the outcome space) if all outcomes carry the same probability mass i.e.\\ the uniform distribution. It can therefore be seen as a measure of how evenly the probability mass is spread out among members of the outcome space.\n",
      "\n",
      "\n",
      "**Definition**: Kullback-Leibler divergence (KL), or relative entropy <cite data-cite=\"kullback1951\">[kullback1951]</cite>\n",
      "$$KL(P||Q) = - \\sum_{\\mathbf{x}}P(\\mathbf{x})ln[\\frac{Q(\\mathbf{x})}{P(\\mathbf{x})}]$$\n",
      "The KL divergence can be seen as a measure of how similar two distributions are. It can be said that $KL(P||Q) \\ge 0$ and $KL(P||Q) = 0$ if and only if $Q(\\mathbf{x}) = P(\\mathbf{x})$. It should be noted that in general $KL(P||Q) \\neq KL(Q||P)$\n",
      "\n",
      "\n",
      "**Definition**: Mututal Information (MI)\n",
      "$$I[\\mathbf{x}, \\mathbf{y}] = - \\sum_{\\mathbf{x}\\in \\Omega_X , \\mathbf{y} \\in \\Omega_Y} P(\\mathbf{x},\\mathbf{y}) ln[\\frac{P(\\mathbf{x})P(\\mathbf{y})}{P(\\mathbf{x},\\mathbf{y})}]$$\n",
      "\n",
      "\n",
      "MI can be also defined as the KL of the joint distribution $P(\\mathbf{x}, \\mathbf{y})$ and the product of the marginals $P(\\mathbf{x})P(\\mathbf{y})$. It becomes obvious that $I[\\mathbf{x},\\mathbf{y}] = 0$ if $\\mathbf{x}$ and $\\mathbf{y}$ are independent, that is $P(\\mathbf{x}, \\mathbf{y})=P(\\mathbf{x})P(\\mathbf{y})$, and $I[\\mathbf{x},\\mathbf{y}] > 0$ otherwise. It can be seen as a measure of how far $\\mathbf{x}$ and $\\mathbf{y}$ are from being independent.\n",
      "Another definition of MI is as the difference in entropy of $\\mathbf{x}$ and $\\mathbf{x}$ given $\\mathbf{y}$. It can then be interpreted as a measure of how much information about $\\mathbf{x}$ we gain by observing $\\mathbf{y}$. \n",
      "In contrast to the Kullback-Leibler divergence MI is symmetric, that is $I[\\mathbf{x}, \\mathbf{y}] = I[\\mathbf{y}, \\mathbf{x}]$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Potts Model for DCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We shall now take a look at Weigt et al.'s derivation of the Potts Model<cite data-cite=\"Potts52\">[Potts52]</cite> as a solution to the Protein Structue Prediction problem stated above.\n",
      "We want to build a global model $P(x_1,...,x_n)$ where each $x_i$ represents a position in the domain that can take the value of of $q=21$ different amino-acids (and the gap). The question is thus; to what degree shall our model be consistent with the the observed data? Weight et al.\\ opted for consistency in the single site- and pairwise frequencies. \n",
      "\n",
      "\n",
      "To account for under-sampling effects the empirical local frequencies $f_i$ and pairwise frequencies $f_{ij}$ are adjusted with a pseudo-count $\\rho$:\n",
      "$$f_i(x_i) = \\frac{1}{\\rho q + M}[\\rho + \\sum_{a=1}^{M}\\delta(x_i,x_i^a)]$$\n",
      "$$f_{ij}(x_i, x_j) = \\frac{1}{\\rho q + M}[\\frac{\\rho}{q} + \\sum_{a=1}^{M}\\delta(x_i,x_i^a)\\delta(x_j,x_j^a)]$$\n",
      "With the Kronecker-delta $\\delta(a, b) = 1$ if $a=b$ and $\\delta(a, b) = 0$ otherwise.\n",
      "Apart from these constraints, we want the least constrained model. This can be achieved by maximizing the entropy, as proposed by Jaynes in 1949.\n",
      "\n",
      "\n",
      "Note that our $x_i$ are now no longer scalars as in the Ising model, but rather unordered categorical values from our set of amino-acids $\\mathcal{Q}$. In order to work with these values mathematically we introduce indicator functions (or *feature functions*).\n",
      "Single amino-acids are projected into a $21$-dimensional vector space, with each one being represented by a unit vector:\n",
      "$$\\phi_{i} : \\mathcal{Q} \\to \\mathbb{R^{21}}$$\n",
      "The carthesian product of $\\mathcal{Q}$ with itself represents the space of all possible pairwise observations and is projected into a $21\\times21$-dimensional space:\n",
      "$$\\phi_{ij} : \\mathcal{Q}\\times\\mathcal{Q} \\to \\mathbb{R^{21\\times21}}$$\n",
      "$$\\phi_{ij}(x_i, x_j) = \\phi(x_i) \\otimes \\phi_j(x_j) = \\phi_i (x_i) \\phi_j(x_j)^\\top$$\n",
      "\n",
      "Note that our frequencies $f_i(x_i)$ are also $21$-dimensional vectors, with entries for the frequencies of all possible amino-acids. The pairwise frequencies $f_{ij}(x_i, x_j)$ are $21\\times21$ dimensional matrices.\n",
      "\n",
      "We want the expectations of these indicator functions to be the same as our empirical frequencies and thus formulate our constraints:\n",
      "\n",
      "$$f_i(x_i) \\equiv P(x_i)\\phi_i(x_i) = \\sum_{x_k| k \\neq i}{P(x_1,...,x_n)}\\phi_i(x_i)$$\n",
      "$$f_{ij}(x_i, x_j) \\equiv P(x_i, x_j)\\phi_{ij}(x_i, x_j) = \\sum_{x_k| k \\neq i, j}{P(x_1,...,x_n)}\\phi_{ij}(x_i, x_j)$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Maximum-Entropy Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The question of what probability distribution to assume given a limited amount of data is an old problem going back to Laplace. In 1956 Jaynes proposed the *maximum entropy principle* as a solution <cite data-cite=\"Jaynes1957a\">[Jaynes1957a]</cite>. The idea is to maximize Shannon's entropy whilst preserving constraints posed by the data. The intuition is that the probability mass thus gets spread out as evenly as possible, therefore providing us with the most unbiased prior distribution.\n",
      "Choosing a distribution thus turns into a constrained optimization problem. Our function to optimize is the entropy.\n",
      "$$\\max H[P(\\mathbf{x})] = - \\sum_{\\mathbf{x}}P(\\mathbf{x})lnP(\\mathbf{x})$$\n",
      "$$s.t. \\sum_{\\mathbf{x}\\in \\Omega_X} P(\\mathbf{x}) = 1 \\land$$\n",
      "$$f_i(x_i) = \\sum_{x_k| k \\neq i}{P(x_1,...,x_n)}\\phi_{i}(x_i) |\\forall i \\in \\{1,..., N\\} \\land$$\n",
      "$$f_{ij}(x_i, x_j) = \\sum_{x_k| k \\neq i, j}{P(x_1,...,x_n)}\\phi_{ij}(x_i, x_j) |\\forall i,j \\in \\{1,..., N\\}$$\n",
      "We can solve this optimization problem through Lagrange's method. Introducing the Lagrange multipliers $\\Theta=\\{\\theta_{1,2},...,\\theta_{ij},...,\\theta_{N-1, N}, \\theta_1,...\\theta_i...\\theta_N\\}$ and $\\lambda$ we get the Lagrangian: <cite data-cite=\"Topics2012\">[Topics2012]</cite>\n",
      "$$L[P(\\mathbf{x}), \\Theta, \\lambda)] =$$\n",
      "$$- \\sum_{\\mathbf{x} \\in \\Omega_X}P(\\mathbf{x})lnP(\\mathbf{x})\n",
      "+ \\sum_{i,j|i<j}\\theta_{ij}[\\sum_{x_k| k \\neq i, j}P(\\mathbf{x})\\phi_{ij}(x_i, x_j) - f_{ij}(x_i, x_j)]\n",
      "+ \\sum_{i}\\theta_{i}[\\sum_{x_k| k \\neq i}P(\\mathbf{x})\\phi(x_i) - f_{i}(x_i)] + \\lambda (\\sum_{\\mathbf{x}}P(\\mathbf{x})-1)$$ \n",
      "Setting the derivative with respect to $P(\\mathbf{x})$ to zero yields us the optimal distribution\n",
      "\n",
      "$$P(\\mathbf{x}) = \\frac{1}{Z(\\Theta)} \\exp[\\sum_{i,j|i<j}\\theta_{ij}\\phi_{ij}(x_i, x_j )+ \\sum_{i}\\theta_i\\phi(x_i)]$$\n",
      "$$Z(\\Theta) = \\sum_{\\mathbf{x}\\in \\Omega_X}[ \\exp(\\sum_{i,j|i<j}\\theta_{ij}\\phi_{ij}(x_i, x_j )+ \\sum_{i}\\theta_i\\phi_i(x_i))]$$\n",
      "The Lagrange factors $\\Theta$ remain as parameters of the model that have to be fitted.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "q-State Potts Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The derived model is very similar to the Ising model discussed above. It contains several generalizations though.\n",
      "First, our \"spins\" $x_i$ can now take on any of $q$ values (often called colours in the Potts model context). It is therefore called a *q-state Potts model*. Our second generalization is the fact that there is no grid pattern among the variables. Every variable is potentially interacting with every other variable, the model is completely connected. \n",
      "\n",
      "The complete analytical form of the Potts model looks as following;\n",
      "$$P(\\mathbf{x}|\\beta, \\mathbf{e}, \\mathbf{h}) = \\frac{1}{Z(\\beta, \\mathbf{e}, \\mathbf{h})} exp(-\\beta E(\\mathbf{x}, \\mathbf{e}, \\mathbf{h}))$$\n",
      "with the energy function $E$ (also called the Hamiltonian $\\mathcal{H}$ <cite data-cite=\"Weigt2009\">[Weigt2009]</cite>):\n",
      "$$E(\\mathbf{x}, \\mathbf{e}, \\mathbf{h}) = -\\sum_{i, j | i<j} e_{i, j}(x_i, x_j) - \\sum_i h_i x_i$$\n",
      "and the partition function $Z$ (or normalization constant):\n",
      "$$Z(\\beta, \\mathbf{e}, \\mathbf{h}) = \\sum_{\\mathbf{x} \\in \\Omega_X} exp[-\\beta E(\\mathbf{x}, \\mathbf{e}, \\mathbf{h})]$$\n",
      "The $\\beta = \\frac{1}{k_bT}$ is a constant which is composed of the temperature $T$ and the *Boltzmann constant* $k_b$ (which is of dimension energy divided by temperature). For our purposes we can set $\\beta=1$ and disregard it.<cite data-cite=\"Weigt2009\">[Weigt2009]</cite>\n",
      "\n",
      "The Potts model has been thoroughly examined in statistical physics. It is used for analysing systems, making predictions about their phase transitions and computing phase transitions. Wu <cite data-cite=\"Wu1982\">[Wu1982]</cite> offers a good overview. In those classical physical applications of the parameters **e** and **h** are usually known.\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\\begin{figure}[ht]\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.78\\textwidth]{img/PottsModelFactor}\n",
      "    \\caption{A completely connected Potts model in its factor graph representation (only some factors labeled)}\n",
      "    \\label{fig:pottsfactor}\n",
      "\\end{figure}\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Interpretation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that our goal is to make statements about the interactions of sites in a protein domain. We are not given the values of our model parameters **e** and **h**. Instead we want to estimate the parameters from observed data. We can then look at the parameters $e_{ij}$ to provide a measure of the interaction strength of sites $i$ and $j$. Instead of looking at a model with given parameters to make predictions about the state of our system, we estimate the model parameters to fit observed data and then interpret the parameters to gain insights.\n",
      "\n",
      "Cocco et al.\\ <cite data-cite=\"Cocco2013\">[Cocco2013]</cite> provide a method for interpreting the interaction parameters **e**. $e_{ij}$ is a matrix and we want a scalar score to measure how \"large\" that matrix is.  The *Frobenius Norm* of $e_{ij}$ is introduced for doing this: \n",
      "$$F_{ij} = \\sqrt{\\sum_{a, b = 1}^q \\tilde{e}_{ij}(a, b)^2 }$$\n",
      "with $\\tilde{e}_{ij}$ being the transformed coupling matrices:\n",
      "$$\\tilde{e}_{ij}(a, b) = e_{ij}(a, b) - e_{ij}(\\cdot, b) + e_{ij}(a, \\cdot) + e_{ij}(\\cdot, \\cdot)$$\n",
      "The dot denotes averages over all values of $q$ for the concerned position. This results in the sums over all rows and columns in $\\tilde{e}_{ij}$ equaling zero. Cocco et al.\\ explain this gauging with the intuition of \"putting as much as possible\" of the statistical mass into  the local field parameters and \"as little as necessary\" into the couplings.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Proposed Estimation Strategies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to estimate the parameters, is the key question that follows after describing a model. Before diving into the details later on, we shall give a brief overview of the different strategies devised so far.\n",
      "Weigt et al.<cite data-cite=\"Weigt2011\">[Weigt2011]</cite> started off with using a loopy-belief propagation algorithm for inference, combined with gradient descent for learning. (mpDCA)\n",
      "Three years later, a team composed mostly of the same people proposed mfDCA<cite data-cite=\"Ekeberg2012\">[Ekeberg2012]</cite> , a naive mean-field based approach, that essentially takes the inverse of the correlation matrix as an estimator, without having to resort to inference or optimization.\n",
      "Ekeberg <cite data-cite=\"Ekeberg2012\">[Ekeberg2012]</cite> uses a pseudo-likelihood as an objective that is maximized (plmDCA) with results that surpass mfDCA.\n",
      "Apart from the estimation method, data preprocessing like sample re-weighting and sub-selecting can have big impact on performance and form the other pillar of research on DCA (ie. gap-optimized plmDCA <cite data-cite=\"Feinauer14\">[Feinauer14]</cite>).\n",
      "\n",
      "It should be noted, that both mpDCA and mfDCA are performed on column-wise frequencies calculated from the MSA. plmDCA on the other hand looks at each sample individually, which we will do as well in our method elaborated below."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Imperatively Defined Factor Graphs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Factor Graphs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Factor graphs are a very general kind of graphical model proposed by Kischang in 2001 <cite data-cite=\"Kschischang2001a\">[Kschischang2001a]</cite>. They are a generalization of *Tanner graphs* <cite data-cite=\"Wiberg1996\">[Wiberg1996]</cite> and thus have their origin in coding theory. They where originally thought up to discribe a generic message passing algorithm for decoding, the *sum-product algorithm*. In the same original work Kschischang et al.\\ note that the sum-product algorithm is actually a generalization of many established message-passing algorithms, both from coding theory as well as those used in artificial intelligence (including Pearl's *belief propagation* <cite data-cite=\"Pearl1982\">[Pearl1982]</cite>, the Viterbi algorithm <cite data-cite=\"Cocco2013\">[Viterbi1967]</cite> and many others). The work can therefore be seen as a major contribution to the insight that coding theory and statistical inference form two sides of the same coin <cite data-cite=\"MacKay2003\">[MacKay2003]</cite>. Almost in parallel Aji and McEliece proposed the *generalized distributive law* which also generalizes belief propagation <cite data-cite=\"Cocco2013\">[Aji2000]</cite>. Although factor graphs had been developed as a device to describe the sum-product algorithm, they have shown to be very valuable in their own right.\n",
      "\n",
      "Factor graphs can be seen as a sort of *lingua franca* of graphical models. Bayes' nets (directed graphical models) and Markov Random Fields (undirected graphical models) can be readily transformed into Factor graphs <cite data-cite=\"factorgraph\">[factorgraph]</cite>. Markov Random Fields are an important family of graphical models, interestingly also derived from the Ising model <cite data-cite=\"Kindermann1980\">[Kindermann1980]</cite>. We could therefore show that our Potts model is also a MRF, we will not do so though as it very naturally translates into a factor graph.\n",
      "\n",
      "Common to both directed and undirected graphical models is that they encode a set of conditional independence assumptions that have to be satisfied by any factorization that might be derived from the graph. In contrast to that the structure of a factor graph does not imply any conditional independence assumptions, in fact the factors do not have any direct probabilistic interpretation.\n",
      "\n",
      "Kschischang et al.\\ define a factor graph as following <cite data-cite=\"Kschischang2001a\">[Kschischang2001a]</cite>:\n",
      "\n",
      "*Suppose that $g(x_1,...,x_n)$ factors into a product of several local functions, each having some subset of ${x_1, ...,x_n}$ as arguments; i.e., suppose that\n",
      "$$g(x_1,...,x_n) = \\prod_{j\\in J} f_j(X_j)$$\n",
      "where $J$ is a discrete index set, $X_j$ is a subset of ${x_1,...,x_n}$ and $f_j(X_j)$ is a function having the elements of $X_j$ as arguments.*\n",
      "\n",
      "*Definition:\n",
      "A factor graph is a bipartite graph that expresses the structure of the factorization above. A factor graph has a variable node for each variable $x_i$, a factor node for each local function $f_j$, and an edge connecting variable node $x_i$ to factor node $f_j$ if and only if $x_i$ is an argument of $f_j$*\n",
      "\n",
      "This definition is not constrained to probability distributions. In our case though, a factor graph shall refer to a factorization of a joint probability distribution over all components ${x_1,...x_n} \\in \\mathbf{x}$. The factors may be normalized such that the sum over all possible outcomes of $\\mathbf{x}$ is $1$.\n",
      "$$P(\\mathbf{x}) = \\prod_{j\\in J} f_j(X_j)$$\n",
      "or we may write our factorization to include a normalization constant $Z$\n",
      "$$P(\\mathbf{x}) = \\frac{1}{Z}\\prod_{j\\in J} f_j(X_j)$$ \n",
      "this does not affect our ability to draw out the factorization as a factor graph, as $Z$ is a constant and can therefore be multiplied into any of our factors and not be represented separately in the graph.\n",
      "\n",
      "If we now take a look at our model:\n",
      "$$P(\\mathbf{x}| \\mathbf{e}, \\mathbf{h}) = \\frac{1}{Z} exp[\\sum_{i, j | i<j} e_{i, j}(x_i, x_j)  \\sum_i h_i x_i]$$\n",
      "we find that it naturally factorizes into local functions of pairwise interactions and single variable preferences:\n",
      "$$P(\\mathbf{x}| \\mathbf{e}, \\mathbf{h}) = \\frac{1}{Z} \\prod_ {i, j | i<j}exp(e_{i, j}(x_i, x_j)) \\prod_i exp(h_i x_i)$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Algebraic Representation and Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One might wonder what these local functions actually look like. Factor graphs are agnostic to this, but we shall describe a specific form that we will make use of later. Let $\\phi_j$ be a sufficient statistics (we could say that $\\phi(X_j)$ is a feature function that returns a sufficient statistics). Let $\\theta_j$ be a parameter vector. Then our factor is $f_j=\\theta_j\\phi_j$.\n",
      "\n",
      "To exemplify this lets look at our pairwise factors:\n",
      "$$e_{ij}(x_i, x_j) = \\theta_{ij}\\phi(x_i, x_j)$$\n",
      "If we assume the domain $\\mathcal{X}$ of $x$ to be categorical and $|\\mathcal{X}|=3$ then we can represent $x$ as a \"one hot\" three-component vector. \n",
      "\n",
      "$$x \\in \\{\n",
      "\\left(\\begin{array}{c}\n",
      "1 \\\\\n",
      "0 \\\\\n",
      "0 \\end{array} \\right)\n",
      "\\left(\\begin{array}{c}\n",
      "0 \\\\\n",
      "1 \\\\\n",
      "0 \\end{array} \\right)\n",
      "\\left(\\begin{array}{c}\n",
      "0 \\\\\n",
      "0 \\\\\n",
      "1 \\end{array} \\right)\n",
      "\\}$$\n",
      "With this representation for the states of $x$, the feature function is the outer product. $\\phi_{ij} = x_i \\otimes x_j = x_ix_j^\\top$. Our sufficient statistic is a $|\\mathcal{X}|\\times|\\mathcal{X}|$ matrix. Then $\\theta_{ij}$ will also be a $|\\mathcal{X}|\\times|\\mathcal{X}|$ matrix. We have to introduce a scalar (or dot) product defined between two matrices, as our factors have to result in scalars. It turns out that the *Frobenius inner product* fulfils our needs. \n",
      "\n",
      "**Definition**: Frobenius inner product:\n",
      "$$A : B = \\sum_{n,m}A_{nm}B_{nm} $$\n",
      "\n",
      "A complete example:\n",
      "$$x_1 = \\left(\\begin{array}{c}\n",
      "1 \\\\\n",
      "0 \\\\\n",
      "0 \\end{array} \\right), x_2 = \\left(\\begin{array}{c}\n",
      "0 \\\\\n",
      "1 \\\\\n",
      "0 \\end{array} \\right)$$\n",
      "\n",
      "$$\\phi_{12} = x_1x_2^\\top = \\left(\\begin{array}{ccc}\n",
      "0 & 1 & 0\\\\\n",
      "0 & 0 & 0\\\\\n",
      "0 & 0 & 0\\end{array} \\right), \\theta_{12} = \\left(\\begin{array}{ccc}\n",
      "1 & 4 & 7\\\\\n",
      "2 & 5 & 8\\\\\n",
      "3 & 6 & 9\\end{array} \\right)$$\n",
      "\n",
      "$$f_{12} = \\theta_{12}\\phi_{12} = 4$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Probabilistic Programming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the success probabilistic models, and graphical models in particular, have enjoyed in machine learning, the question arises how these methods can be made available to a wider audience of domain experts. Establishing an abstraction layer between model description and inference and learning procedures seems like a reasonable approach to this problem. This idea has been called *Probabilistic Programming* and is a field of research that resides at the intersection of machine learning and language design. \n",
      "\n",
      "Gordon, Henzinger, Nori et al.\\ define  a probabilistic program as:\n",
      "\n",
      "*[...] functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of variables in a program via observations.* <cite data-cite=\"Gordon2014\">[Gordon2014]</cite>\n",
      "\n",
      "\n",
      "Goodman gives an even simpler definition:\n",
      "\n",
      "*[...] probabilistic programming languages\n",
      "extend a well-specified deterministic programming language with primitive constructs for random choice.* <cite data-cite=\"Goodman2013\">[Goodman2013]</cite>\n",
      "\n",
      "Current probabilistic programming languages could roughly be classified along two dimensions: Language paradigm (imperative, functional, logical) <cite data-cite=\"Gordon2014\">[Gordon2014]</cite> and whether they are a self-contained language including a compiler or are implemented as an extension to an established deterministic language.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\\input{./prob-pro-table}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This table is an almost comprehensive collection of probabilistic programming frameworks mentioned in <cite data-cite=\"Mccallum\">[Mccallum]</cite> and <cite data-cite=\"Gordon2014\">[Gordon2014]</cite>. It is easy to see that there is a heavy bias towards functional and logical languages. The majority of frameworks also exist in a stand-alone fashion, separated from potentially rich ecosystems of established languages. This has not been a reason for practitioners to refrain from using them of course. Especially \\textsc{Bugs} is a widely used language for Bayesian modelling.\n",
      "In contrast stand the frameworks that are extensions to all-purpose, imperative languages. Infer.NET is an extension to C# developed at Microsoft Research, currently with an academic-only license. Figaro is an extension to Scala, developed by Charles River Analytics and available under a custom open-source license. In this work we shall use \\textsc{Factorie}, also an extension to Scala. It is maintained by the Information Extraction and Synthesis Laboratory at the University of Massachusetts, Amherst and available under the Apache 2.0 licence."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Factorie"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The expressive power of factor graphs to represent both directed and undirected graphical models has motivated a team of researches around Andrew McCallum (author of NLP toolkit \\textsc{Mallet} <cite data-cite=\"MALLET\">[MALLET]</cite>) to use them as a basis for a general purpose probabilistic programming framework. <cite data-cite=\"Mccallum2009\">[Mccallum2009]</cite><cite data-cite=\"Mccallum\">[Mccallum]</cite>. Their system is called \\textsc{Factorie} (**Factor** graphs, **I**mperative, **E**xtensible) and combines multiple design decisions, many of which are motivated by McCallum et al.'s focus on NLP.\n",
      "\\textsc{Factorie} is implemented as a library of Scala<cite data-cite=\"Odersky04\">[Odersky04]</cite>, a functional, interactive (including a REPL), and object-oriented language compiling to the JVM <cite data-cite=\"Lindholm99\">[Lindholm99]</cite>. Scala stands for **sca**lable **la**nguage and was designed with parallelization and concurrency in mind. Scala has a static typing system that supports multiple inheritance through *traits*. It also allows for trivial implementation of additional operators, thus blurring the lines of libraries and *domain specific languages*. All of these language features make Scala a great fit for a probabilistic programming framework.\n",
      "\n",
      "While factor graphs offer a concise and intuitive way to grasp and reason about graphical models, their declarative nature stands in contrast to how computer programs are usually written. McCallum et al.\\ resolve this contrast through their approach of *imperatively defined factor graphs*. What they mean by this is that \\textsc{Factorie} allows the user to use the well-known concepts of a general purpose language to describe the components of a model: variables, factors, structure, sufficient statistics etc.. These imperatively defined model components also offer an abstraction layer for developing inference and learning components that work across many different models.\n",
      "\n",
      "Factorie contains modules for all sub tasks of a machine learning system, and separates them into three main steps (1) *Model Structure* (2) *Inference* and (3) *Learning*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Model Structure**\n",
      "*Variables* in Factorie are typed objects in the object-oriented language. In addition to common variable kinds like *binary*, *categorical*, *real* and *ordinal*, variables can also be *strings*, *sets*, *objects* etc.. They hold a single possible value of a random variable, not distributions. Therefore they can be seen as containers of data or a single possible world. Variables have a *domain* which are also typed objects. Changes to variables are stored in *DiffLists*, which allows for very efficient implementation of sampling algorithms, as rejected sampling steps can be easily reverted.\n",
      "\n",
      "\n",
      "The class hierarchy around factors is made up of the three distinct concepts of *families*, *templates* and *factors*. A *family* describes the general structure of a factor. I.e.\\ our factors explained above which can be separated into a weight vector and a sufficient statistics function would form a family. A *template* inherits from a family. it describes how to locate neighbouring variable nodes and can have *weights*. The choice to use *templated factors*, stems from the creators' background in NLP. Factor graphs in NLP settings often use *parameter tying*, this means that the same parameter is used in different factors. *Factors* themselves are not persistent objects, but are instead instantiated by a template whenever they are needed. In our case parameter tying and templates are not necessary, so each factor is represented by a distinct family-template.\n",
      "\n",
      "A *model* is simply a collection of templates or families (not of variables). It can be extended with the *parameters* trait, which makes all the weight vectors accesable for learning algorithms.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "**Inference**\n",
      "\\textsc{Factorie} supplies different inference algorithms, as well as a convenient architecture to implement additional ones. There are belief propagation variants for chains, trees <cite data-cite=\"Pearl1982\">[Pearl1982]</cite> and loopy graphs <cite data-cite=\"Yedidia2000\">[Yedidia2000]</cite>. Sampling algorithms include a ready-to-use Gibbs sampler <cite data-cite=\"Geman1984\">[Geman1984]</cite> as well as a Metropolis-Hastings sampler <cite data-cite=\"Metropolis53\">[Metropolis53]</cite> <cite data-cite=\"Hastings70\">[Hastings70]</cite> which has to be supplied with a proposal distribution.\n",
      "Inference results in a *summary*, which is a collection of single-variable *marginals* and a value for $Z$.\n",
      "\n",
      "**Learning**\n",
      "Different learning strategies can be employed through a choice of optimization algorithms, objective (or loss) function, regulatizations and learning rates. We find *Stochastic Gradient Descent* <cite data-cite=\"Bottou98\">[Bottou98]</cite>, *AdaGrad* <cite data-cite=\"Duchi2010\">[Duchi2010]</cite>, *RDA* <cite data-cite=\"Xiao\">[Xiao]</cite> for online-learning, and *limited-memory BFGS* <cite data-cite=\"Byrd1994\">[Byrd1994]</cite><cite data-cite=\"Byrd1996\">[Byrd1996]</cite> for batch learning. Objective functions are implemented through objects called *examples*, which are constructed out of a data point and are able to compute an objective value and a gradient. Available objectives which require inference as a sub-routine are maximizing the *likelihood* (Maximum Likelihood learning) and maximizing the *posterior* (MAP or Bayseian learning). We also find *Contrastive Divergence* <cite data-cite=\"Hinton2002\">[Hinton2002]</cite> and *SampleRank* <cite data-cite=\"Wick2008\">[Wick2008]</cite>, which optimize different objectives that don't require inference. \n",
      "Different *trainers* are available, which provide convenient combinations of the methods above and take care of thread-safe parallelization.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Parameter Estimation Methodology"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that our task is to infer the pairwise connection parameters $e_{ij}$ from  data, and use these parameters as a measure for direct dependency between variables. Wu et al.<cite data-cite=\"Wu2012\">[Wu2012]</cite> note that approaches to this problem of *structure learning* could be separated into two groups. The first approach is to search for possible structures based on local conditional independence tests <cite data-cite=\"Abbeel2006\">[Abbeel2006]</cite>. This approach foremostly tries to decide which $e_{ij} \\in \\mathbf{e}$ should exist at all i.e.\\ which $e_{ij} \\ne \\mathbf{0}$. This means that the problem is essentially a combinatorial search problem. The second approach would be to treat the problem as a $\\ell_1$ or $\\ell_2$ regularized maximum likelihood estimation of the parameters $e_{ij}$. <cite data-cite=\"Varun2006\">[Varun2006]</cite> Because the likelihood is convex, the problem reduces to a convex optimization problem. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Maximum Likelihood Estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although it poses a set of challenges, which we will explore shortly, our work here shall focus on the second approach. Weigt et al.<cite data-cite=\"Weigt2009\">[Weigt2009]</cite> use this approach without citing any prior work and might have derived it themselves. We shall follow a more formal derivation proposed in <cite data-cite=\"Varun2006\">[Varun2006]</cite>. \n",
      "Instead of separating the steps of structure learning (deciding which interactions should exist) and parameter learning (deciding how strong interactions should be) the key idea is to subsume both of them into just learning the parameters. By enforcing some penalty on the \"size\" (a norm) of the parameters.\n",
      "\n",
      "The likelihood of an outcome $x$ given a model and parameters $\\theta$ can be written as:\n",
      "$$\\mathcal(L; x) = P(x|\\theta$$\n",
      "\n",
      "If we assume that our training samples $D=\\{\\mathbf{x}^{(1)},...,\\mathbf{x}^{(m)}\\}$ to be i.i.d. (a strong assumption that doesn't necessarily hold in biological contexts) we can write the likelihood of the training dataset as following:\n",
      "$$\\mathcal{L}(\\theta; D) = \\prod_{\\mathbf{x}^{(k)} \\in D} P(\\mathbf{x}^{(k)}|\\theta) = P(D|\\theta)$$\n",
      "It is common to use the negative log-likelihood to turn the product into a sum:\n",
      "$$l(\\theta; D) =  -log\\mathcal{L}(\\theta; D) = -\\sum_{\\mathbf{x}^{(k)} \\in D} logP(\\mathbf{x}^{(k)}|\\theta)$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we now apply this to our Potts model from above: \n",
      "$$P(d|\\theta) = P(\\mathbf{x}^{(d)}| \\mathbf{e}, \\mathbf{h})$$\n",
      "$$=\\frac{1}{Z( \\mathbf{e}, \\mathbf{h})} \\prod_ {i, j | i<j}exp(e_{i, j}(x_i^{(d)}, x_j^{(d)})) \\prod_i exp(h_i x_i)$$\n",
      "\n",
      "$$l(\\mathbf{e}, \\mathbf{h}; D) = \\sum_{\\mathbf{x}^{(k)} \\in D} [logZ(\\mathbf{e}, \\mathbf{h}) - \n",
      "\\sum_{i, j| i<j}e_{i, j}(x_i^{(k)}, x_j^{(k)})-\n",
      " \\sum_{i}h_ix_i^{(k)}]$$\n",
      " $$=\\ MlogZ(\\mathbf{e}, \\mathbf{h}) - \\sum_{\\mathbf{x}^{(k)} \\in D} [ \n",
      "\\sum_{i, j| i<j}e_{i, j}(x_i^{(k)}, x_j^{(k)})+\n",
      " \\sum_{i}h_ix_i^{(k)}]$$\n",
      " because $l$ is our objective function which we want to maximize, we can do linear transformations to the right-hand side, without changing the left hand side, and write:\n",
      " $$l(\\mathbf{e}, \\mathbf{h}; D) = logZ(\\mathbf{e}, \\mathbf{h}) - \\frac{1}{M}\\sum_{\\mathbf{x}^{(k)} \\in D} [ \n",
      "\\sum_{i, j| i<j}e_{i, j}(x_i^{(k)}, x_j^{(k)})+\n",
      " \\sum_{i}h_ix_i^{(k)}]$$\n",
      "$$ = logZ(\\mathbf{e}, \\mathbf{h}) - \\langle \\sum_{i, j| i<j}e_{i, j}(x_i^{(k)}, x_j^{(k)})+\n",
      " \\sum_{i}h_ix_i^{(k)} \\rangle_{D}$$\n",
      " \n",
      "We shall from now on use our factor graph notation of $e_{ij}(x_i, x_j) = \\theta_{ij}\\phi(x_i, x_j)$ and $h_ix_i = \\theta_i\\phi(x_i)$. $\\Theta$ shall denote the set of all $\\theta_{ij}$ and $\\theta_i$.\n",
      "  $$l(\\mathbf{e}, \\mathbf{h}; D) = l(\\Theta; D)  $$\n",
      " $$= logZ(\\Theta) - \n",
      "  \\langle \n",
      "      \\sum_{i, j| i<j} \\theta_{i, j}\\phi(x_i^{(k)}, x_j^{(k)})\n",
      "      +\\sum_{i}\\theta_i \\phi(x_i^{(k)}) \\rangle_{D}$$\n",
      "This log-likelihood is a sum of convex functions, and is therefore a convex function itself. This means training our model can be seen as an unconstrained convex optimization problem. \n",
      "$$\\hat{\\Theta} = \\arg\\min l(\\Theta; D)$$\n",
      "These can be solved through gradient descen methods. The basic idea of gradient descent is to step through the function in search of a local extreme by always stepping in the direction of the gradient of the current point: <cite data-cite=\"MacKay2003\">[MacKay2003]</cite>\n",
      "$$\\Theta_{t+1} = \\Theta_{t} + \\mu \\nabla l (\\Theta_t)$$ \n",
      "\n",
      "$\\mu$ here is the step length, also called the *learning rate*. $\\mu$ can be seen as a hyperparameter of our model and should be treated accordingly. This means it should be tuned as it can affect our time to learn, the accuracy of our predictions and might even lead to pathological behaviour if chosen unwisely.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\frac{\\partial l}{\\partial\\theta_{ij}} = \n",
      "\\frac{\\partial logZ(\\Theta)}{\\partial\\theta_{ij}}- \\langle\\phi(x_i, x_j)\\rangle_{D}$$\n",
      "\n",
      "$$\\frac{\\partial l}{\\partial\\theta_{i}} = \n",
      "\\frac{\\partial logZ(\\Theta)}{\\partial\\theta_{i}}- \\langle\\phi(x_i)\\rangle_{D}$$\n",
      "\n",
      "There is a fundamental hurdle in this approach though. We have to know the value of the partial derivatives of $logZ$ which are actually the expectations of the feature functions(analogusly for $\\theta_i$):<cite data-cite=\"Woodford\">[Woodford]</cite>\n",
      "$$\\frac{\\partial logZ(\\Theta)}{\\partial\\theta_{ij}} \n",
      "=\\frac{1}{Z(\\Theta)}\\frac{\\partial Z(\\Theta)}{\\partial\\theta_{ij}}$$\n",
      "\n",
      "$$= \\frac{1}{Z(\\Theta)}\\frac{\\partial}{\\partial\\theta_{ij}}\n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} exp[E(\\mathbf{x}|\\Theta)]$$\n",
      "$$= \\frac{1}{Z(\\Theta)}\n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} exp[E(\\mathbf{x}|\\Theta)] \\phi(x_i, x_j) $$\n",
      "$$= \n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} \\frac{1}{Z(\\Theta)}exp[E(\\mathbf{x}|\\Theta)] \\phi(x_i, x_j) $$\n",
      "$$= \n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} P(\\mathbf{x}|\\Theta) \\phi(x_i, x_j) $$\n",
      "$$=\\langle \\phi(x_i, x_j) \\rangle_{P(\\mathbf{x}|\\Theta)}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To know $logZ$ or $Z$ exactly we would have to sum over the combinatorial state space of $\\mathbf{x}$ ($|\\Omega_\\mathbf{x}|=Q^N$). If our factor graph was treelike there would be methods to compute $Z$ exactly without enumerating all possible states. For the general case, including loops in the model structure, $Z$ is intractable. It happens that our model is loopy, as loopy as it could be: completely connected.\n",
      "To approximate Z, many different strategies have been devised. The best method heavily depends on the model structure, and there is still active research going on in this field. One approach would be to use a general message-passing (or belief- propagation) algorithm, which is run until a certain convergence criterion is met.<cite data-cite=\"Weigt2009\">Weigt2009</cite>  This loopy belief-propagation can be fruitful in many cases, but may take a very long time. An alternative approach is a sampling based approximation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sampling Based Approximation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The basic idea behind sampling methods (also called *Monte Carlo* methods) is simple. Assume you have a distribution $P(x)$ and you wish to compute an expectation $\\langle f(x)\\rangle_{P} = \\sum_{x \\in \\Omega x} P(x)f(x)$  of a generic function. If you can't find a closed form solution for the expectation, you can approximate it numerically. If you have access to a sufficiently big set $\\mathcal{H}$ of i.i.d. samples drawn from $P(x)$.\n",
      "$$\\mathcal{H} = \\{x^{(1)},..., x^{(L)}\\}$$\n",
      "$$\\langle f(x)\\rangle_{P_\\Theta^L} = \\frac{1}{L}\\sum_{\\mathcal{x^{(i)} \\in H}}f(x^{(i)})$$\n",
      "Note that our samples in $\\mathcal{H}$ can be synthetic samples. They have do be drawn from $P(x)$ but they don't have to be from an empirical dataset.\n",
      "We wrote the approximated expectation with respect to $P_\\Theta^L$, this denotes that we took the expectation with respect to a data distribution of $L$ samples. Because our samples are i.i.d. and because of the law of large numbers, it can be shown that $\\lim_{L \\to \\infty} P^L_\\Theta = P_\\Theta^{\\infty} = P(\\mathbf{x}|\\Theta)$ <cite data-cite=\"samplingmackay\">[samplingmackay]</cite>.\n",
      "\n",
      "How to generate these i.i.d. samples from $P(x)$ is where the actual challenge lies. We don't have access to the normalized probabilities and our original goal was to avoid enumerating the whole state space of $x$. This is where *Markov Chain Monte Carlo* (MCMC) methods come in. We shall focus on *Gibbs sampling* <cite data-cite=\"Geman1984\">[Geman1984]</cite> which is a special case of the much older and more general Matropolis-Hastings method <cite data-cite=\"Metropolis53\">[Metropolis53]</cite> <cite data-cite=\"Hastings70\">[Hastings70]</cite>.\n",
      "\n",
      "The idea common to all MCMC methods is that we produce our samples in a stochastic process in which each sample $x^{(t+1)}$ is dependent only on it's predecessor $x^{(t)}$, hence *Markov chain*. The process has to be chosen in such a way that its stationary distribution is $P(\\mathcal{x})$.\n",
      "\n",
      "Gibbs sampling is one of these processes that is intuitive and works well in high dimensional graphical models.\n",
      "Assume a multi-dimensional distribution $P(\\mathbf{x}) = P(x_1, ..., x_n)$ is given by a factor graph $P(\\mathbf{x}) = \\frac{1}{Z}\\prod_{j\\in J} f_j(X_j)$ Sampling from $P(\\mathbf{x})$ is impossible, but sampling from the conditionals distributions $P(x_i|\\{x_1,...,x_n\\} \\setminus x_i)$ is easy. If we fix all components apart from $x_i$ this is equivalent to removing all factors which don't have $x_i$ as an argument.\n",
      "\n",
      "\n",
      "$$P(x_i|\\{x_1,...,x_n\\} \\setminus x_i) = \\frac{1}{Z}\\prod_{j\\in J|x_i \\in X_j} f_j(X_j) $$\n",
      "The normalization constant of this distribution is easy to compute as we only have to sum over the $Q$ possible states of $x_i$. \n",
      "\n",
      "To generate a new sample $\\mathbf{x}^{(t+1)}$ each component $x_i$ of $\\mathbf{x}^{(t)}$ is simply updated one by one. The conditioning is done with respect to the new state of already updated components and the old state of components yet to come.\n",
      "$$x_1^{(t+1)} \\sim P(x_1^{(t)}|\\{x_2^{(t)},...,x_n^{(t)}\\})$$\n",
      "$$x_2^{(t+1)} \\sim P(x_2^{(t)}|\\{x_1^{(t+1)},x_3^{(t)}...,x_n^{(t)}\\}) etc. $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have obtained our *trace* $\\mathcal{H}$, we could compute the estimated expectations of the feature functions needed for our gradient:\n",
      "$$\\frac{\\partial l}{\\partial\\theta_{ij}} = \n",
      "\\langle \\phi_{ij}(x_i, x_j) \\rangle_{P^L} - \\langle\\phi(x_i, x_j)\\rangle_{D} $$\n",
      "\n",
      "Similar to message-passing, there are problems with this approach though. For once, there is the problem of convergence. It might take a very long time until the samples obtained through our Gibbs sampler are actually drawn from $P(\\mathbf{x})$. Diagnosing if the chain has converged is hard enough to make up its own field of research. Once the chain is converged, we have to spend even more resources on drawing a sufficiently big number of samples. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Contrastive Divergence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because of these problems in computing the partial derivatives, alternatives to ML learning have been proposed. Hinton <cite data-cite=\"Hinton2002\">[Hinton2002]</cite> suggested a method he named *Contrastive Divergence* (CD) for random fields of the form $P(\\mathbf{x}|\\mathbf{\\Theta}) = \\frac{1}{Z(\\mathbf{\\Theta})}e^{-E(\\mathbf{x, \\Theta})}$.\n",
      "\n",
      "Above we found that we need the following derivative:\n",
      "$$\\frac{\\partial l}{\\partial\\theta_{ij}} = \n",
      " \\langle \\phi_{ij}(x_i, x_j) \\rangle_{P_\\Theta^\\infty} -\\langle\\phi_{ij}(x_i, x_j)\\rangle_{P^0}$$ \n",
      "$P^0$ shall denote our data distribution, denoted above with $D$, and $P_\\Theta^\\infty = P(\\mathbf{x}|\\Theta)$. \n",
      "We tried to approximate it by running our Gibbs sampler for very many intervals $L$ and using the distribution of our trace $P_\\Theta^L$ in place of $P_\\Theta^\\infty$.\n",
      "\n",
      "Hinton's key idea is that it might be sufficient to use a small number $K$ instead of a very large $L$ to give us a general idea in which direction to walk. In fact he proposes that $L=1$ might often be enough. So the gradient used in Contrastive Divergence looks like this:\n",
      "\n",
      "$$\\frac{\\partial l}{\\partial\\theta_{ij}} = \n",
      "\\langle \\phi_{ij}(x_i, x_j) \\rangle_{P_\\Theta^K} -\\langle\\phi_{ij}((x_i, x_j)\\rangle_{P^0}$$ \n",
      "\n",
      "With $P_\\Theta^K$ being the the the data distribution generated by running $K$ steps of Gibbs sampling on our original $D$ given the current parameters $\\Theta$.\n",
      "\n",
      "In practice, these gradients are sufficient for parameter learning, without any need to look at the objective function that is being optimized. It should be noted though that Contrastive Divergence learning minimizes the difference between $KL(P^0, P_\\Theta^\\infty)$ and $KL(P^K, P_\\Theta^\\infty)$ <cite data-cite=\"Hinton2002\">[Hinton2002]</cite>. This objective function is not identical with the one ML-learning optimizes and CD-learning does not converge to the same parameter estimates. In general CD-learning shows lesser performance than ML-learning, but on average is very close to it. <cite data-cite=\"Hinton05\">[Hinton05]</cite>\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stochastic Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Combining this Contrastive Divergence gradient with the gradient descent updating term from above we get:\n",
      "$$\\Theta_{t+1} = \\Theta_{t} + \\mu (\\langle \\Phi(D) \\rangle_{P_\\Theta^K} - \\langle \\Phi(D) \\rangle_{P^0}) $$ \n",
      "\n",
      "This straightforward way to implement our Contrastive Divergence learning would use *batch gradient descent*<cite data-cite=\"Bottou98\">[Bottou98]</cite>, i.e.\\ do $K$ sampling steps on all samples in $D$ and perform the above sum to get our gradient for a given step. Summing over all training samples at each updating step can be costly though, which is why alternatives have been explored.\n",
      "\n",
      "*Stochastic gradient descent* (SGD) (also *online gradient descent*) is one of these alternatives <cite data-cite=\"Bottou98\">[Bottou98]</cite>. Instead of computing the average gradient over our training data, we randomly step through our training samples, compute a sub-gradient for each one of them and update the parameters immediately. Our update step for one sample $\\mathbf{x}^{(t)}$ thus looks like this:\n",
      "$$\\Theta_{t+1} = \\Theta_{t} + \\mu ( \\Phi(\\mathbf{x}^{(t)}_{P^K_\\Theta}) - \\Phi(\\mathbf{x}^{(t)}) ) $$ \n",
      "After all training samples have been passed over once, the process is usually repeated multiple times. Each time the order of the samples is randomly chosen (the *stochastic* aspect) and the number of these iterations is a hyperparameter of SGD learning.\n",
      "\n",
      "The learning rate $\\mu$ is another hyperparameter in SGD and can severely impact the outcome.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Adaptable Learning Rate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As Weigt et al.<cite data-cite=\"Weigt2011\">[Weigt2011]</cite> note, our protein dataset suffers from a sampling bias. Because of a necessary sub-choice of organisms to sample from, and because of *phylogenetic* relatedness between these organisms, our samples can not be seen as independent. Essentially this means that across the sequences we will see a lot of similarities, which in turn endanger swamping the actually interesting micro-mutations by sheer mass. <cite data-cite=\"Weigt2011\">[Weigt2011]</cite> resolve this issue by stepping through the sequences and reweighing them with the inverse of the count of sequences that are $>80\\%$ identical with the sequence at hand.\n",
      "\n",
      "We shall attempt to address this issue by using the \\textsc{AdaGrad} algorithm <cite data-cite=\"Duchi2011\">[Duchi2011]</cite>. In Duchi et al.\\ own words: \"[\\textsc{AdaGrad}] give[s] frequently occurring features very low learning rates and infrequent features high learning rates, where the intuition is that each time an infrequent feature is seen, the learner should \u201ctake notice.\u201d Thus, the adaptation facilitates finding and identifying very predictive but comparatively rare features.\" <cite data-cite=\"Duchi2011\">[Duchi2011]</cite>. \n",
      "In practice this means that our learner keeps a running sum of the squares of all previous gradients. The means the update for one of our features looks as follows:  <cite data-cite=\"Dyer\">[Dyer]</cite>.\n",
      "$$\\theta_{ij, t+1} = \\theta_{ij, t} - \\frac{\\mu}{\\sqrt{\\sum_{t'=1}^t g^2_{ij, t'}}}g_{ij, t}$$\n",
      "$$g_{ij, t} = \\phi(x_{ij,P^K_\\Theta}^{(t)}) - \\phi(x_{ij}^{(t)})$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regularized Dual Averaging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "As in <cite data-cite=\"Varun2006\">[Varun2006]</cite> we want to use $\\ell_1$-regularization to induce sparsity. Unfortunately classical regularization, which introduces a penalty term $\\lambda ||\\theta||_1$ to the objective function is not applicable to SGD. Xiao <cite data-cite=\"Xiao\">[Xiao]</cite> proposes a method he calls *Regularized Dual Averaging* (RDA) to enable $\\ell_1$ and $\\ell_2$ regularization in online learning settings.\n",
      "The idea of RDA is to keep a running average of the sub-gradients:\n",
      "$$\\overline{g}_{ij,t} = \\frac{1}{t}\\sum_{t'=1}^t g_{ij, t'}$$\n",
      "for each feature and only update the parameter should its absolute become bigger than the $\\ell_1$ hyperparameter $\\lambda$:\n",
      "$$\\theta_{ij, t+1} = \n",
      "    \\begin{cases}\n",
      "    0 & \\text{if} |\\overline{g}_{ij,t}| \\leq \\lambda \\\\\n",
      "    -\\text{sgn}(\\overline{g}_{ij,t}) \\mu \\sqrt{t}(|\\overline{g}_{ij,t}| - \\lambda) & \\text{otherwise}\n",
      "    \\end{cases}\n",
      "$$\n",
      "Combining this with the learning rate term from AdaGrad we get the following update step per feature:\n",
      "$$\\theta_{ij, t+1} = \n",
      "    \\begin{cases}\n",
      "    0 & \\text{if} |\\overline{g}_{ij,t}| \\leq \\lambda \\\\\n",
      "    -\\text{sgn}(\\overline{g}_{ij,t}) \\frac{\\mu}{\\sqrt{t \\sum_{t'=1}^t g^2_{ij, t'}} }(|\\overline{g}_{ij,t}| - \\lambda) & \\text{otherwise}\n",
      "    \\end{cases}\n",
      "$$\n",
      "<cite data-cite=\"Dyer\">[Dyer]</cite>\n",
      "\n",
      "As our regularization happens in a per-sample basis, the we want to take number of samples $M$ into account. Our overall hyperparameter therefore is $\\lambda = \\Lambda /M$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Experiment Setup"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Note on Choice of Method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "One of the main goals of this work is to explore the practicability of probabilistic programming with \\textsc{Factorie} in an bioinformatics context. The algorithms explained above where all pre-implemented in Factorie and were deemed the most promising and interesting combination for our problem at hand. Especially the similarity of our model with *Restricted Boltzmann Machines* and Contrastive Divergence's success in training these (see for example <cite data-cite=\"Bengio2009a\">[Bengio2009a]</cite>) was an inspiration."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modelling Details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As neighbouring (low distance $|i-j|$) amino-acids are expected to show high correlations as well and are logically close in physical space, they are except from our analysis. Following Ekeberg's [source] example we opt to only include pairs with a $|i-j| > 4$.\n",
      "\n",
      "We follow Weigt et al.'s<cite data-cite=\"Weigt09\">[Weigt09]</cite> example in only passing interaction pairs into our Contrastive Divergence, that pass a certain mutual information threshold. The threshold used in <cite data-cite=\"Weigt09\">[Weigt09]</cite> was $0.26$, but as they used corrected mutual information measure, we opted for a lower value. Based on a visual inspection of the mutual information distribution, we decided on ($0.06$)which is where visible gap in the histogram lies. Tests found thresholds of $0.0$ or $>0.08$ to impact our performance in a negative way."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\\begin{figure}[ht]\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.78\\textwidth]{img/mi-hist}\n",
      "    \\caption{The histogram of the MI of all the pairs in PF00014, PF00017 and PF00035}\n",
      "    \\label{fig:mihist}\n",
      "\\end{figure}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Another important detail is the initial values of our parameters. Here we again follow Weigt et al.\\ and initialize the pairwise parameters $\\theta_{ij}$ with zero and the local parameters $\\theta_i$ with the empirical log-frequencies $log(f_i(x_i))$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our data consists of Pfam <cite data-cite=\"Pfam\">[Pfam]</cite> multiple sequence alignments and actual distances between positions calculated from PDB <cite data-cite=\"pdb\">[pdb]</cite> radio-crystallography datasets. This dataset was originally composed by Weigt et al.<cite data-cite=\"Weigt2011\">[Weigt2011]</cite> based on the Pfam v24 and updated to match the Pfam v26 indexes by Magnus Ekeberg <cite data-cite=\"Ekeberg2012\">[Ekeberg2012]</cite>. Because of limited computational resources, we will look at domain with relatively short length: Pf00014, Pf00017 and Pf00035.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation Metric"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We will follow Ekberg's methodology and consider all position pairs $ij$ with a measured distance $< 8.5 \\AA $ as *contacts* and in the set of positives $\\mathcal{P}$.\n",
      "\n",
      "To evaluate how many of these contacts we predicted, we will order our pairwise interaction scores $F_{ij} \\in \\mathcal{F}$ descendingly and look at the true positive rates in the subset $\\mathcal{F}_n$ of the n strongest predictions.\n",
      "$$\\mathcal{TP}_n = \\mathcal{F}_n \\cup \\mathcal{P}$$\n",
      "$$TPR_n = \\frac{|\\mathcal{TP}_n|}{n}$$\n",
      "and the overall TP-Rate for all $p = |\\mathcal{P}|$ positives.\n",
      "$$TPR = \\frac{|\\mathcal{TP}_p|}{p}$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hyperparameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We have $4$ hyperparameters that have to be tuned: the learning rate $\\mu$, the $\\ell_1$ parameter $\\Lambda$, the number of steps our CD sampler should take $K$ and the number of times our SGD algorithm should pass over the dataset $N$.\n",
      "To not over-complicate our hyperparameter search, we decide on $K=1$ (the canonical Contrastive Divergence $K$) and $N=3$ (the default value in \\textsc{Factorie}.\n",
      "\n",
      "To find appropriate $\\mu$ and $\\Lambda$ we use \\textsc{Factorie}'s hyperparameter infrastructure to randomly visit the search space $100$ times. \n",
      "\n",
      "[100 trails scatterplot]?\n",
      "\n",
      "In particular we log-uniformly draw $\\mu$'s from $[10^{-3}, 10]$ and $\\Lambda$'s from $[10^{-9}, 1]$. Our target measure for selecting \"good\" values was the $TPR$. Another interesting measure would have been to find the highest $n$ for which $TPR_n=1$. \n",
      "We will use Pf00014 for this tuning and then see if the found hyperparemeters are robust enough to generate resonable predictions for Pf00017 and Pf000035.\n",
      "\n",
      "\n",
      "As expected, choice of hyperparameters severely impacted our outcomes.\n",
      "\n",
      "[show different bad ones]\n",
      "\n",
      "Out of these $100$ trails the best combination showed to be $\\mu=0.009$ and $\\Lambda=0.0035$ with a $TPR_{CD}=0.3743$ compared to $TPR_MI=0.2765$. \n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\\begin{figure}[ht]\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.60\\textwidth]{img/l10-0035_l20-0_lr0-009_it3_st1_de0-1}\n",
      "    \\caption{Our best result out of 100 trails with $\\mu=0.009$ (lr) and $\\Lambda=0.0035$ (l1)}\n",
      "    \\label{fig:firstfound}\n",
      "\\end{figure}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To find a possibly better combination in the neighbourhood of these values, $25$ additional experiments where run to explore the space around them. \n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\\begin{figure}[ht]\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.99\\textwidth]{img/grid}\n",
      "    \\caption{25 different $\\Lambda$ (l1) and $\\mu$ (lr) combinations near the found optimum $\\mu=0.009$, $\\Lambda=0.0035$}\n",
      "    \\label{fig:hypernearby}\n",
      "\\end{figure}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that variations in the hyperparameters change our predictions notably, but do not change the overall tendency which shall be discussed in a bit. The best result out of this sub-search showed to be $\\mu=0.009$ and $\\Lambda=0.006$."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\\begin{figure}[ht]\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.60\\textwidth]{img/l10-006_l20-0_lr0-007_it3_st1_de0-1}\n",
      "    \\caption{Our best result out of the sub-search, with $\\mu=0.007$ (lr) and $\\Lambda=0.006$ (l1)}\n",
      "    \\label{fig:goodresult}\n",
      "\\end{figure}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our IDF Potts model succeeded in producing reasonable contact predictions. As a general tendency we observe that our $TPR_n$ for small $n$ are worse than those of the mutual information measure. At some $n$ our CD predictions surpass the MI predictions, resulting in an overall better total $TPR$, which was about $0.1$ higher. The fact that our results where notably worse than other DCA approaches, such as mfDCA and plmDCA, could have multiple reasons. One big aspect of this shortcoming is that we didn't perform much data preprocessing. Weigt et al.\\ and Ekeberg took care of redundancy in the MSA by re-weighting of the sequences based on their similarity to the other, which we hoped to tackle through the AdaGrad algorithm. This might simply not be an appropriate method for our problem. Another aspect would be that Contrastive Divergence is ill-suited for the relatively large ($q=21$) categorical domain of our variables. [problems with cd, mackay?]\n",
      "[show mfDCA and plmDCA results]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From a software engineering perspective, we succeeded in building a high-level, object-oriented Potts model trainer. Factorie managed to supply us with efficient and thoroughly tested modules for optimization, regularization, sampling and hyperparameter tuning. There was no need for third-party software (safe for data preprocessing which was done in Python) at any of the major steps in constructing our system. The model construction itself took up the biggest chunk of time and lines-of-code for this work. The Imparatively Defined Factor Graphs framework worked well for modelling our problem, save for one cavet. The class hierarchy around factors (Families, Templates, Factors and Weights) was much to convoluted for our problem. This class design is very powerful and convenient in Natural Language Processing contexts, where factor templating and shared weights among different factors is common. In our context, where all four of the above mentioned classes had a 1:1 relationship, it caused some cognitive overhead. As our model structure was most easy to reason about in set theory concepts, the imperative nature of our model declaration was not really necessary. One of the logic-based probabilistic programming languages might have been a better fit."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}