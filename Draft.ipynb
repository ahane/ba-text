{
 "metadata": {
  "name": "",
  "signature": "sha256:5b4d0cfcf02332bc240593edf6fd6b8b45400321be82e830472f53df61aacfe8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Motivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It has become commonplace to hear talk about the abundance of data that is being generated each day. Of course the essential question that follows from this statement is: how do we gain actionable knowedge from this flood of data? This, among other trends, explains the incredible mass of research and the breakthroughs that have taken place in fields like machine learning, artifical intelligence and pattern recognition in the recent years. \n",
      "\n",
      "Many of the method on which the complex probabilistic models that are in common use these days are built, have their origin in multiple fields. Fundamentals where first discussed in statistical physics, while some of the biggest breakthroughs have been contributed by codeing- and information theory. Recently the need to gain insights from huge unstructued piles of data has seen big contributions from artificial intelligence diciplines like natural language processing and image recognition.\n",
      "Because of this interdiciplinary and somewhat parallel history of the field, practitioners might struggle to get a hold on the subject. Potential users in empirical disciplines which want to take advantage of these sophisticated tools are confronted with a huge mass of different models, methods, notations and terminologies to navigate. This means that far less data is being exploited with state-of-the art technology than would be possible otherwise.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "*Probabilistic Programming* promises to bridge this gap. It allows lay-people to create probabilistic models without having to worry too much about the computational details of how train the model or do inference on it.\n",
      "Factor Graphs, which have their origin in coding theory, have been called been called the *lingua franca* of graphical models. They enable us a common way to express graphical models such as Markov Random Fields, Bayes' Nets and statistical physics models like the Potts Model.\n",
      "This work explores the practicability of a Probabilistic Programming environment based on this universality of Factor Graphs. Factorie, with it's paradigm of Imparatively Defined Factor Graphs, was built on top of the Scala language and offers building blocks for model description, training and inference. \n",
      "In particular we take a look at a use-case from computational biology called Direct Coupling Analysis. Here we train a Potts Model from protein sequences to predict pairwise statistical dependencies between amino-acidsgain. This allows us to gain insights about the 3D-structure of the protein. By trainig the model with a regularizer, we intend to get sparse parameter estimates in which many entries equal zero. This allows us to use the norm of the pairwise parameters as a predictor for our interactions. After discussing some of the problems with classical training methods for this kind model, we proceed in laying out our training strategy. We emply Contrastive Divergence to estimate gradients which are used in an AdaGrad Stochastic Gradient Descent optimizer. Regularization in an SGD setting is enabled through Regularized Dual Averaging.\n",
      "We show that our method performs better in predicting contacts in  amino-acids than a naive approach based on mutual information. While our performances don't reach the levels of most recent research on DCA, we succeed in demonstrating the practicability of Factorie for computational biologie."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Direct Coupling Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computational biology and biophysics have created immense amounts of data in recent years. One such rich data source is the sequencing of proteins. A protein fundamentally is a sequence $\\mathbf{x}$ of length $N$, with each sequence member $x_i (i \\in \\{1,...,N\\})$ being one of $q=20$ different amino-acids. In a biological context sequence members might be called *sites* or *positions*, amino-acids might be called *residues*. Instead of a whole protein one might also look at a *domain*, which is a building block of a larger protein, but has the same properties as just defined. The two-dimensional amino-acid sequence is folded in three-dimensional space to make up the actual protein. This three-dimensional structure is of great interest to biologists, and might be observed through expensive radio-christallography. Protein sequencing is much cheaper and data abundant, so one might wonder how to infer the three-dimensional structure from the two-dimensional sequence. This task is calls *Protein Structure Prediction* (PSP). <cite data-cite=\"Weigt2009\">Weigt et al.</cite> have used statistical models and approximate inference techniques to answer a very similar question, namely the three-dimensional alignment of two different, coupled proteins. In their further work they have called this technique *Direct Coupling Analysis* (DCA). PSP and DCA are actually the same problem, as in DCA the sequences of the two proteins in question are simply concatenated and can then be treated with the same methods used for PSP. \n",
      "\n",
      "The key biological phenomenon that enables us to use statistical models on amino-acid sequences is that there are *micro-evolutions* happening within a certain domain family. A micro-evolution is when individual or multiple sites change or drop their amino-acid, while the domain preserves its general structure. To account for the dropped amino-acids, we introduce the notion of a *gap* in a sequence and expand our $q=21$ to account for this. The output of protein sequencing over $M$ samples of the same protein is therefore not a unique sequence but $M$ different sequences which show a lot of similarities. Sites that interact in three-dimensional space are expected to show correlations across these different sequences. This is can be explained as following: if one amino-acid of an= pair that somehow interact with on each other, be it functionally or structurally, is changed and the other is not, the domain might not be stable and thus not show in nature at all.\n",
      "\n",
      "Simple methods involving the covariance of sites have been used before (see for example [Suel2003]) to infer these interactions. Those methodes can not differenciate between indirect correlations and direct interactions. If site $x_1$ interacts with $x_5$ and $x_{10}$, then $x_5$ and $x_{10}$ will show a high correlation even if they don't interact directly at all. Thus methods are required to mute these indirect correlations.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preliminiaries"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Ising Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Ising Model is a statistical model, proposed by Ising as an explanation for ferro-magnetism in 1952 [Ising52]. Dispite, or espacially because, of its simplicity it has proved over time to be of great use in areas outside of physics. It's generalization, the Potts Model, shall be the main focus of this thesis. But because of the Ising Model's intuitive nature we will take a look at it first. \n",
      "\n",
      "Consider a two dimensional, quadratic, grid of spins $\\mathbf{x}$ with size $N$ such that each component $x_{ij}$ | $i, j \\in \\{1,...,N\\}$ can take one of two values $+1$ and $-1$. In a ferro-magnetic substance each pair of neighbouring spins will influence each other to take on the same value, thus making homogenous states for $X$, where all $x_{ij}$ have the same value, more likely. Formally we can express this as an energy function: $$E(\\mathbf{x}) = -\\sum_{i, j}J_{ij}x_ix_j$$ \n",
      "With $$J_{i,j} = \\begin{cases}\n",
      "                J & \\text{if} x_i, x_j \\text{are neighbours}\\\\\n",
      "                0 & \\text{otherwise}\n",
      "                \\end{cases}$$\n",
      "                \n",
      "and $J$ being a positive real number, ie. $1$ which we might call *interaction strength*. \n",
      "\n",
      "We can further introduce a *local field* $h_i$, indicating a preference for a given $x_i$ for either of our two values. Our modified energy function thus looks like this:\n",
      "$$E(\\mathbf{x}) = -\\sum_{i, j}J_{ij}x_ix_j - \\sum_{i}h_ix_i$$\n",
      "We thus find that the energy function should have minimums at states of $X$ where all the terms in the first sum evaluate to $1$, which is the case if they have the same state.\n",
      "\n",
      "The probability distribution over all of the possible states of $X$ is given by the Boltzmann distribution [MacKay]\n",
      "$$P(\\mathbf{x}) = \\frac{1}{Z}e^{-E(\\mathbf{x})} $$\n",
      "$$= \\frac{1}{Z}exp(\\sum_{i, j}J_{ij}x_ix_j + \\sum_{i}h_ix_i) $$\n",
      "$$= \\frac{1}{Z}\\prod_{i, j}{exp(J_{ij}x_ix_j)}\\prod_iexp(h_i)$$\n",
      "with the normalization constant or *partition function* $Z$ over all possible states of $X$:\n",
      "$$Z(\\mathbf{x}, J, h) = \\sum_{\\mathbf{x}} exp(\\sum_{i, j}J_{ij}x_ix_j + \\sum_{i}h_ix_i)$$\n",
      "$$ = \\sum_{\\mathbf{x}}(\\prod_{i, j}{exp(J_{ij}x_ix_j)}\\prod_iexp(h_ix_i))$$\n",
      "\n",
      "Computing the partition function is NP-hard in general. [Barahona82] It involves enumerating all possible states of $\\mathbf{x}$, and is therefore of complexity $O(2^n)$ for our Ising model. This intractactability of the partition function is common to many complex statistical models and many approximation methods have been developed. For our problem these approximate methods unfortunately are NP-hard aswell [Jerrum90]. We shall therefore look at approaches which try to avoid evaluating the partition function alltogether."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Entropy and Mutual Information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition**: Entropy\n",
      "\n",
      "$$H[\\mathbf{x}] = - \\sum_{\\mathbf{x}}P\\mathbf{x})lnP(\\mathbf{x})$$\n",
      "\n",
      "The entropy is non-negative and is $0$ if one possible realization of $\\mathbf{x} \\in \\Omega_X$  carries all the probability mass and all others $\\Omega_X \\setminus \\mathbf{x}$ none. It takes its maximum value (dependent on the magnitude $|\\Omega_X|$ of the outcome space) if all outcomes carry the same probability mass ie. the uniform distribution. It can therefore be seen as a measure of how evenly the probability mass is spread out among members of the outcome space.\n",
      "\n",
      "**Definition**: Kullback-Leibler divergence, or relative entropy [KL51]\n",
      "$$KL(P||Q) = - \\sum_{\\mathbf{x}}P(\\mathbf{x})ln[\\frac{Q(\\mathbf{x})}{P(\\mathbf{x})}]$$\n",
      "The KL divergence can be seen as a measure of how similar two distributions are. It can be said that $KL(P||Q) \\ge 0$ and $KL(P||Q) = 0$ if and only if $Q(\\mathbf{x}) = P(\\mathbf{x})$. It should be noted that in general $KL(P||Q) \\neq KL(Q||P)$\n",
      "\n",
      "\n",
      "**Definition**: Mututal Information\n",
      "$$I[\\mathbf{x}, \\mathbf{y}] = - \\sum_{\\mathbf{x}, \\mathbf{x}} P(\\mathbf{x},\\mathbf{y}) \\frac{P(\\mathbf{x})P(\\mathbf{y})}{P(\\mathbf{x},\\mathbf{y})} = H[\\mathbf{X}] - H[\\mathbf{x}|\\mathbf{Y}] = H[\\mathbf{y}] - H[\\mathbf{y}|\\mathbf{x}]$$\n",
      "\n",
      "\n",
      "MI be defined as the KL of the joint distribution $P(\\mathbf{x}, \\mathbf{y})$ and the product of the marginals $P(\\mathbf{x})P(\\mathbf{y})$. It becomes obvious that $I[\\mathbf{x},\\mathbf{y}] = 0$ if $\\mathbf{x}$ and $\\mathbf{Y}$ are independent, that is $P(\\mathbf{x}, \\mathbf{y})=P(\\mathbf{x})P(\\mathbf{y})$, and $I[\\mathbf{x},\\mathbf{y}] > 0$ otherwise. It can be seen as a measure of how far $\\mathbf{x}$ and $\\mathbf{y}$ are from being independent.\n",
      "Another definition of MI is as the difference in entropy of $\\mathbf{x}$ and $\\mathbf{x}$ given $\\mathbf{y}$. It can then be interpreted as a measure of how much information about $\\mathbf{x}$ we gain by observing $\\mathbf{y}$. \n",
      "In contrast to the Kullback-Leibler divergence MI is symmetric, that is $I[\\mathbf{x}, \\mathbf{y}] = I[\\mathbf{y}, \\mathbf{x}]$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Potts Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We shall now examine Weigt et al.'s derivation of the Potts Model [Potts52] as a solution to the Protein Structue Prediction problem stated above.\n",
      "We want to build a global model $P(x_1,...,x_n)$ to discribe our domain's behaviour. Deriving such a model from a histogram would require at least $q^L$ samples, to give every amino-acid the chance to appear at least once per site. Because common sizes of $M$ are in the range of $1 - 5 \\cdot 10^3$ [source] this approach is not feasible.\n",
      "\n",
      "The question is thus; to what degree shall our model be consistant with the the observed data? Weight et al. opted for consistency in the single site- and pairwise frequencies. Thus our constraints are that the empircal frequencies are equal to the marginalized probabilities for individual and pairwise variables:\n",
      "\n",
      "\n",
      "$$f_i(x_i) \\equiv P(x_i) = \\sum_{x_k| k \\neq i}{P(x_1,...,x_n)}$$\n",
      "$$f_{ij}(x_i, x_j) \\equiv P(x_i, x_j) = \\sum_{x_k| k \\neq i, j}{P(x_1,...,x_n)}$$\n",
      "\n",
      "To account for undersampling effects the emperical local frequencies $f_i$ and pairwise frequencies $f_{ij}$ are adjusted with a pseudocount:\n",
      "$$f_i(x_i) = \\frac{1}{\\lambda Q + M}[\\lambda + \\sum_{a=1}^{M}\\delta(x_i,x_i^a)]$$\n",
      "$$f_{ij}(x_i, x_j) = \\frac{1}{\\lambda Q + M}[\\frac{\\lambda}{Q} + \\sum_{a=1}^{M}\\delta(x_i,x_i^a)\\delta(x_j,x_j^a)]$$\n",
      "With the Kronecker-delta $\\delta(a, b) = 1$ if $a=b$ and $\\delta(a, b) = 0$ otherwise.\n",
      "Apart from these constraints, we want the least constained model. This can be achieved by maximizing the (information theory) entropy, as proposed by Jaynes in 1949."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Max-Ent Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The question of what probability distribution to assume given a limited amount of data is an old problem going back to Laplace. In 1956 Jaynes proposed the *maximum entropy principle* as a solution [Jaynes1957a]. The idea is to maximize Shannon's entropy whilst preserving constraints posed by the data. The intuition is that the probability mass thus gets spread out as evenly as possible, therefore providing us with the most un-biased prior distribution.\n",
      "Chosing a distribution thus turns into a constrained optimization problem. Our function to optimize is the entropy.\n",
      "$$\\max H[P(\\mathbf{x})] = - \\sum_{\\mathbf{x}}P(\\mathbf{x})lnP(\\mathbf{x})$$\n",
      "$$s.t. \\sum_{\\mathbf{x}\\in \\Omega_X} P(\\mathbf{x}) = 1 \\land$$\n",
      "$$f_i(x_i) = \\sum_{x_k| k \\neq i}{P(x_1,...,x_n)} |\\forall i \\in \\{1,..., N\\} \\land$$\n",
      "$$f_{ij}(x_i, x_j) = \\sum_{x_k| k \\neq i, j}{P(x_1,...,x_n)} |\\forall i,j \\in \\{1,..., N\\}$$\n",
      "We can solve this optimization problem through Lagrange's method. Introducing the Lagrange multipliers $\\mathbf{e}=\\{e_{1,2},...,e_{ij},...,e_{N-1, N}\\}$, $\\mathbf{h} = \\{h_1,...h_N\\}$ and $\\lambda$ we get the Lagrangian: [Topics2012]\n",
      "$$L[P(\\mathbf{x}), \\mathbf{e}, \\mathbf{h}, \\lambda)] =$$\n",
      "$$- \\sum_{\\mathbf{x} \\in \\Omega_X}P(\\mathbf{x})lnP(\\mathbf{x})\n",
      "+ \\sum_{i,j|i<j}e_{ij}[\\sum_{x_k| k \\neq i, j}P(\\mathbf{x}) - f_{ij}(x_i, x_j)]\n",
      "+ \\sum_{i}h_{i}[\\sum_{x_k| k \\neq i}P(\\mathbf{x}) - f_{i}(x_i)] + \\lambda (\\sum_{\\mathbf{x}}P(\\mathbf{x})-1)$$ \n",
      "Setting the derivative with respect to $P(\\mathbf{x})$ to zero yields us the optimal distribution\n",
      "\n",
      "$$P(\\mathbf{x}) = \\frac{1}{Z(\\mathbf{e}, \\mathbf{h})} exp(\\sum_{i,j|i<j}e_{ij}(x_i, x_j )+ \\sum_{i}h_i(x_i))$$\n",
      "$$Z(\\mathbf{e}, \\mathbf{h}) = \\sum_{\\mathbf{x}\\in \\Omega_X}[ exp(\\sum_{i,j|i<j}e_{ij}(x_i, x_j )+ \\sum_{i}h_i(x_i))]$$\n",
      "The Lagrange factors $\\mathbf{e}$ and $\\mathbf{h}$ remain as parameters of the model that have to be fitted.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "q-State Potts Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The derived model is very similar to the Ising model discussed above. It contains several generalizations though.\n",
      "First, our \"spins\" $x_i$ can now take on any of q values (often called colors in the Potts Model context). It is therefore called a *q-state Potts model*. Our second generalization is the fact that there is no grid pattern among the variables. Every variable is potentially interacting with every other variable, the model is completely connected. \n",
      "\n",
      "The complete analytical form of the Potts model looks as following;\n",
      "$$P(\\mathbf{x}|\\beta, \\mathbf{e}, \\mathbf{h}) = \\frac{1}{Z(\\beta, \\mathbf{e}, \\mathbf{h})} exp(-\\beta E(\\mathbf{x}, \\mathbf{e}, \\mathbf{h}))$$\n",
      "with the energy function $E$ (also called the Hamiltonian $\\mathcal{H}$ [Weigt2008]):\n",
      "$$E(\\mathbf{x}, \\mathbf{e}, \\mathbf{h}) = \\sum_{i, j | i<j} e_{i, j}(x_i, x_j) + \\sum_i h_i x_i$$\n",
      "and the partition function $Z$ (or normalization constant):\n",
      "$$Z(\\beta, \\mathbf{e}, \\mathbf{h}) = \\sum_{\\mathbf{x} \\in \\Omega_X} exp[-\\beta E(\\mathbf{x}, \\mathbf{e}, \\mathbf{h})]$$\n",
      "The $\\beta = \\frac{1}{k_bT}$ is a constant which is composed of the temperature $T$ and the *Boltzmann constant* $k_b$ (which is of dimension energy divided by temperature). For our purposes we can set $\\beta=1$ and disregard it.[Weigt2008]\n",
      "\n",
      "The Potts model has been throughly examined in statistical physics. It is used for analyzing systems, making predictions about their phase transitions and computing phase transitions. Wu [SOURCE] offers a good overview. In those classical physical applications of the parameters **e** and **h** are ususally known.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Interpretation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that our goal is to make statements about the interactions of sites in a protein domain. We are not given the values of our model parameters **e** and **h**. Instead we want to estimate the parameters from observed data. We can then look at the parameters $e_{ij}$ to provide a measure of the interaction strength of sites $i$ and $j$. Instead of looking at a model with given parameters to make predictions about the state of our system, we estimate the model parameters to fit observed data and then interpret the parameters to gain insights.\n",
      "\n",
      "[Cocco2013] provides a method for interpreting the interaction parameters **e**. $e_{ij}$ is a matrix and we want a scalar score to measure how \"large\" that matrix is.  The *Frobenius Norm* of $e_{ij}$ is introduced for doing this: \n",
      "$$F_{ij} = \\sqrt{\\sum_{a, b = 1}^q \\tilde{e}_{ij}(a, b)^2 }$$\n",
      "with $\\tilde{e}_{ij}$ being the transformed coupling matrices:\n",
      "$$\\tilde{e}_{ij}(a, b) = e_{ij}(a, b) - e_{ij}(\\cdot, b) + e_{ij}(a, \\cdot) + e_{ij}(\\cdot, \\cdot)$$\n",
      "The dot denotes averages over all values of q for the concerned position. Results in the sums over all rows and columns in $\\tilde{e}_{ij}$ equaling zero. Cocco et al. explain this gauging with the intuition of \"putting as much as possible\" of the statistical mass into  the local field parameters and \"as little as necessary\" into the couplings.(MAYBE average product correction as in Burger2010)\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Imperatively Defined Factor Graphs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Factor Graphs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Factor graphs are a very general kind of graphical model proposed in [Kschischang2001a]. They are a generalization of *Tanner graphs* [Wiberg1996] and thus have their origin in coding theory. They where originally thought up to discribe a generic message passing algorithm for decoding, the *sum-product algorithm*. In the same original work Kschischang et al. note that the sum-product algorithm is actually a generalization of many established message-passing algorithms, both from coding theory as well as those used in artificial intelligence (including Pearl's *belief propagation* [Pearl1982], the Viterbi algorithm [Viterbi1967] and many others). The work can therefore be seen as a major contribution to the insight that coding theory and statistical inference form two sides of the same coin [MacKay2003]. Almost in parallel Aji and McEliece proposed the *generalized distributive law* which also generalizes belief propagation [Aji2000]. Although Factor graphs had been developed as a device to discribe the sum-product algorithm, they have shown to be very valuable in their own right.\n",
      "\n",
      "Factor graphs can be seen as a sort of *lingua franca* of graphical models. Bayes' nets (directe graphical models) and Markov random fields (undirect graphial models) can be readily transformed into Factor graphs [Bishop2006a]. Makrov random fields are an important family of graphical models, interestingly also derived from the Ising model [Kindermann1980]. We could therefore show that our Potts model is also a MRF, we will not do so though as it very naturally translates into a Factor graph.\n",
      "\n",
      "Common to both directed and undirected graphical models is that they encode a set of conditional independence assumptions that have to be satisfied by any factorization that might be derived from the graph. In contrast to that the structure of a factor graph does not imply any conditional independence assumptions.\n",
      "\n",
      "Kschischang et al. define a factor graph:\n",
      "\n",
      "*Suppose that $g(x_1,...,x_n)$ factors into a product of several local functions, each having some subset of ${x_1, ...,x_n}$ as arguments; i.e., suppose that\n",
      "$$g(x_1,...,x_n) = \\prod_{j\\in J} f_j(X_j)$$\n",
      "where $J$ is a discrete index set, $X_j$ is a subset of ${x_1,...,x_n}$ and $f_j(X_j)$ is a function having the elements of $X_j$ as arguments.*\n",
      "\n",
      "*Definition:\n",
      "A factor graph is a bipartite graph that expresses the structure of the factorization above. A factor graph has a variable node for each variable $x_i$, a factor node for each local function $f_j$, and an edge connecting variable node $xi$ to factor node $f_j$ if and only if $x_i$ is an argument of $f_j$*\n",
      "\n",
      "This definition is not constrained to probability distributions. In our case though, a factor graph shall refer to a factorization of a joint probability distribution over all components ${x_1,...x_n} \\in \\mathbf{x}$. The factors may be normalized such that the sum over all possible of $\\mathbf{x}$ is $1$.\n",
      "$$P(\\mathbf{x}) = \\prod_{j\\in J} f_j(X_j)$$\n",
      "or we may write our factorization to include a normalization constant $Z$\n",
      "$$P(\\mathbf{x}) = \\frac{1}{Z}\\prod_{j\\in J} f_j(X_j)$$ \n",
      "this does not affect our ability to draw out the factorization as a factor graph, as $Z$ is a constant and can therefore be multiplied into any of our factors and not be represented seperately in the graph.\n",
      "\n",
      "If we now take a look at our model:\n",
      "$$P(\\mathbf{x}| \\mathbf{e}, \\mathbf{h}) = \\frac{1}{Z} exp[\\sum_{i, j | i<j} e_{i, j}(x_i, x_j) + \\sum_i h_i x_i]$$\n",
      "we find that it naturally factorizes into local functions of pairwise interactions and single variable preferences:\n",
      "$$P(\\mathbf{x}| \\mathbf{e}, \\mathbf{h}) = \\frac{1}{Z} \\prod_ {i, j | i<j}e^{e_{i, j}(x_i, x_j)} \\prod_i e^{h_i x_i}$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mathematical Representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One might wonder what these local functions actually look like. Factor graphs are agnostic to this, but we shall discribe a specific form that we will make use of later. Let $\\phi_j$ be a sufficient statistics (we could say that$\\phi(X_j)$ is a feature function that returns a sufficient statistics). Let $\\theta_j$ be a parameter vector. Then our factor is $f_j=\\theta_j\\phi_j$.\n",
      "\n",
      "To exemplifiy this lets look at our pairwise factors:\n",
      "$$e_{ij}(x_i, x_j) = \\theta_{ij}\\phi(x_i, x_j)$$\n",
      "If we assume the domain $\\mathcal{X}$ of $x$ to be categorical and $|\\mathcal{X}|=3$ then we can represent $x$ as a \"one hot\" thee-component vector. \n",
      "\n",
      "$$x \\in \\{\n",
      "\\left(\\begin{array}{c}\n",
      "1 \\\\\n",
      "0 \\\\\n",
      "0 \\end{array} \\right)\n",
      "\\left(\\begin{array}{c}\n",
      "0 \\\\\n",
      "1 \\\\\n",
      "0 \\end{array} \\right)\n",
      "\\left(\\begin{array}{c}\n",
      "0 \\\\\n",
      "0 \\\\\n",
      "1 \\end{array} \\right)\n",
      "\\}$$\n",
      "With this representation for the states of $x$, the feature function is the outer product. $\\phi_{ij} = x_i \\otimes x_j = x_ix_j^\\top$. Our sufficient statistic is a $|\\mathcal{X}|\\times|\\mathcal{X}|$ matrix. Then $\\theta_{ij}$ will also be a $|\\mathcal{X}|\\times|\\mathcal{X}|$ matrix. We have to introduce a scalar (or dot) product defined between two matrices, as our factors have to result in scalars. It turns out that the *Frobenius inner product* fulfills our needs. \n",
      "\n",
      "Definition: Frobenius inner product:\n",
      "$$A : B = \\sum_{n,m}A_{nm}B_{nm} $$\n",
      "\n",
      "A complete example:\n",
      "$$x_1 = \\left(\\begin{array}{c}\n",
      "1 \\\\\n",
      "0 \\\\\n",
      "0 \\end{array} \\right), x_2 = \\left(\\begin{array}{c}\n",
      "0 \\\\\n",
      "1 \\\\\n",
      "0 \\end{array} \\right)$$\n",
      "\n",
      "$$\\phi_{12} = x_1x_2^\\top = \\left(\\begin{array}{ccc}\n",
      "0 & 1 & 0\\\\\n",
      "0 & 0 & 0\\\\\n",
      "0 & 0 & 0\\end{array} \\right), \\theta_{12} = \\left(\\begin{array}{ccc}\n",
      "1 & 4 & 7\\\\\n",
      "2 & 5 & 8\\\\\n",
      "3 & 6 & 9\\end{array} \\right)$$\n",
      "\n",
      "$$f_{12} = \\theta_{12}\\phi_{12} = 4$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Probabilistic Progamming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the success probabilistic models, and graphical model in particular, have enjoyed in machine learning, the question arises how these methods can be made availiable to a wider audience of domain experts. Establishing an abstraction layer between model description and inference and learning procedures seems like a reasonable approeach to this problem. This idea has been called *Probabilistic Programming* and is a field of research that resides at the intersection of machine learning and language design. \n",
      "\n",
      "Gordon, Henzinger, Nori et al. define  a probabilistic program as \n",
      "\n",
      "*[...] functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of variables in a program via observations.* [Gordon2014]\n",
      "\n",
      "Goodman gives an even simpler definition:\n",
      "\n",
      "*[...] probabilistic programming languages\n",
      "extend a well-specified deterministic programming language with primitive constructs for random choice.* [Goodman2013]\n",
      "\n",
      "Current probabilstic programming languages could roughly be classified along two dimensions: Language paradigm (imperative, functional, logical) [Gordon2014] and wether they are a self-contained language including a compiler or are implemented as an extension to an established deterministic language.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table>\n",
      "<tr>\n",
      "    <td>.</td>\n",
      "    <td>Imperative</td>\n",
      "    <td>Functional</td>\n",
      "    <td>Logical</td>\n",
      "</tr>\n",
      "<tr>\n",
      "    <td>Self-contained</td>\n",
      "    <td></td>\n",
      "    <td><span class=\"cap\">Bugs</span>[Gilks94]<br>\n",
      "        Church[Goodman2008]<br>\n",
      "        Stan[SOURCE]<br>\n",
      "        <span class=\"cap\">Ibal</span>[Pfeffer]<br></td>\n",
      "    <td><span class=\"cap\">Blog</span>[Milch]<br>\n",
      "        Alchemy[Kok2007]<br>\n",
      "        Tuffy[Niu]<br>\n",
      "        ProbLog[Kimmig2011]<br>\n",
      "        <span class=\"cap\">Prism</span>[Sato1997]</td>\n",
      "</tr>\n",
      "<tr>\n",
      "    <td>Extension</td>\n",
      "    <td><span class=\"cap\">Factorie</span>[Mccallum]<br>\n",
      "        Figaro (OO)[Pfeffer09]\n",
      "        <br> Infer.NET [InferNET12]</td>\n",
      "    <td><span class=\"cap\">Factorie</span><br>\n",
      "        Figaro<br>\n",
      "       \n",
      "    <td></td>\n",
      "</tr>\n",
      "</table>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This table is an almost comprehensive collection of probabilistic programming frameworks mentioned in [Mccallum] and [Gordon2014]. It is easy to see that there is a heavy bias towards functional and logical languages. The majority of frameworks also exist in a stand-alone fashion, seperated from potentially rich ecosystems of established languages. This has not been a reason for practitioners to refrain from using them of course. Especially <span class=\"cap\">Bugs</span>[Gilks94] is a wildly used language for Baysian modeling.\n",
      "In contrast stand the frameworks that are extensions to all-purpose, imperative languages. Infer.NET is an extension to C# developed at Microsoft Research, currently with an academic-only license. Figaro is an extension to Scala, developed by Charles River Analytics and availiable under a custom open-source licens. In this work we shall use <span class=\"cap\">Factorie</span>, also an extension to Scala. It is maintained by the Information Extraction and Synthesis Laboratory at the University of Massachusetts, Amherst and availiable under the Apache 2.0 licence."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Factorie"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The expressive power of factor graphs to represent both directed and undirected graphical models has motivated a team of researches around Andrew McCallum (author of NLP toolkit <span class=\"cap\">Mallet</span> [MALLET]) to use them as a basis for a general purpose probabilistic programming framework. [Mccallum2009][Mccallum]. Their system is called <span class=\"cap\">Factorie</span> (Factor graphs, Imperative, Extensible) and combines multiple design decisions, many of which are motivated by McCallum et al.'s focus on NLP.\n",
      "<span class=\"cap\">Factorie</span> is implemented as a library of Scala[Odersky04], a functional, interactive,(including a REPL) and object-oriented language compiling to the JVM [Lindholm99]. Scala stands for *scalable language* and was designed with parallelization and concurrancy in mind. Scala has a static typing system that supports multiple inheritance through *traits*. It also allows for trivial implementation of additional operators, thus blurring the lines of libraries and *domain specific languages*. All of these language features make Scala a great fit for a probabilistic progamming framework.\n",
      "\n",
      "While factor graphs offer a concise and intuitive way to grasp and reason about graphical models, their declarative nature stands in contrast to how computer programms are usually written. McCallum et al. resolve this contrast through their approach of *imperatively defined factor graphs*. What they mean by this is that FACTORIE allows the user to use the well-known concepts of a general purpose language to discribe the components of a model: variables, factors, structure, sufficient statistics etc. These imperatively defined model components also offer an abstraction layer for developing inference and learning components that work across many different models.\n",
      "\n",
      "FACTORIE contains modules for all sub tasks of a machine learning system, and seperates them into three main steps (1) *Model Structure* (2) *Inference* and (3) *Learning*.\n",
      "\n",
      "**Model Structure**\n",
      "*Variables* in FACTORIE are typed objects in the object-oriented language. In addition to common variable kinds like *binary*, *categorical*, *real* and *ordinal*, variables can also be *strings*, *sets*, *objects* etc. They hold a single possible value of a random variable, not distributions. Therefore they can be seen as containers of data or a single possible world. Variables have a *domain* which are also typed objects. Changes to variables are stored in *DiffLists*, which allows for very efficient implementation of sampling algorithms, as rejected sampling steps can be easily reverted.\n",
      "\n",
      "The class hierarchy around factors is made up of the three distinct concepts of *families*, *templates* and *factors*. A *family* discribes the the general structure of a factor. Ie. our factors explained above which can be seperated into a weight vector and a sufficient statistics function would form a family. A *template* inherits from a family and further discribes where to find neighbouring variable nodes. The choice to use *templated factors*, stems from the creators' background in NLP. Factor graphs in NLP settings often use *parameter tying*, this means that the same parameter is used in different factors. *Factors* themselves are not persistend objects, but are instead instantiated by a template whenever they are needed. In our case parameter tying and templates are not neccessary, so each factor is represented by a distinct template.\n",
      "\n",
      "A *model* is simply a collection of templates (not of variables). It can be extended with the *parameters* trait, which makes all the weight vectors accesable for learning algorithms.\n",
      "\n",
      "**Inference**\n",
      "<span class=\"cap\">Factorie</span> supplies different inference algorithms, as well as a convenient architecture to implement additional ones. There are belief propagation variants for chains, trees [Pearl1988] and loopy graphs [Yedidia2000]. Sampling algorithms include a ready-to-use Gibbs sampler [Geman1984] as well as a Matropolis-Hastings sampler [Metropolis53] [Hastings70] which has to be supplied with a proposal distribution.\n",
      "Inference results in a *summary*, which is a collection of single-variable *marginals* and a value for $Z$.\n",
      "\n",
      "**Learning**\n",
      "Different learning strategies can be employed through a choice of optimization algorithms, objective (or loss) function, regulatizations and learning rates. We find *Stochastic Gradient Descent* [Bottou98], *AdaGrad* [Duchi2010], *RDA* [Xiao] for online-learning, and *limited-memory BFGS* [Byrd1994][Byrd1996] for batch learning. Objective functions are implemented through objects called *examples*, which are constructed out of a data point and are able to compute an objective value and a gradient. Availiable objectives which require inference as a sub-routine are maximizing the *likelihood* (Maximum Likelihood learning) and maximizing the *posterior* (MAP or Bayseian learning). We also find *Contrastive Divergence* [Hinton2002] and *SampleRank* [Wick2008][Wick2011], which optimize different objectives that don't require inference. \n",
      "Different *trainers* are availiable, which provide convenient combinations of the methods above and take care of thread-safe parallelization.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Parameter Estimation Methodology"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Structure Learning Approaches"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that our task is to infer infer the pairwise connection parameters $e_{ij}$ from  data, and use these parameters as a measure for direct dependency between variables. [Wu2012] notes that approaches to this problem of *structure learning* could be seperated into two groups. The first approeach is to search for possible structures based on local conditional independence tests [Abbeel2006]. This approach foremostly try to decide which $e_{ij} \\in \\mathbf{e}$ should exist at all ie. which $e_{ij} \\ne \\mathbf{0}$. This means that the problem is essentially a combinatorial search problem. The second approach would be to treat the problem as a $\\ell_1$ or $\\ell_2$ regularized maximum likelihood estimation of the parameters $e_{ij}$. [Varun2006] Because the likelihood is convex, the problem reduces to a convex optimization problem. Another, third approach has been proposed in [Chen2009a]. Here a fully Baysian approach is taken with a \"spike and slab\" prior over the parameters, which is similar to a $l_0$ regularization."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "l1- Regularization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Maximum Likelihood Estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Although it poses a set of challanges, which we will explore shortly, our work here shall focus on the second approach. [Weigt2009] use this approach without citing any prior work and might have derived it themselves. We shall follow a more formal derivation proposed in [Varun2006]. \n",
      "Instead of seperating the steps of structure learning (deciding which interactions should exist) and parameter learning (deciding how strong interactions should be) the key idea is to subsume both of them into just learning the parameters. By enforcing some penalty on the \"size\" (a norm) of the parameters.\n",
      "\n",
      "Maximum likelihood learning is the most common way to estimate parameters, but we shall derive it here anyways.\n",
      "Suppose that we want to find the most likely parameter given our dataset $D$. In probabilistic language we could say we want to find the maximum of the following distribution:\n",
      "$$P(\\theta|D)$$\n",
      "\n",
      "using Bayes' theorem this can can be written as:\n",
      "$$P(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{\\sum_{D}[P(D|\\theta)P(\\theta)]}$$\n",
      "\n",
      "this is such a common expression that the individual terms have been given names [MacKay]:\n",
      "\n",
      "$$posterior = \\frac{likelihood \\times prior}{evidence}$$\n",
      "\n",
      "if we assume an uniform prior (the uninformative assumtion) we can disregard it. The *evidence* term (sometimes called *marginal likelihood*) is simply a normalization constant so we can write:\n",
      "\n",
      "$$P(\\theta|D) \\propto P(D|\\theta)$$\n",
      "\n",
      "As we are not interested in the actual distribution of $P(\\theta|D)$, just its maximum, we can thus maximize the likelihood $P(D|\\theta)$ instead.\n",
      "\n",
      "If we assume that our training samples $D=\\{\\mathbf{x}^{(1)},...,\\mathbf{x}^{(m)}\\}$ are i.i.d. (a strong assumtion that doesn't necessarily hold in biological contexts) we can write our likelihood as following:\n",
      "$$\\mathcal{L}(\\theta; D) = \\prod_{\\mathbf{x}^{(k)} \\in D} P(\\mathbf{x}^{(k)}|\\theta)$$\n",
      "it is common to use the negative log-likelihood to turn the product into a sum:\n",
      "$$\\mathcal{l}(\\theta; D) =  log\\mathcal{L}(\\theta; D) = \\sum_{\\mathbf{x}^{(k)} \\in D} logP(\\mathbf{x}^{(k)}|\\theta)$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "If we now apply this to our Potts model from above: \n",
      "$$P(d|\\theta) = P(\\mathbf{x}| \\mathbf{e}, \\mathbf{h}) = \\frac{1}{Z( \\mathbf{e}, \\mathbf{h})} \\prod_ {i, j | i<j}e^{e_{i, j}(x_i, x_j)} \\prod_i e^{h_i x_i}$$\n",
      "\n",
      "$$\\mathcal{l}(\\mathbf{e}, \\mathbf{h}; D) = \\sum_{\\mathbf{x}^{(k)} \\in D} [-logZ(\\mathbf{e}, \\mathbf{h}) + \n",
      "\\sum_{i, j| i<j}e_{i, j}(x_i, x_j)+\n",
      " \\sum_{i}h_ix_i]$$\n",
      " $$=\\ -MlogZ(\\mathbf{e}, \\mathbf{h}) + \\sum_{\\mathbf{x}^{(k)} \\in D} [ \n",
      "\\sum_{i, j| i<j}e_{i, j}(x_i, x_j)+\n",
      " \\sum_{i}h_ix_i]$$\n",
      " because $\\mathcal{l}$ is our objective function which we want to maximize, we can do linear transformations to the right-hand side, without changing the left hand side, and write:\n",
      " $$\\mathcal{l}(\\mathbf{e}, \\mathbf{h}; D) = -logZ(\\mathbf{e}, \\mathbf{h}) + \\frac{1}{M}\\sum_{\\mathbf{x}^{(k)} \\in D} [ \n",
      "\\sum_{i, j| i<j}e_{i, j}(x_i^{(k)}, x_j^{(k)})+\n",
      " \\sum_{i}h_ix_i^{(k)}]$$\n",
      "$$ = -logZ(\\mathbf{e}, \\mathbf{h}) + \\langle \\sum_{i, j| i<j}e_{i, j}(x_i^{(k)}, x_j^{(k)})+\n",
      " \\sum_{i}h_ix_i^{(k)} \\rangle_{D}$$\n",
      " \n",
      "We shall from now on use our factor graph notation of $e_{ij}(x_i, x_j) = \\theta_{ij}\\phi(x_i, x_j)$ and $h_ix_i = \\theta_i\\phi(x_i)$. $\\Theta$ shall denote the set of all $\\theta_{ij}$ and $\\theta_i$.\n",
      " \n",
      "This log-likelihood is a sum of convex functions, and is therefore a convex function itself. This means training our model can be seen as an unconstrained convex optimization problem. \n",
      "$$\\hat{\\Theta} = \\arg\\max\\mathcal{l}(\\Theta; D)$$\n",
      "These can be solved through gradient ascent methods (or gradient descent, then we find the minimum of the negative log-likelihood. We will use the terms interchangeably.). The basic idea of gradient ascent is to step through the function in search of an a local extremum by always stepping in the direction of the gradient of the current point: [Source]\n",
      "$$\\Theta_{t+1} = \\Theta_{t} + \\mu \\nabla\\mathcal{l}(\\Theta_t)$$ \n",
      "\n",
      "$\\lambda$ here is the step length, also called the *learning rate*. $\\lambda$ can be seen as a hyperparameter of our model and should be treated accordingly. This means it should be tuned as it can affect our time to learn, the accuracy of our predictions and might even lead to pathological behavior if chosen unwisely.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\frac{\\partial \\mathcal{l}}{\\partial\\theta_{ij}} = \n",
      "-\\frac{\\partial logZ(\\Theta)}{\\partial\\theta_{ij}}+ \\langle\\phi(x_i, x_j)\\rangle_{D}$$\n",
      "\n",
      "$$\\frac{\\partial \\mathcal{l}}{\\partial\\theta_{i}} = \n",
      "-\\frac{\\partial logZ(\\Theta)}{\\partial\\theta_{i}}+ \\langle\\phi(x_i)\\rangle_{D}$$\n",
      "\n",
      "There is a fundamental hurdle in this approach though. We have to know the value of the partial derivatives of $logZ$ which are actually the expectations of the feature functions(analog for $\\theta_i$):[Woodford]\n",
      "$$\\frac{\\partial logZ(\\Theta)}{\\partial\\theta_{ij}} \n",
      "=\\frac{1}{Z(\\Theta)}\\frac{\\partial Z(\\Theta)}{\\partial\\theta_{ij}}$$\n",
      "\n",
      "$$= \\frac{1}{Z(\\Theta)}\\frac{\\partial}{\\partial\\theta_{ij}}\n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} exp[E(\\mathbf{x}|\\Theta)]$$\n",
      "$$= \\frac{1}{Z(\\Theta)}\n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} exp[E(\\mathbf{x}|\\Theta)] \\phi(x_i, x_j) $$\n",
      "$$= \n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} \\frac{1}{Z(\\Theta)}exp[E(\\mathbf{x}|\\Theta)] \\phi(x_i, x_j) $$\n",
      "$$= \n",
      "\\sum_{\\mathbf{x} \\in \\Omega_\\mathbf{x}} P(\\mathbf{x}|\\Theta) \\phi(x_i, x_j) $$\n",
      "$$=\\langle \\phi(x_i, x_j) \\rangle_{P(\\mathbf{x}|\\Theta)}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To know $Z$ exactly we would have to sum over the combinatorial state space of $\\mathbf{x}$ ($|\\Omega_\\mathbf{x}|=Q^N$). If our factor graph was treelike there would be methods to compute $Z$ exactly without enumerating all possible states. For the general case, including loops in the model structure, $Z$ is intractable. It happens that our model is loopy, as loopy as it could be: completely connected. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sampling Based Approximation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The basic idea behind sampling methods (also called *Monte Carlo* methods is simple. Assume you have a distribution $P(x)$ and you wish to compute an expectation $\\langle f(x)\\rangle_{P} = \\sum_{x \\in \\Omega x} P(x)f(x)$  of a generic function. If you can't find a closed form solution for the expectation, you can approximate it numerically. If you have access to a sufficiently big set $\\mathcal{H}$ of i.i.d. samples drawn from $P(X)$.\n",
      "$$\\mathcal{H} = \\{x^{(1)},..., x^{(L)}\\}$$\n",
      "$$\\langle f(x)\\rangle_{P_\\Theta^L} = \\frac{1}{L}\\sum_{\\mathcal{x^{(i)} \\in H}}f(x^{(i)})$$\n",
      "Note that our samples in $\\mathcal{H}$ are synthetic samples. They have do be drawn from $P(X)$ but they don't have to be from an empirical dataset.\n",
      "We wrote the approximated expectation with respect to $P_\\Theta^L$, this denotes that we took the expectation with respect to a data distribution of $L$ samples. Because our samples are i.i.d. and because of the law of large numbers, it can be shown that $P_\\Theta^{\\infty} = P(\\mathbf{x}|\\Theta)$.[SOURCE].\n",
      "**Generating synthetic samples**\n",
      "How to generate these i.i.d. samples from $P(x)$ is where the actual challange lies. We don't have access to the normalized probabilties and our original goal was to avoid enumerating the whole state space of $x$. This is where *Markov Chain Monte Carlo* (MCMC) methods come in. We shall focus on *Gibbs sampling* [Geman1984] which is a special case of the much older and more general Matropolis-Hastings method [Metropolis53] [Hastings70].\n",
      "\n",
      "The idea common to all MCMC methods is that we produce our samples in a stochastic process in which each sample $x^{(t+1)}$ is dependent only on it's predecessor $x^{(t)}$, hence *Markov chain*. The process has to be chosen in such a way that it is guaranteed to converge.\n",
      "\n",
      "Gibbs sampling is one of these processes that is intuitive and works well in high dimensional graphical models.\n",
      "Assume a multi-dimensional distribution $P(\\mathbf{x}) = P(x_1, ..., x_n)$ is given by a factor graph $P(\\mathbf{x}) = \\frac{1}{Z}\\prod_{j\\in J} f_j(X_j)$ Sampling from $P(\\mathbf{x})$ is impossible, but sampling from the conditionals distributions $P(x_i|\\{x_1,...,x_n\\} \\setminus x_i)$ is easy. If we fix all components apart from $x_i$ this is equivalent to removing all factors which don't have $x_i$ as an argument.\n",
      "\n",
      "\n",
      "$$P(x_i|\\{x_1,...,x_n\\} \\setminus x_i) = \\frac{1}{Z}\\prod_{j\\in J|x_i \\in X_j} f_j(X_j) $$\n",
      "The normalization constant of this distribution is easy to compute as we only have to sum over the $Q$ possible states of $x_i$. \n",
      "\n",
      "To generate a new sample $\\mathbf{x}^{(t+1)}$ each component $x_i$ of $\\mathbf{x}^{(t)}$ is simply updated one by one. The conditioning is done with respect to the new state of already updated components and the old state of components yet to come.\n",
      "$$x_1^{(t+1)} \\sim P(x_1^{(t)}|\\{x_2^{(t)},...,x_n^{(t)}\\})$$\n",
      "$$x_2^{(t+1)} \\sim P(x_2^{(t)}|\\{x_1^{(t+1)},x_3^{(t)}...,x_n^{(t)}\\}) etc. $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have obtained our *trace* $\\mathcal{H}$, we could compute the estimated expectations of the feature funtions needed for our gradient:\n",
      "$$\\frac{\\partial \\mathcal{l}}{\\partial\\theta_{ij}} = \n",
      "\\langle\\phi(x_i, x_j)\\rangle_{D} - \\langle \\phi_{ij}(x_i, x_j) \\rangle_{P^K}$$\n",
      "\n",
      "There are a few problems with this approach though. Firstly, the problem of convergence. It might take a long time till the samples obtained through our Gibbs sampler are actually drawn from $P(\\mathcal{x})$, and diagnosing if the chain has converged is hard enough to make up its own field of research.\n",
      "But even if we assume that our sampler will converge in reasonable time, a second problem remains [Hinton2002]:\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Contrastive Divergence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because of these problems in computing the partial derivatives, alternatives to ML learning have been proposed. Hinton [Hinton2002] suggested a method he named *Contrastive Divergence* (CD) for random fields of the form $P(\\mathbf{x}|\\mathbf{\\Theta}) = \\frac{1}{Z(\\mathbf{\\Theta})}e^{-E(\\mathbf{x, \\Theta})}$.\n",
      "\n",
      "Above we found that we need the following derivative:\n",
      "$$\\frac{\\partial \\mathcal{l}}{\\partial\\theta_{ij}} = \n",
      "\\langle\\phi_{ij}(x_i, x_j)\\rangle_{P^0} - \\langle \\phi_{ij}(x_i, x_j) \\rangle_{P_\\Theta^\\infty}$$ \n",
      "$P^0$ shall denote our data distribution, denoted above with $D$, and $P_\\Theta^\\infty = P(\\mathbf{x}|\\Theta)$. \n",
      "We tried to approximate it running our Gibbs sampler for very many intervals $L$ and using the distribution of our trace $P_\\Theta^L$ in place of $P_\\Theta^\\infty$.\n",
      "\n",
      "Hintons key idea is that it might be sufficient to use a small number $K$ instead of a very large $L$ to give us a general idea in which direction to walk. In fact he proposes that $L=1$ might often be enough. So the gradient used in Contrastive Divergence looks like this:\n",
      "\n",
      "$$\\frac{\\partial \\mathcal{l}}{\\partial\\theta_{ij}} = \n",
      "\\langle\\phi_{ij}((x_i, x_j)\\rangle_{P^0} - \\langle \\phi_{ij}(x_i, x_j) \\rangle_{P_\\Theta^K}$$ \n",
      "\n",
      "With $P_\\Theta^K$ being the the the data distribution generated by running $K$ steps of Gibbs sampling on our original $D$ given the current parameters $\\Theta$.\n",
      "\n",
      "In practice these gradients are sufficient for parameter learning, without any need to look at the objective function that is being optimized. It should be noted though that Contrastive Divergence learning minimizes the difference between $KL(P^0, P_\\Theta^\\infty)$ and $KL(P^K, P_\\Theta^\\infty)$ [Hinton2002]. This objective function is not identical with the one ML-learning optimizes and CD-learning does not converge to the same parameter estimates. In general CD-learning shows lesser performance than ML-learning, but on average is very close to it. [Hinton05]\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stochastic Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Combining this Contrastive Divergence gradient with the gradient descent updating term from above we get:\n",
      "$$\\Theta_{t+1} = \\Theta_{t} + \\mu (\\langle \\Phi(D) \\rangle_{P^0} - \\langle \\Phi(D) \\rangle_{P_\\Theta^K}) $$ \n",
      "\n",
      "This straightforward way to implement our Contrastive Divergence learning would use *batch gradient descent [Bottou98]*, ie. do $K$ sampling steps on all samples in $D$ and perform the above sum to get our gradient for a given step. Summing over all training samples at each updating step, can be costly though which is why alternatives have been explored.\n",
      "\n",
      "*Stochastic gradient descent* (SGD) (also *online gradient descent*) is one of these alternatives [Bottou98]. Instead of computing the average gradient over our training data, we randomly step through our training samples, compute a sub-gradient for each one of them and update the parameters immediately. Our update step for one sample $\\mathbf{x}^{(t)}$ thus looks like this:\n",
      "$$\\Theta_{t+1} = \\Theta_{t} + \\mu ( \\Phi(\\mathbf{x}^{(t)}) - \\Phi(\\mathbf{x}^{(t)}_{P^K_\\Theta}) ) $$ \n",
      "After all training samples have been passed over once, the process is usually repeated multiple times. Each time the order of the samples is randomly chosen (the *stochastic* aspect*) and the number of these iterations is a hyperparameter of SGD learning.\n",
      "\n",
      "The learning rate $\\lambda$ is another hyperparameter in SGD and can severely impact the outcome.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Adaptable Learning Rate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As [Weigt2011] notes, our protein dataset sufferes from a sampling bias. Because of a neccessary sub-choice of organisms to sample from, and because of *phylogenetic* relatedness between these organisms, our samples can not be seen as independent. Essentially this means that across the sequences we will see a lot of similarities, which in turn endanger swamping the actually interesting micro-mutations by sheer mass. [Weigt2011] relsove this issue by stepping through the sequences and reweighting them with the inverse of the count of sequences that are $>80\\%$ identical with the sequence at hand.\n",
      "\n",
      "We shall attempt to address this issue by using the <span class=\"cap\">AdaGrad</span> algrorithm [Duchi2011]. In Duchi et al. own words: \"[AdaGrad] give[s] frequently occurring features very low learning rates and infrequent features high learning rates, where the intuition is that each time an infrequent feature is seen, the learner should \u201ctake notice.\u201d Thus, the adaptation facilitates finding and identifying very predictive but comparatively rare features.\" [Duchi2011] \n",
      "In practice this means that our learner keeps a running sum of the squares of all previous gradients. The means the update for one of our features looks as follows:  [Dyer]\n",
      "$$\\theta_{ij, t+1} = \\theta_{ij, t} - \\frac{\\mu}{\\sqrt{\\sum_{t'=1}^t g^2_{ij, t'}}}g_{ij, t}$$\n",
      "$$g_{ij, t} = \\phi(x_{ij}^{(t)}) - \\phi(x_{ij,P^K_\\Theta}^{(t)})$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regularized Dual Averaging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "As in [Varun2006] we want to use $\\ell_1$-regularization to induce sparcity. Unfortunately classical regularization, which introduces a penalty term $\\lambda \\|\\theta\\|_1$ to the objective function (see [MacKay]) is not applicable to SGD. [Xiao] proposes a method he calles *Regularized Dual Averaging* (RDA) to enable $\\ell_1$ and $\\ell_2$ regularization in online learning settings.\n",
      "The idea of RDA is to keep a running average of the sub-gradients:\n",
      "$$\\overline{g}_{ij,t} = \\frac{1}{t}\\sum_{t'=1}^t g_{ij, t'}$$\n",
      "for each feature and only update the parameter should it's absolute become bigger than the $\\ell_1$ hyperparameter $\\lambda$:\n",
      "$$\\theta_{ij, t+1} = \n",
      "    \\begin{cases}\n",
      "    0 & \\text{if} |\\overline{g}_{ij,t}| \\leq \\lambda \\\\\n",
      "    -\\text{sgn}(\\overline{g}_{ij,t}) \\mu \\sqrt{t}(|\\overline{g}_{ij,t}| - \\lambda) & \\text{otherwise}\n",
      "    \\end{cases}\n",
      "$$\n",
      "Combining this with the learning rate term from AdaGrad we get the following update step per feature:\n",
      "$$\\theta_{ij, t+1} = \n",
      "    \\begin{cases}\n",
      "    0 & \\text{if} |\\overline{g}_{ij,t}| \\leq \\lambda \\\\\n",
      "    -\\text{sgn}(\\overline{g}_{ij,t}) \\frac{\\mu}{\\sqrt{t \\sum_{t'=1}^t g^2_{ij, t'}} }(|\\overline{g}_{ij,t}| - \\lambda) & \\text{otherwise}\n",
      "    \\end{cases}\n",
      "$$\n",
      "[Dyer]\n",
      "\n",
      "As our regularization happens in a per-sample basis, the we want to take number of samples $M$ into account. Our overall hyperparameter therefore is $\\lambda = \\Lambda /M$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Empirical Results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Note on Choice of Method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "One of the main goals of this work is to explore the practicability of probabilistic progamming with Factorie in an bioinformatics context. The algorithms explained above where all preimplemented in Factorie and were deemed the most promising and interesting combination for our problem at hand. Especially the similarity of our model with *Restricted Boltzmann Machines* and Contrastive Divergence's success in training these (see for example [Bengio2009a]) was an inspiration."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Our data consists of Pfam [source] multiple sequence alignments and actual distances between positions calculated from PDB [SOURCE] radio-crystallography datasets. This dataset was originally composed by Morocos et al. [Source] based on the Pfam v24 and updated to match the Pfam v26 indexes by Magnus Ekberg [soruce].\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation Metric"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We will follow Ekberg's methodology and consider all position pairs $ij$ with a measuresd distance $< 8.5 \\AA $ as *contacts* and in the set of positives $\\mathcal{P}$. We will completely ignore position pairs $ij$ if $|i - j| <= 4$ as neighbouring amino-acids are expected to be close to each other.\n",
      "To evaluate how many of these contacts we predicted, we will order our pairwise interaction scores $F_{ij} \\in \\mathcal{F}$ descendingly and look at the true positive rates in the subset $\\mathcal{F}_n$ of the n strongest predictions.\n",
      "$$\\mathcal{TP}_n = \\mathcal{F}_n \\cup \\mathcal{P}$$\n",
      "$$TPR_n = \\frac{|\\mathcal{TP}_n|}{n}$$\n",
      "and the overall TP-Rate for all $p = |\\mathcal{P}|$ positives.\n",
      "$$TPR = \\frac{|\\mathcal{TP}_p|}{p}$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hyperparameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We have $4$ hyperparameters that have to be tuned: the learning rate $\\mu$, the $\\ell_1$ parameters $\\lambda$, the number of steps our CD sampler should take $K$ and the number of times our SGD algorithm should pass over the dataset $N$.\n",
      "As Ekberg reports relative robustness of the hyperparameters with regard to the MSA at hand, we shall do a hyperparameter search on one of the domains, and then try the best $5$ combinations on the other MSAs.\n",
      "Do do this we will use Factorie's hyperparameter infrastructure to randomly visit the search space $100$ times.\n",
      "In particular we will uniformly drawing a $K$ from $\\{1, 2, 3, 5, 10\\}$ and a $N$ from $\\{1, 3, 8, 10\\}$, as well as log-uniformly drawing a $\\mu$ from $[10^{-3}, 10]$ and $\\Lambda$ from $[10^{-9}, 1]$. \n",
      "This choice of sampling spaces is kind of arbitrary, but shall suffice to find a somewhat optimal combination.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "\n",
      "\n",
      "def css_styling():\n",
      "    styles = open(\"custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "        margin-left:16% !important;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: Computer Modern, Helvetica, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "\n",
        "\n",
        "    span.cap { font-variant: small-caps; }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 130%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 22pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<IPython.core.display.HTML at 0x2a513d0>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The task of estimating parameters, or \"fitting a model to data\", is a non-trivial problem. Many different fields have concerned themselves with this task, among them physics, statistics, computer science and information theory. An optimization based approach has proven useful. Usually an objective function, discribing the quality of fit between a model-parameter pair and the given data, is maximized (or minimized) while varying the parameters.\n",
      "\n",
      "With the rapid growth of computational resources as well as generated data, most of the research developed in parallel in the fields mentioned above seems to have converged in computer science. \n",
      "If we introduce a task that our model is supposed to inform, the above approach fits Tom Mitchell's definition [Mitchell1990] of *machine learning*:\n",
      "\n",
      "*A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P,  improves with experience E.*\n",
      "\n",
      "Mutual Information\n",
      "Another simple method for infering interactions is estimating the *mutual information* (MI). [Steuer2002] MI is one of the fundamental concepts of information theory. In order to understand MI lets first introduce another essential concept of information theory, *entropy*. Both of these concepts where introduced by Shannon, the father or information theory, in 1948 [Shannon48]\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}